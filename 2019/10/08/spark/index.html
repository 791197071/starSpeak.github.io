<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/jsmile.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/jsmile_32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/jsmile_16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/jsmile.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据,Spark,Hadoop,">





  <link rel="alternate" href="/atom.xml" title="平凡而诗意" type="application/atom+xml">






<meta name="description" content="Spark简介 Spark是由加州伯克利大学AMP实验室于2009年开发并于2013年加入Apache的开源大数据并行计算框架，它凭借自身独有的优势迅速成为Apache三大分布式计算框架之一，对比于常用的hadoop，它具有低时延、速度快、通用性强等优点。">
<meta name="keywords" content="大数据,Spark,Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据处理 | Spark&amp;HDFS集群配置及基本使用">
<meta property="og:url" content="https://jackpopc.github.io/2019/10/08/spark/index.html">
<meta property="og:site_name" content="平凡而诗意">
<meta property="og:description" content="Spark简介 Spark是由加州伯克利大学AMP实验室于2009年开发并于2013年加入Apache的开源大数据并行计算框架，它凭借自身独有的优势迅速成为Apache三大分布式计算框架之一，对比于常用的hadoop，它具有低时延、速度快、通用性强等优点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uhcnzV.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uhcMsU.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uhcKMT.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uhcQLF.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/08/uhc1Z4.png">
<meta property="og:updated_time" content="2019-10-08T14:22:49.460Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大数据处理 | Spark&amp;HDFS集群配置及基本使用">
<meta name="twitter:description" content="Spark简介 Spark是由加州伯克利大学AMP实验室于2009年开发并于2013年加入Apache的开源大数据并行计算框架，它凭借自身独有的优势迅速成为Apache三大分布式计算框架之一，对比于常用的hadoop，它具有低时延、速度快、通用性强等优点。">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/10/08/uhcnzV.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jackpopc.github.io/2019/10/08/spark/">





  <title>大数据处理 | Spark&HDFS集群配置及基本使用 | 平凡而诗意</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">平凡而诗意</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Jackpop</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://jackpopc.github.io/2019/10/08/spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jackpop">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/zhihu.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="平凡而诗意">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">大数据处理 | Spark&HDFS集群配置及基本使用</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-08T22:06:40+08:00">
                2019-10-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/IT技术/" itemprop="url" rel="index">
                    <span itemprop="name">IT技术</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/08/spark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/10/08/spark/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a><strong>Spark简介</strong></h2><p><img src="https://s2.ax1x.com/2019/10/08/uhcnzV.png" alt="uhcnzV.png"></p>
<p>Spark是由加州伯克利大学AMP实验室于2009年开发并于2013年加入Apache的开源大数据并行计算框架，它凭借自身独有的优势迅速成为Apache三大分布式计算框架之一，对比于常用的hadoop，它具有低时延、速度快、通用性强等优点。<a id="more"></a>此外，Spark具有完善的生态系统，在资源调度方面它拥有Mesos和YARN，在存储方面它支持本地文件系统、HDFS、Amazon S3、HBase等，在数据仓库方面它拥有Hive SQL、Spark SQL，在接口方面它拥有mlib、GraphX等。</p>
<p>除了运算和生态方面的优势，Spark在数据处理方式方面同时支持批计算和流计算，虽然Spark在流计算方面不如storm、flink能够支持毫秒级别，但是对于大多数对实时性要求不高的在线计算已经足够使用。</p>
<p>基于上述众多优点使得Spark成为一个非常热门和受欢迎的大数据处理框架，目前在很多大型公司被广泛使用。</p>
<p>Spark不仅可以支持集群模式，还可以支持单机模式，但是我认为之所以使用大数据处理框架，它的主要优势就体现在多机并行方面，随着数据集的增加和节点数量的增加，它的对比于传统并行模式和其他大数据处理框架的优势更加明显。单机Spark配置相对集群配置相对简单一些，也节省很多步骤，因此，本文就讲解一下集群Spark配置方式，本文的配置是建立在已经配制好JDK的基础上，所以不再详细介绍JDK的安装和配置。</p>
<h2 id="Hadoop集群环境搭建"><a href="#Hadoop集群环境搭建" class="headerlink" title="Hadoop集群环境搭建"></a><strong>Hadoop集群环境搭建</strong></h2><p><img src="https://s2.ax1x.com/2019/10/08/uhcMsU.png" alt="uhcMsU.png"></p>
<p>Spark可以读取多种数据源的数据，例如Amazon s3、HBase、HDFS、本地文件系统，由于数据存放在某一个节点路径下，在Spark集群的其他节点无法直接读取相应路径下的数据，而HBase、Amazon s3这些存储服务在很多场景下很难满足，例如学校实验室。因此本文就以HDFS为例来进行讲解。</p>
<p>Hadoop主要包括两个部门，HDFS文件存储系统用于存储数据源，MapReduce用于从文件存储系统读取数据并进行分布式处理，由于本文只用到文件存储系统HDFS，用不到MapReduce，所以本文就配置一下集群Hadoop，讲解HDFS的使用，不深入研究MapReduce的使用。</p>
<p><strong>准备工作</strong></p>
<p>首先要保证集群中不同节点能够互相通信，然后为每个节点配置对应的hostname，后面会用到，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/hosts</span><br><span class="line">10.110.113.132  master</span><br><span class="line">10.110.113.133  slave0</span><br><span class="line">10.110.113.134  slave1</span><br><span class="line">10.110.113.135  slave2</span><br></pre></td></tr></table></figure>
<p>上述master和slave是每个节点的hostname，可以作为IP的地带，通过ping的方式可以测试hostname是否正常通信，可以在master节点上测试是否连接到不同的slave节点，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ping slave0</span><br><span class="line">或者</span><br><span class="line">$ ping 10.110.113.133</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>hostname的配置可以通过sudo vim /etc/hostname修改文件进行配置。</p>
<p><strong>ssh无密码登陆集群机器</strong></p>
<p>由于集群配置hadoop涉及多台机器，当在master节点启动或者关闭集群hadoop时需要输入所有slave节点的密码，这样显然太麻烦，因此需要配置无密码登陆，这样后续启动时就不需要输入密码，</p>
<p>首先，如果节点没有安装ssh需要安装ssh，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install ssh</span><br></pre></td></tr></table></figure>
<p>然后，在每个节点上输入下面命令，测试是否能够正常登陆每个节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure>
<p>为了保证master节点能够无密码登陆所有slave节点，需要首先生成master节点的公钥，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen  -t  rsa</span><br></pre></td></tr></table></figure>
<p>一直点击确定即可，然后会在home路径下生成两个文件，id_rsa和id_rsa.pub，这时需要把id_rsa.pub的内容追加到authorized_keys后面，然后把master节点id_rsa.pub拷贝到所有slave节点并追加到所有slave节点authorized_keys的后面，</p>
<p>首先在master节点执行操作，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p>然后把master节点生成的id_rsa.pub拷贝到所有slave节点，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ scp ~/.ssh/id_rsa.pub user_name@slave0:/home/user_name/</span><br><span class="line">$ scp ~/.ssh/id_rsa.pub user_name@slave1:/home/user_name/</span><br><span class="line">$ scp ~/.ssh/id_rsa.pub user_name@slave2:/home/user_name/</span><br></pre></td></tr></table></figure>
<p>上述user_name是slave节点的用户名，然后把id_rsa.pub追加到每个slave节点authorized_keys的后面，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p>然后可以在master节点上通过下方命令测试是否能够正常登陆每个slave节点，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ssh slave0</span><br><span class="line">$ ssh slave1</span><br><span class="line">$ ssh slave2</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>上述都是假设master和每个slave节点的用户名user_name一样的前提下，如果不一样需要修改~/.ssh/config文件。</p>
<p><strong>安装Hadoop</strong></p>
<p><img src="https://s2.ax1x.com/2019/10/08/uhcKMT.png" alt="uhcKMT.png"></p>
<p>打开下面链接进入到下载页面，点击下载binary文件，把hadoop-3.2.1.tar.gz文件下载到home路径下，</p>
<p><a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener">https://hadoop.apache.org/releases.html</a></p>
<p>然后解压下载的文件到指定目录，同时需要修改对应目录的拥有者，因为hadoop在启动后会记录日志文件，如果不修改拥有者则没有权限写入文件，无法正常启动，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar -zxvf ~/下载/hadoop-3.2.1.tar.gz -C /usr/local  </span><br><span class="line">$ cd /usr/local/</span><br><span class="line">$ sudo mv ./hadoop-3.2.1/ ./hadoop          </span><br><span class="line">$ sudo chown -R user_name ./hadoop</span><br></pre></td></tr></table></figure>
<p>然后，把hadoop路径加入到环境变量里，如果需要长期有效，需要修改~/.bashrc，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<p>然后保存退出，执行下面命令让环境变量生效，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><strong>集群配置</strong></p>
<p>首先进入到Hadoop配置文件所在目录，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>
<p>然后修改slave节点配置文件，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vim slaves</span><br><span class="line">slave0</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>上述slave0~2不是IP地址，是前面配置的hostname。</p>
<p>首先配置core-site.xml，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>上述内容只需要修改<value>hdfs://master:9000</value>这一句即可，其他的不需要修改，需要根据自己定义的master节点的hostname进行修改，例如你的master节点的hostname是hadoop，那么就需要修改成<value>hdfs://hadoop:9000</value>，端口默认为9000，</p>
<p>然后配置hdfs-site.xml，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>这里dfs.replication默认值是3，它的含义是数据副本备份的数量，如果slave节点数量小于这个数的话则会报错，所以为了防止报错可以把这个值改为1，当然如果slave节点大于等于3的话也可以不修改。</p>
<p>如果需要使用MapReduce还需要配置mapred-site.xml，因为本文只使用HDFS而不实用MapReduce，因此就不配置mapred-site.xml了。</p>
<p>另外，如果需要yarn资源调度的话，需要修改yarn-site.xml，yarn是一个资源调度和分配工具，Spark本身自带资源管理器，也可以与yarn、Mesos结合使用，本文就使用Spark自带的资源管理器。</p>
<p>最后，需要在hadoop中配置JAVA环境，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/path/to/java</span><br></pre></td></tr></table></figure>
<p>上述/path/to/java需要替换成自己JAVA安装的路径。</p>
<p><strong>slave节点配置</strong></p>
<p>上述全部操作都是在master节点进行的，下面需要配置slave节点。配置slave节点比较容易，只需要把文件打包复制到各个slave节点，解压即可，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zcf ~/hadoop.tar.gz /usr/local/hadoop</span><br><span class="line">$ scp ~/hadoop.tar.gz slave0:~/</span><br><span class="line">$ scp ~/hadoop.tar.gz slave1:~/</span><br><span class="line">$ scp ~/hadoop.tar.gz slave2:~/</span><br></pre></td></tr></table></figure>
<p>然后在各个slave节点上都执行下面相同操作，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rm -rf /usr/local/hadoop</span><br><span class="line">$ sudo tar -zxvf ~/hadoop.tar.gz -C /usr/local</span><br><span class="line">$ sudo chown -R user_name /usr/local/hadoop</span><br></pre></td></tr></table></figure>
<p><strong>启动Hadoop集群</strong></p>
<p>通过上述的配置，master节点和slave节点的Hadoop都配置好了，下面需要做的就是启动集群上每个节点的Hadoop，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/hadoop</span><br><span class="line">$ ./bin/hdfs namenode -format</span><br><span class="line">$ ./sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>./bin/hdfs namenode -format这一句命令很重要，不能缺少。如果前面没有配置ssh免密登陆，执行./sbin/start-all.sh时会让输入密码。</p>
<p>然后在每个节点上执行下面命令，会在master节点上看到多了一个NameNode，slave节点上会多出DataNode，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br></pre></td></tr></table></figure>
<h2 id="HDFS简单使用"><a href="#HDFS简单使用" class="headerlink" title="HDFS简单使用"></a><strong>HDFS简单使用</strong></h2><p>HDFS的使用和Linux命令非常相似，例如上传数据用put，创建目录用mkdir，查看目录内容用ls，删除目录用rm，但是也有不同之处，下面就来看一下简单的示例，</p>
<p><strong>创建目录</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/hdfs dfs -mkdir -p /hello</span><br></pre></td></tr></table></figure>
<p><strong>上传文件到HDFS</strong></p>
<p>首先先新建一个本地文件，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/test.txt</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure>
<p>然后使用下面命令进行上传，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/hdfs dfs -put ~/test.txt /hello</span><br></pre></td></tr></table></figure>
<p><strong>查看目录和文件内容</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/hdfs dfs -ls /hello</span><br><span class="line">/hello/test.txt</span><br><span class="line">$ ./bin/hdfs dfs -cat /hello/test.txt</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure>
<p>然后我们就可以用Spark访问hdfs文件系统的文件，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_file = sc.textFile(&quot;hdfs://master:9000/hello/text.txt&quot;)</span><br></pre></td></tr></table></figure>
<p>文件的路径分为两部分，一部分是前面core-site.xml中配置的hostname:port，一部分是HDFS上传文件的相对路径。</p>
<h2 id="Spark集群环境搭建"><a href="#Spark集群环境搭建" class="headerlink" title="Spark集群环境搭建"></a><strong>Spark集群环境搭建</strong></h2><p><img src="https://s2.ax1x.com/2019/10/08/uhcQLF.png" alt="uhcQLF.png"></p>
<p>如果已经理解了上述Hadoop集群环境的搭建，那么学习Spark集群环境的搭建会容易很多，因为Hadoop和Spark不仅安装包目录结构非常相似，在配置方面也十分接近。均是在master节点上进行所有配置，然后打包复制到每个slave节点，然后启动集群Spark即可，下面就来详细介绍一下Spark集群环境的搭建。</p>
<p><strong>下载安装</strong></p>
<p><img src="https://s2.ax1x.com/2019/10/08/uhc1Z4.png" alt="uhc1Z4.png"></p>
<p>进入Spark的下载目录，</p>
<p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">https://spark.apache.org/downloads.html</a></p>
<p>可以看到Spark分多个版本，有基于Hadoop构建好的，有没基于Hadoop构建的，有基于Hadoop2.6之前版本构建的，也有基于Hadoop2.7以后版本构建的，由于前面讲解Hadoop集群环境搭建时采用的是Hadoop 3.2.1，因此，而且本文需要使用HDFS依赖Hadoop，因此需要下载<strong>Pre-built</strong> <strong>for</strong> <strong>Apache Hadoop 2.7 and later</strong>,</p>
<p>把spark-2.4.4-bin-hadoop2.7.tgz文件下载到home路径下，然后解压到指定目录，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf ~/spark-2.4.4-bin-hadoop2.7.tgz -C /usr/local/</span><br></pre></td></tr></table></figure>
<p>然后进入目录并像Hadoop那样，修改Spark目录的拥有者，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local</span><br><span class="line">$ sudo mv ./spark-2.4.4-bin-hadoop2.7 ./spark</span><br><span class="line">$ sudo chowm -R user_name ./spark</span><br></pre></td></tr></table></figure>
<p><strong>配置环境变量</strong></p>
<p>修改bashrc，配置环境变量，把Spark的bin和sbin路径加入到环境变量，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bashrc</span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br><span class="line">export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH</span><br><span class="line">export PYSPARK_PYTHON=python3</span><br></pre></td></tr></table></figure>
<p><strong>Master节点配置</strong></p>
<p>进入Spark目录，修改spark-env.sh文件，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/spark</span><br><span class="line">$ vim ./conf/spark-env.sh</span><br></pre></td></tr></table></figure>
<p>在spark-env.sh中添加下面内容，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</span><br><span class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">export  SPARK_MASTER_IP=10.110.113.132</span><br></pre></td></tr></table></figure>
<p>SPARK_MASTER_IP指定的是master节点的IP，后面启动集群Spark时slave节点会注册到SPARK_MASTER_IP，如果这一项不配置，Spark集群则没有可使用资源，</p>
<p><strong>修改slaves文件</strong></p>
<p>配置完master节点信息之后需要配置slave节点信息，slave节点的信息配置在slaves文件里，由于Spark目录下没有这个文件，因此需要首先从slaves.template拷贝一下，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/spark/</span><br><span class="line">$ cp ./conf/slaves.template ./conf/slaves</span><br></pre></td></tr></table></figure>
<p>然后添加如下内容，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">slave0</span><br><span class="line">slave0</span><br><span class="line">slave1</span><br></pre></td></tr></table></figure>
<p>需要注意的是，slaves文件里配置的是运行作业任务的节点(worker)，这样的话master的节点只作为控制节点，而不作为工作节点，如果需要把master节点的资源也充分利用起来，需要把master节点也加入到slaves文件中。</p>
<p><strong>slave节点配置</strong></p>
<p>首先在master节点上把配制好的目录进行打包，拷贝到每个slave节点上，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local</span><br><span class="line">$ tar -zcf ~/spar.tar.gz ./spark</span><br><span class="line">$ scp ~/spark/tar.gz slave0:~/</span><br><span class="line">$ scp ~/spark/tar.gz slave1:~/</span><br><span class="line">$ scp ~/spark/tar.gz slave2:~/</span><br></pre></td></tr></table></figure>
<p>然后在每个slave节点上执行下方命令，把文件解压到相应路径下，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rm -rf /usr/local/spark</span><br><span class="line">$ sudo tar -zxvf ~/spark.tar.gz -C /usr/local</span><br><span class="line">$ sudo chown -R user_name /usr/local/spark</span><br></pre></td></tr></table></figure>
<p>这样就完成了slave节点的配置。</p>
<p><strong>启动Spark集群</strong></p>
<p>如果要使用HDFS的话，在启动Spark集群前需要先启动Hadoop集群，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/hadoop/</span><br><span class="line">$ ./sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>然后进入Spark目录，启动Spark集群，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/spark</span><br><span class="line">$ ./sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>需要说明一下，前面配置Hadoop集群是提到，需要配置ssh免密登陆，对于Spark也是同样的道理，如果不配置ssh免密登陆的话，执行./sbin/start-all.sh会提示输入密码。</p>
<p>除了使用./sbin/start-all.sh启动Spark集群外，还可以分开启动，先启动master节点，然后启动slave节点，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-master.sh</span><br><span class="line">$ ./sbin/start-slaves.sh</span><br></pre></td></tr></table></figure>
<p>如果前面没有完成<strong>Master节点配置</strong>指定master节点IP，那么执行./sbin/start-slaves.sh时则无法注册master节点的IP，这样集群计算资源则无法使用。除了配置spark-env.sh指定master节点IP外，还可以通过下面方式指定注册的master节点IP，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./sbin/start-slave.sh 10.110.113.132</span><br></pre></td></tr></table></figure>
<p>然后分别在master节点和slave节点执行下面命令会看到分别多出一个Master进程和Worker进程。</p>
<h2 id="Spark基本使用"><a href="#Spark基本使用" class="headerlink" title="Spark基本使用"></a><strong>Spark基本使用</strong></h2><p><strong>运行原理</strong></p>
<p>如果使用过tensorflow的话，应该对Spark的使用很容易理解，Spark的计算过程和tensorflow有相似之处。</p>
<p>回忆一下，我们在使用tensorflow时需要首先构造一个计算图，然后实例化一个session，然后用session.run来启动图运算。</p>
<p>其实Spark也是这样，RDD(弹性分布式数据集)是Spark中最重要的概念之一，它提供了一个共享内存模型。Saprk的执行过程中主要包括两个动作：转换与行动。其中转换操作就如同tensorflow中的构造计算图的过程，在这个过程中Spark构造一个有向无环图(DAG)，但是不进行运算，输入为RDD输出则是一个不同的RDD，当执行行动操作时就如同tensorflow中的session.run，开始执行运算。</p>
<p>Spark中有很多转换操作，例如，</p>
<ul>
<li>groupByKey</li>
<li>reduceByKey</li>
<li>sortByKey</li>
<li>map</li>
<li>filter</li>
<li>join</li>
<li>……</li>
</ul>
<p>行动操作包括，</p>
<ul>
<li>count</li>
<li>collect</li>
<li>first</li>
<li>foreach</li>
<li>reduce</li>
<li>take</li>
<li>……</li>
</ul>
<p><strong>运行模式</strong></p>
<p>Spark中通过master url来执行Spark的运行模式，Spark的运行模式包括本地运行、集群运行、yarn集群等，关于Spark master url的指定不同运行模式的含义如下，</p>
<p><strong>URL值运行模式</strong>local使用1个线程本地化运行local[K]使用K个线程本地化运行local[*]使用逻辑CPU个数数量的线程来本地化运行spark://HOST:PORT指定集群模式运行Sparkyarn-cluster集群模式连接YARN集群yarn-client客户端模式连接YARN集群mesos://HOST:PORT连接到指定的Mesos集群</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h2><p>下面就以一个简单的示例把前面Hadoop和Spark串联在一起，讲解一下HDFS+Spark的使用方法。</p>
<p><strong>上传数据到HDFS</strong></p>
<p>新建一个hello_world.txt的本地文件，并在文件中添加3行hello world，然后上传至HDFS，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/hadoop/</span><br><span class="line">$ ./bin/hdfs dfs -mkdir -p /usr/hadoop</span><br><span class="line">$ touch hello_world.txt</span><br><span class="line">$ echo -e &quot;hello world \nhello world \nhello world&quot; &gt;&gt; hello_world.txt</span><br><span class="line">$ ./bin/hdfs dfs -put ./hello_world.txt /usr/hadoop</span><br></pre></td></tr></table></figure>
<p><strong>编写Spark程序</strong></p>
<p>新建一个spark.py的Python文件，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim spark.py</span><br></pre></td></tr></table></figure>
<p>添加如下内容，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark import SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(&quot;FirstProject&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">sc = SparkContext.getOrCreate(conf)</span><br><span class="line">rdd = sc.textFile(&quot;hdfs:///master:9000/usr/hadoop/hello_world.txt&quot;)</span><br><span class="line">rdd.map(lambda line: line).foreach(print)</span><br></pre></td></tr></table></figure>
<p>然后运行程序，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python spark.py</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure>
<p>以上就是Spark的集群配置过程和基本使用方法。</p>
<hr>
<h2 id="往期内容"><a href="#往期内容" class="headerlink" title="往期内容"></a>往期内容</h2><p><a href="https://jackpopc.github.io/2019/09/14/jupyter/#more">开发工具 | 你真的会用jupyter吗？</a></p>
<p><a href="https://jackpopc.github.io/2019/09/01/cnn-dropout/">【动手学计算机视觉】第十四讲：正则化之Dropout</a></p>
<p><a href="https://jackpopc.github.io/2019/09/13/lenet/">【动手学计算机视觉】第十五讲：卷积神经网络之LeNet</a></p>
<blockquote>
<p>更多精彩内容，请关注公众号【平凡而诗意】，或者收藏我的个人主页~</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
            <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/08/generator-iterator/" rel="next" title="【进阶Python】第五讲：迭代器与生成器">
                <i class="fa fa-chevron-left"></i> 【进阶Python】第五讲：迭代器与生成器
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/11/screenshot/" rel="prev" title="实用工具 | 推荐3款令人惊艳的截图工具">
                实用工具 | 推荐3款令人惊艳的截图工具 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/zhihu.jpg" alt="Jackpop">
            
              <p class="site-author-name" itemprop="name">Jackpop</p>
              <p class="site-description motion-element" itemprop="description">原创技术分享网站</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">48</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Jackpopc" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:498073774@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/sharetechlee" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-book"></i>知乎</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://zhuanlan.zhihu.com/sharetechlee" target="_blank" title="专栏">
                      
                        <i class="fa fa-fw fa-edit"></i>专栏</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark简介"><span class="nav-number">1.</span> <span class="nav-text">Spark简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop集群环境搭建"><span class="nav-number">2.</span> <span class="nav-text">Hadoop集群环境搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS简单使用"><span class="nav-number">3.</span> <span class="nav-text">HDFS简单使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark集群环境搭建"><span class="nav-number">4.</span> <span class="nav-text">Spark集群环境搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark基本使用"><span class="nav-number">5.</span> <span class="nav-text">Spark基本使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#示例"><span class="nav-number">6.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#往期内容"><span class="nav-number">7.</span> <span class="nav-text">往期内容</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jackpop</span>

  
</div>





        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'U1oIMCJI4zYogWXqIKva7Fsa-gzGzoHsz',
        appKey: 'qJvgYGbG712q47zzMmQ5z9XF',
        placeholder: '请在此输入您的留言',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  


  

  

</body>
</html>
