<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[开发工具 | 即将取代jupyter的新一代notebook]]></title>
    <url>%2F2019%2F11%2F18%2Fpolynote%2F</url>
    <content type="text"><![CDATA[前言 我们在实验室、教学、数据分析及开发测试环境，经常需要一个用于测试代码片段的工具。其中用的较多的，尤其是对于Python开发者而言，包括两款开发工具， jupyter notebook jupyterlab 不得不说，这两款都是非常不错的工具，它们有着很多显而易见的优点， 轻量化 触手可及 支持丰富的插件 支持语言丰富 …… 对于我个人而言，我使用jupyter notebook的频率的确很多。在工作中有时候需要测试一段代码，如果在shell中打开Python交互界面，对于多行代码格式会非常混乱，而且调试不方便，而打开Pycharm这些编辑器/IDE又比较耗费时间、繁琐。而使用jupyter notebook或者jupyterlab这些基于浏览器的notebook，效率就高很多，随时随地能够打开、支持交互式调试、支持富文本等。 但是，这次jupyter遇到了劲敌，Netflix开源的PolyNote，没错，就是那个影音娱乐领域的奈飞，虽然看似和开发没什么太大关系，但是，它们开源的这款工具却着实的不错，开源不久在Github已经达到3k+star。 由于这款工具开源时间相对较短，目前比起成熟的jupyter还存在很多不足之处，但是它既然受到这么多的关注，自然就有它的独特之处，下面我就来介绍一下这款开发工具的特点及安装方法，如果对新鲜事物比较感兴趣，可以尝试一下。 PolyNote开门见山，直接介绍PolyNote的10大特性。 第一大特性：支持多语言 PolyNote是一款与众不同的开发工具，我个人认为，它是一款为数据而生的开发工具，默认支持如下3种语言， Python SQL Scala 这也是PolyNote的第一大特性：支持多语言。 另外，通过简单的配置即可支持Spark，从它原生支持的工具即可看出，它主要都是在围绕大数据在展开，其中包括大数据开发和数据可视化。 第二大特性：支持共享数据 目前PolyNote支持Python、Scala等语言，它可以实现不同语言之间的数据共享。例如，我在一个Python的cell中定义一个变量，然后添加一个Scala的cell，它可以访问Python中定义的变量。 第三大特性：自动补全 我觉得这是jupyter的一大弱点，虽然我们可以敲击tab键进行补全，但是这显然不够便捷，虽然可以通过配置插件的方式让jupyter具有自动补全的功能，但是，效果差强人意，只能勉强使用，效果不太理想。而PolyNote默认支持代码自动补全，而且效果丝毫不亚于常见的IDE。 第四大特性：高亮代码错误 这也是jupyter的一个不足之处，它不能高亮代码错误的地方，这在vs code、pycharm中可以实现这项功能，但是软件太过于臃肿，PolyNote把高亮代码错误这项功能也加入了进去，让PolyNote成为一个更加成熟、完善的IDE。 第五大特性：富文本编辑 虽然jupyter也支持富文本，但是，在PolyNote功能栏有丰富的文本编辑选项，让它更像一个完备的文本编辑器。此外，能够轻松地将LaTeX公式插入文本单元格。 第六大特性：实时状态显示 能够实时显示内核的工作状态和工作内容。此外，能够准确地查看当前正在执行的代码，因为Polynote实时地高亮显示正在运行的语句。 第七大特性：详细的可视化 Polynote实现实时跟踪定义的内容，以及当前单元格中可用的内容。能够在表格中显示，可以获得详细信息和丰富的可视化效果。 第八大特性：高度集成Spark 和大数据处理框架Spark高度集成，更加便于数据的分析及可视化。 第九大特性：可重复性 能够确保notebook重复执行，哪怕是某个cell被删除，也可以轻松恢复进行再次执行。 第十大特性：可视化效果好 PolyNote内置了绘图编辑器，可以轻松的进行数据可视化，另外，它也默认支持Vega可视化语言，同样非常方便与可视化。 PolyNote配置首先需要说明，目前PolyNote仅在Linux和MacOS上进行了测试，暂时不支持windows，自己有Linux或者MacOS开发环境，可以继续往下看一下配置过程。 PolyNote还没有jupyter notebook和jupyterlab那么成熟，因此，它不支持像jupyter那样能够直接通过pip install进行安装，但是它的配置过程也非常容易。 安装JDK PolyNote的运行需要Java环境的支持，目前PolyNote已经在Java 8和Java 11上进行成功测试，如果其他JDK版本不成功，可以切换一下Java版本。 安装Python依赖包 为了执行Python，需要配置Python3.X和pip3.x，然后执行下面命令安装一些依赖包， 1$ pip3 install jep jedi pyspark virtualenv 下载安装包 打开下面链接，下载PolyNote压缩包， https://github.com/polynote/polynote/releases 解压压缩包，进入目录， 12$ tar -zxvf polynote-dist-2.12.tar.gz$ cd polynote 配置 进入目录后会发现有一个config-template.yml文件，需要把它进行重命名， 1$ cp config-template.yml config.yml config.yml中可以配置打开的IP和端口，打开文件可以看到这段文字， 123#listen:# host: 127.0.0.1# port: 8192 默认打开的IP是127.0.0.1，端口为8192，我们可以把这个IP修改为服务器或者开发机的IP，那么可以在任何其他与该IP互通的机器上进行使用。 开启服务 执行下面命令开启PolyNote服务， 1$ python polynote.py 如果觉得这样会占用shell，可以执行下面命令，让PolyNote在后台运行， 1$ nohup python polynote.py &gt; /dev/null 2&gt;&amp;1 &amp;]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十八讲：卷积神经网络之GoogLeNet]]></title>
    <url>%2F2019%2F11%2F17%2F18-GoogLeNet%2F</url>
    <content type="text"><![CDATA[前言在前一篇文章介绍VGG时，我提到2014年对于计算机视觉领域是一个丰收的一年，在这一年的ImageNet图像识别挑战赛(ILSVRC,ImageNet Large Scale Visual Recognition Challenge)中出现了两个经典、影响至深的卷积神经网络模型，其中第一名是GoogLeNet、第二名是VGG。 没错，本文的主角就是2014年ILSVRC的第一名—GoogLeNet(Going Deeper with Convolutions)，要注意的是，这个网络模型的名称是”GoogLeNet”，而不是”GoogleNet”，虽然只有一个大小写字母的却别，含义却不同，GoogLeNet之所以叫做这个名字，主要是为了想LeNet致敬。 GoogLeNet与VGG出现在同一年，二者自然有一些相似之处，但是两个模型更多的是差异性。 首先说一下GoogLeNet与VGG的相同之处： 都提出了基础块的思想 均是为了克服网络逐渐变深带来的问题 首先，说一下第一点—都提出了基础块的思想。 前文已经介绍了，VGG使用块代替层的思想，这使得VGG在迁移性方面表现非常好，也因此得到了广泛的应用。而GoogLeNet也使用了基础块的思想，它引入了Inception块，想必说到这里应该接触过深度计算机视觉的同学应该恍然大悟，也许对GoogLeNet的概念已经变的模糊，但是Inception却如雷贯耳，目前在很多CNN模型中同样作为基础模块使用。 其次，说一下第二点—均是为了克服网络逐渐变深带来的问题。 随着卷积神经网络模型的更新换代，我们发现网络层数逐渐变多，模型变的越来越深，这是因为提升模型效果最为直接有效的方法就是增加网络深度和宽度，但是，随着网络层数的加深、加宽，它也会带来很多负面影响， 参数数量增加 梯度消失和梯度爆炸 计算复杂度增加 因此，从VGG、GoogLeNet开始，包括后面会讲到的ResNet，研究者逐渐把目光聚焦在”如何在增加网络深度和宽度的同时，避免上述这些弊端？” 不同的网络模型所采取的方式不同，这也就引出了VGG与GoogLe的不同之处， 输出层不同 克服网络加深弊端的方式不同 首先，说一下第一点—输出层不同， VGG是在LeNet、AlexNet的基础上引入了基础块的思想，但是在网络架构、输出等放并没有进行太多的改变，在输出层方面同样是采用连续三个全连接层，全连接层的输入是前面卷积层的输出经过reshape得到。 虽然GoogLeNet是向LeNet致敬，但是在GoogLeNet的身上却很难看到LeNet和AlexNet的影子，它的输出更是采用NiN的思想(Network in Network)，它把全连接层编程了1*1的卷积层。 其次，说一下第二点—克服网络加深弊端的方式不同， VGG在克服网络加深带来的问题方面采用的是引入基础块的思想，但是整体上还是偏向于”更深”，而GoogLeNet更加偏重于”更宽”，它引入了并行网络结构的思想，每一层有4个不同的线路对输入进行处理，然后再块的输出部分在沿着通道维进行连接。 GoogLeNet通过对模型的大幅度改进，使得它在参数数量、计算资源方面要明显优于VGG，但是GoogLeNet的模型复杂度相对于VGG也要高一些，因此，在迁移性方面VGG要优于GoogLeNet。 GoogLeNet模型Inception块是GoogLeNet模型中一个非常重要的组成部分，因此，在介绍完整的GoogLeNet模型之前，我先来讲解一下Inception块的结构。 Inception块 上图就是就是Inception的结构，Inception分为两个版本： 简化版 降维版 二者主要的区别就在于1*1的卷积层，降维版在第2、3、4条线路添加了1*1的卷积层来减少通道维度，以减小模型复杂度，本文就以降维版为例来讲解GoogLeNet。 现在来看一下Inception的结构，可以很清楚的看出，它包含4条并行线路，其中，第1、2、3条线路分别采用了1*1、3*3、5*5，不同的卷积核大小来对输入图像进行特征提取，使用不同大小卷积核能够充分提取图像特征。其中，第2、3两条线路都加入了1*1的卷积层，这里要明确一点，第2、3两条线路的1*1与第1条线路1*1的卷积层的功能不同，第1条线路是用于特征提取，而第2、3条线路的目的是降低模型复杂度。第4条线路采用的不是卷积层，而是3*3的池化层。最后，4条线路通过适当的填充，使得每一条线路输出的宽和高一致，然后经过Filter Concatenation把4条线路的输出在通道维进行连接。 上述就是Inception块的介绍，在GoogLeNet模型中，Inception块会被多次用到，下面就开始介绍GoogLeNet的完整模型结构。 GoogLeNet GoogLeNet在网络模型方面与AlexNet、VGG还是有一些相通之处的，它们的主要相通之处就体现在卷积部分， AlexNet采用5个卷积层 VGG把5个卷积层替换成5个卷积块 GoogLeNet采用5个不同的模块组成主体卷积部分 上述就是GoogLeNet的结构，可以看出，和AlexNet统一使用5个卷积层、VGG统一使用5个卷积块不同，GoogLeNet在主体卷积部分是卷积层与Inception块混合使用。另外，需要注意一下，在输出层GoogleNet采用全局平均池化，得到的是高和宽均为1的卷积层，而不是通过reshape得到的全连接层。 下面就来详细介绍一下GoogLeNet的模型结构。 模块1 第一个模块采用的是一个单纯的卷积层紧跟一个最大池化层。 卷积层：卷积核大小7*7，步长为2，输出通道数64。 池化层：窗口大小3*3，步长为2，输出通道数64。 模块2 第二个模块采用2个卷积层，后面跟一个最大池化层。 卷积层：卷积核大小3*3，步长为1，输出通道数192。 池化层：窗口大小3*3，步长为2，输出通道数192。 模块3 第三个模块采用的是2个串联的Inception块，后面跟一个最大池化层。 第一个Inception的4条线路输出的通道数分别是64、128、32、32，输出的总通道数是4条线路的加和，为256。 第二个Inception的4条线路输出的通道数分别是128、192、96、64，输出的总通道数为480。 池化层：窗口大小3*3，步长为2，输出通道数480。 模块4 第4个模块采用的是5个串联的Inception块，后面跟一个最大池化层。 第一个Inception的4条线路输出的通道数分别是192、208、48、64，输出的总通道数为512。 第二个Inception的4条线路输出的通道数分别是160、224、64、64，输出的总通道数为512。 第三个Inception的4条线路输出的通道数分别是128、256、64、64，输出的总通道数为512。 第四个Inception的4条线路输出的通道数分别是112、288、64、64，输出的总通道数为528。 第五个Inception的4条线路输出的通道数分别是256、320、128、128，输出的总通道数为832。 池化层：窗口大小3*3，步长为2，输出通道数832。 模块5 第五个模块采用的是2个串联的Inception块。 输出层 前面已经多次提到，在输出层GoogLeNet与AlexNet、VGG采用3个连续的全连接层不同，GoogLeNet采用的是全局平均池化层，得到的是高和宽均为1的卷积层，然后添加丢弃概率为40%的Dropout，输出层激活函数采用的是softmax。 激活函数 GoogLeNet每层使用的激活函数为ReLU激活函数。 编程实践当我们拿到一个需求的时候，应该先对它进行一下分析、分解，针对GoogLeNet，我们通过分析可以把它分解成如下几个模块， Inception块 卷积层 池化层 线性层 通过上述分解，我们逐个来实现上述每个模块。 Inception块 前面讲解过程中已经详细介绍Inception块的结构，它包括4条线路，而对于Inception块最重要的参数就是每个线路输出的通道数，由于其中步长、填充方式、卷积核大小都是固定的，因此不需要我们进行传参。我们把4条线路中每层的输出通道数作为Inception块的入参，具体实现过程如下， 123456789101112131415161718192021def inception_block(X, c1, c2, c3, c4, name): in_channels = int(X.get_shape()[-1]) # 线路1 with tf.variable_scope('conv1X1_&#123;&#125;'.format(name)) as scope: weight = tf.get_variable("weight", [1, 1, in_channels, c1]) bias = tf.get_variable("bias", [c1]) p1_1 = tf.nn.conv2d(X, weight, strides=[1, 1, 1, 1], padding="SAME") p1_1 = tf.nn.relu(tf.nn.bias_add(p1_1, bias)) # 线路2 with tf.variable_scope('conv2X1_&#123;&#125;'.format(name)) as scope: weight = tf.get_variable("weight", [1, 1, in_channels, c2[0]]) bias = tf.get_variable("bias", [c2[0]]) p2_1 = tf.nn.conv2d(X, weight, strides=[1, 1, 1, 1], padding="SAME") p2_1 = tf.nn.relu(tf.nn.bias_add(p2_1, bias)) p2_shape = int(p2_1.get_shape()[-1]) with tf.variable_scope('conv2X2_&#123;&#125;'.format(name)) as scope: weight = tf.get_variable("weight", [3, 3, p2_shape, c2[1]]) bias = tf.get_variable("bias", [c2[1]]) p2_2 = tf.nn.conv2d(p2_1, weight, strides=[1, 1, 1, 1], padding="SAME") p2_2 = tf.nn.relu(tf.nn.bias_add(p2_2, bias)) 卷积及池化 在GoogLeNet中多处用到了卷积层和最大池化层，这些结构在AlexNet中都已经实现过，我们直接拿过来使用即可， 1234567891011def conv_layer(self, X, ksize, out_filters, stride, name): in_filters = int(X.get_shape()[-1]) with tf.variable_scope(name) as scope: weight = tf.get_variable("weight", [ksize, ksize, in_filters, out_filters]) bias = tf.get_variable("bias", [out_filters]) conv = tf.nn.conv2d(X, weight, strides=[1, stride, stride, 1], padding="SAME") activation = tf.nn.relu(tf.nn.bias_add(conv, bias)) return activationdef pool_layer(self, X, ksize, stride): return tf.nn.max_pool(X, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding="SAME") 线性层 GoogLeNet与AlexNet、VGG在输出层不同，AlexNet和VGG是通过连续的全连接层处理，然后输入到激活函数即可，而GoogLeNet需要进行全局平均池化后进行一次线性映射，对于这一点实现过程如下， 1234567def linear(self, X, out_filters, name): in_filters = X.get_shape()[-1] with tf.variable_scope(name) as scope: w_fc = tf.get_variable("weight", shape=[in_filters, out_filters]) b_fc = tf.get_variable("bias", shape=[out_filters], trainable=True) fc = tf.nn.xw_plus_b(X, w_fc, b_fc) return tf.nn.relu(fc) 搭建模型 上面几步已经把GoogLeNet主要使用的组件已经搭建完成，接下来要做的就是把它们组合到一起即可。这里需要注意一点，全局平均池化层的填充方式和前面卷积层、池化层使用的不同，这里需要使用VALID填充方式， 1234567891011121314151617181920212223242526272829303132def create(self, X): # 模块1 module1_1 = self.conv_layer(X, 7, 64, 2, "module1_1") pool_layer1 = self.pool_layer(module1_1, 3, 2) # 模块2 module2_1 = self.conv_layer(pool_layer1, 1, 64, 1, "modul2_1") module2_2 = self.conv_layer(module2_1, 3, 192, 1, "module2_2") pool_layer2 = self.pool_layer(module2_2, 3, 2) # 模块3 module3a = self.inception_block(pool_layer2, 64, (96, 128), (16, 32), 32, "3a") module3b = self.inception_block(module3a, 128, (128, 192), (32, 96), 64, "3b") pool_layer3 = self.pool_layer(module3b, 3, 2) # 模块4 module4a = self.inception_block(pool_layer3, 192, (96, 208), (16, 48), 64, "4a") module4b = self.inception_block(module4a, 160, (112, 224), (24, 64), 64, "4b") module4c = self.inception_block(module4b, 128, (128, 256), (24, 64), 64, "4c") module4d = self.inception_block(module4c, 112, (144, 288), (32, 64), 64, "4d") module4e = self.inception_block(module4d, 256, (160, 320), (32, 128), 128, "4e") pool_layer4 = self.pool_layer(module4e, 3, 2) # 模块5 module5a = self.inception_block(pool_layer4, 256, (160, 320), (32, 128), 128, "5a") module5b = self.inception_block(module5a, 384, (192, 384), (48, 128), 128, "5b") pool_layer5 = tf.nn.avg_pool(module5b, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding="VALID") flatten = tf.reshape(pool_layer5, [-1, 1024]) dropout = tf.nn.dropout(flatten, keep_prob=self.keep_prob) linear = self.linear(dropout, self.num_classes, 'linear') return tf.nn.softmax(linear) 验证 为了验证每一个模块输出的形状和原文中给出的是否一致，我使用numpy，生成了样本数为5的随机样本，看一下每一层的输出结果， 1234567891011121314151617181920212223242526272829303132def main(): X = np.random.normal(size=(5, 224, 224, 3)) images = tf.placeholder("float", [5, 224, 224, 3]) googlenet = GoogLeNet(1000, 0.4) writer = tf.summary.FileWriter("logs") with tf.Session() as sess: model = googlenet.create(images) sess.run(tf.global_variables_initializer()) writer.add_graph(sess.graph) prob = sess.run(model, feed_dict=&#123;images: X&#125;) print(sess.run(tf.argmax(prob, 1))) # 输出module1_1: (5, 112, 112, 64)pool_layer1: (5, 56, 56, 64)module2_1: (5, 56, 56, 64)module2_2: (5, 56, 56, 192)pool_layer2: (5, 28, 28, 192)module3a: (5, 28, 28, 256)module3b: (5, 28, 28, 480)pool_layer3: (5, 14, 14, 480)module4a: (5, 14, 14, 512)module4b: (5, 14, 14, 512)module4c: (5, 14, 14, 512)module4d: (5, 14, 14, 528)module4e: (5, 14, 14, 832)pool_layer4: (5, 7, 7, 832)module5a: (5, 7, 7, 832)module5b: (5, 7, 7, 1024)pool_layer5: (5, 1, 1, 1024)flatten: (5, 1024)linear: (5, 1000) 可以从上述输出可以看出，每一层的输出形状和原文中给出的一致，至于在不同场景、不同数据集下的表现效果，这需要针对性的进行调优。 链接本文完整代码 https://github.com/Jackpopc/aiLearnNotes/blob/master/computer_vision/GoogLeNet.py Going Deeper with Convolutions http://www.arxiv.org/pdf/1409.4842.pdf Network In Network http://arxiv.org/pdf/1312.4400 我把本讲文档已经上传到github，如果需要可以搜索github项目aiLearnNotes查看。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 一款基于深度学习的代码自动补全神器]]></title>
    <url>%2F2019%2F11%2F16%2Ftabnine%2F</url>
    <content type="text"><![CDATA[前言代码补全对于大多数开发人员来说是至关重要的，它可以有效的提高开发效率、减少拼写错误和输入代码量。我们使用的大多数开发工具都自带补全功能，或者可以通过安装插件具备补全功能。但是，以往的代码补全功能主要基于语言本身的内置函数和上下文信息进行补全，而在候选项重要程度、代码块补全方面却不理想。与其说我们常用的自动补全工具的价值在补全方面，我更倾向于认为它的价值体现在提示。 近几年随着深度学习的抬头、火热，深度神经网络在学术领域有着很多最为先进的算法，并且在很多商业领域都得到应用。因此，有想法的研究者也尝试把深度学习引入开发工具中。 在之前的一篇文章中，我介绍了一款人工智能自动补全工具Kite，它在自动补全效率、文档阅读等方面的确要优于绝大多数自动补全工具，而且配置简单。本文将介绍另外一款基于深度学习的自动补全工具—TabNine，虽然同是基于人工智能的开发工具，但是它们却有各自的侧重点。下面我就先介绍一下Kite与TabNine的区别，然后详细介绍一下TabNine的特点，各位根据自己的偏好选择其中一款进行配置。 Kite与TabNineKite与TabNine都是优秀的、基于人工智能的代码自动补全工具，相对于大多数自动补全插件，它都能让人感到非常惊艳，但是二者都有各自的侧重点，下面就来说一下Kite与TabNine各自的优点和缺点。 Kite Kite更加倚重于预先存在的知识库，它是在预先存在的知识库上进行学习得到的补全经验，因此，不需要在开发过程中训练和学习，因此，它具备如下优点： 补全速度快 代码块补全功能强大 简洁强大的文档阅读功能 以代码块补全功能强大为例，来说一下Kite的特点，对于大多数补全工具，包括TabNine在内，主要是补全接下来要输入的字符串，而Kite不仅可以补全字符串，还可以补全代码库。举个例子，我们要导入numpy模块，对于大多数补全工具，我们输入numpy的几个字母后，它会联想出完整的numpy，但是对于Kite，当输入import num，它会根据知识库中大多数使用者的习惯直接补全import numpy as np。同样，当我们导入matplotlib中pyplot时，当我们输入from matplotlib，它会补全from matplotlib import pyplot as plt，这样显然更加高效，节省代码量。 没有哪个工具是完美无暇的，Kite也有缺点，我认为Kite是非常优秀的，甚至Python之父都对它赞不绝口，但是它也有3个缺点让我有点无法忍受， 方案偏“重” 支持语言单一 占用资源 首先说一下第一点，方案偏“重”。 对于大多数补全工具，它们只是一个插件，比较轻量化，而Kite相对较重，需要预先下载、安装一个200M+的软件，除了在电脑上安装Kite之外，还需要在对应的编辑器/IDE安全Kite的插件，另外，每次使用Kite自动补全之前需要把Kite打开。所以，这一系列的配置、使用过程相对较“重”。 其次说一下支持语言单一，Kite定位是一款Python自动补全工具，因此在很多特色方面都是针对Python进行优化，因此，支持语言相对单一。 最后说一下占用资源，虽然Kite内存占用情况远远比不上Pycharm、IDEA这些臃肿的开发工具，但是在win10下也要占用将近400M的内存资源，对于大多数工具相对较高，如果电脑配置偏低，打开Kite会使得系统较为卡顿。 TabNine 和Kite不同，首先，TabNine是基于开发者过去的使用习惯进行补全，而不是预先的知识库，因此，当项目较小或者正在向其中添加新库时，它的效果不如Kite。当然，这是二者思想上的差异，TabNine之所以成功，自然有它的优点， 轻量化 配置简单 支持语言丰富 能够给出补全项的概率和地址 以给出补全项的概率和地址为例来说明一下TabNine的优点，TabNine在自动补全时会给出每个候选项的概率，并且按照概率大小进行排序，此外，会给出候选项的来源及地址，这样更加方便查询和阅读。 另外，Kite在语言支持方面更加具有针对性，主要针对Python的自动补全，而TabNine支持的编程语言更加丰富，而不是针对某一特定语言。 TabNine的缺点也非常明显， 补全速度慢 依赖语言服务 首先说一下补全速度慢，TabNine在使用过程中需要大量的计算资源进行学习，因此它会带来高延迟。因此TabNine在专业版和企业版中提供了TabNine Cloud服务，提供GPU计算资源，当然，这都是需要付费的。 其次说一下依赖语言引擎，TabNine默认情况是关闭语义补全的，如果要开启语义补全需要安装不同编程语言对应的服务，例如Python需要安装python-language-server，每一种编程语言都需要配置对应的服务，可以选择繁琐的手动配置，针对这一点TabNine也提供了较好的解决方案，在编辑器中输入TabNine::sem，它会自动开启语义补全在开启过程中会按照对应语言的服务。 TabNine前面概括性的介绍了一下TabNine与Kite各自的优缺点，这里我就详细的介绍一下TabNine强大之处与原理。 优势 前面已经简单的介绍了TabNine的优点， 轻量化 配置简单 支持语言丰富 能够给出补全项的概率和地址 这些非量化的评价指标不够直观，从另外一个可量化的指标来说一下TabNine的强大之处， 支持22种编程语言 支持7类编辑器 支持的编程语言 TabNine支持Python, JavaScript, Java, C++, C, PHP, Go, C#, Ruby, Objective-C, Rust, Swift, TypeScript, Haskell, OCaml, Scala, Kotlin, Perl, SQL, HTML, CSS和Bash这22种编程语言。 支持的编辑器 首先要注意，我前面所说的是7类编辑器，而不是7款，它支持vs code、IntelliJ、Sublime、Vim、Emacs、Atom、Jupyter Notebook共7类编辑器。 其中IntelliJ是一个系列，其中包括IDEA、Pycharm、Android Studio等。 Vim包括Vim(Deopleto)、Vim(Coc)。 原理 TabNine是基于OpenAI的GPT-2模型，在GitHub上200万个文件上进行训练得出，它的训练目标是通过之前给出的标记预测接下来的标记(token)，为了实现这个目标，它学习了复杂的行为，例如动态类型语言的推断。 然后说一下GPT-2模型，它是一个在自然语言领域名气不亚于BERT的网络模型，它使用了Transformer网络作为基础，这个模型最初是用于解决自然语言处理问题(NLP)，尽管纯粹的自然语言和代码补全有很多不同之处，但是自然语言和代码补全在语义上还是有一些可借鉴的地方，例如对于英语的理解。TabNine就利用这一点用于代码补全中函数名、参数、返回类型等方面的推断。 TabNine配置TabNine配置相对于Kite较为简单，只需要在简单的几步即可以完成。需要再重复一遍的是，TabNine默认是不开启语言补全的，如果要开启语义补全需要安装配置对应语言的服务、引擎，当然，这对于TabNine也很简单，只需要在编辑器输入TabNine::sem即可，TabNine支持vs code、IntelliJ、Sublime、Vim、Emacs、Atom、Jupyter Notebook共7类编辑器，我在这里就挑选几个使用较多的编辑器/IDE讲解一下。 vs code 方式一：手动安装 打开vs code 点击插件管理 搜索tabnine 点击安装 这种方式比较简洁明了，首推这一种方法。 方式2：命令安装 打开vs code 快捷键Ctrl+P 输入ext install TabNine.tabnine-vscode 确认 IntelliJ Platform 打开设置 找到Plugins 在marketplace搜索TabNine安装 Sublime Text 快捷键Ctrl+Shift+P 输入Install Package 选择Package Control:Install Package 搜索TabNine确认 Vim 方式一：Vundle 首选需要配置插件管理器Vundle 在.vimrc中添加Plugin ‘zxqfl/tabnine-vim’ 随便打开一个vim，输入:PluginInstall 方式二：Git 克隆项目git clone —depth 1 https://github.com/zxqfl/tabnine-vim 把set rtp+=~/tabnine-vim添加到.vimrc（~/tabnine-vim是克隆到本地的路径，需要根据自己的路径修改） TabNine命令配置 就如同前面提到的，如果没有开启语义补全，需要在编辑器中输入命令开启语义补全，TabNine支持一些命令配置方式，主要有如下几个， TabNine::sem开启语义补全 TabNine::no_sem关闭语义补全 TabNine::config打开配置页面 TabNine::version查看版本信息 福利 我在公众号分享了Python、机器学习、计算机视觉、强化学习等领域相关的学习资源、电子文档。此外，还整理了一些高效的实用工具，如果需要可以关注公众号【平凡而诗意】，回复相应关键字获取~ 作品精选Jackpop：目录 | 精选CV、Python等系列教程 Jackpop：大数据处理 | Spark&amp;HDFS集群配置及基本使用 Jackpop：实用工具 | 推荐3款令人惊艳的截图工具 Jackpop：强烈推荐 | 这将会成为一个优质的github项目 Jackpop：计算机小白如何开始机器学习的学习，有入门课程推荐吗? Jackpop：C盘快满了，该如何清理？ Jackpop：2019 年双十一有哪些值得购买的东西？]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第八讲：代理模式]]></title>
    <url>%2F2019%2F11%2F15%2Fproxy%2F</url>
    <content type="text"><![CDATA[前言学会使用一门编程语言来完成一项功能非常容易，尤其是Python、Go这些脚本语言，也许对于有一定编程基础的同学而言只需要一周或者一天时间。但是如果要想让写的代码变得更加简洁易读、执行效率更高、可扩展性更好，那么就需要超越编程语言之外的知识，这就是设计模式。 在前面文章中，我介绍了一种比较常用的一种设计模式：单例模式。其实软件设计模式有非常多，代理模式、桥接模式、适配器、享元、工厂模式等。这些设计模式有的用的较多，有的在特定场景下才会用到，我会挑选个别应用场景较多的讲解一下。本文的要讲的就是代理模式，话不多说，下面开始代理模式的介绍及在Python中代理模式的实现方式。 代理模式 提到代理这个词汇，应该很多人都不陌生，甚至会有很多人用过。在编程语言之外，我们所接触的代理主要是在网络通信方面会涉及。当我们的网络不能直接与目标主机进行通信时，例如我们不能直接访问Google、Facebook，我们可以通过搭建代理服务器的方式实现我们所用的网络与目标主机之间的通信，这就是网络中代理的概念。用一句话来概括就是：允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。 之所以开始便讲解网络代理，是因为代理模式和网络代理有着很多相似的特点，而我们日常中接触更多的是网络代理，这样类比一下能够有助于大家的理解。 在代理模式中，我们作为访问者就相当于编程语言中调用的实体，目标主机就相当于被调用的实体，它们二者直接不产生直接联系，而是通过中间的代理服务器(代理实体)来实现二者的间接联系。 代理模式的作用 一个概念的产生，自然有它的价值存在，代理模式也是这样，简而言之，概括代理模式的作用如下： 当我们访问一个实体考虑到安全等因素不方便时，代理可以为这个实体提供一个替代者，来控制它的访问权限和访问内容。 通俗的来讲，代理模式就如同一个”过滤器”，它不实现具体功能，具体功能由被调用的实体来实现，代理实现的是对调用的控制功能，它能够允许或者拒绝调用实体对被调用实体的访问。 举个例子，我们访问一个具备敏感数据时，需要提供一个验证信息，比如，用户名、密码，代理模式完成的是更具访问者提供的用户名和密码来判断是否允许它进一步调用实体的功能。 代理模式的适用场景一个设计模式，只有当它有了应用场景，它才具备存在的价值。代理模式有很多可以使用的场景，主要分为如下几类： 远程代理：为一个对象的地址空间提供局部代表。 虚拟代理：根据需要来创建开销较大的对象。 保护代理：用于对象应该具有不同访问权限的场景，控制对原始对象的访问。 智能指引：取代简单的指针，它在访问对象时执行一些附加操作。 — 引自《设计模式：可复用面向对象软件的基础》 Python代理模式 代理模式在Java、C++中使用较多，可以用于虚拟代理和远程代理。由于Python这门语言相对简单，在企业中也常用于算法模块的验证，在一些大型系统很少会采用Python这门语言，因此，在软件设计模式方面考虑相对较少。但是，我认为，养成一个良好的软件设计思维，对日常开发和维护也具有非常多的好处，能够让开发效率更高，能够让代码可靠性更高，能够让后期维护成本更低。 在开发代码实现Python代理模式之前，我们首先来设定一个应用场景，这样才能够更加体现它的价值，同时让各位更加容易理解。 场景设定 假如，我们现在想了解一个班级的情况，主要包括两点：人数和学习成绩。 当我们想要了解这个班级的人数时，这个数据不敏感，不涉及隐私，因此可以直接访问。但是，当我们想查询特定某个学生成绩时，这样就涉及隐私信息，需要提供对应学生的姓名(user_name)，访问的密码(password)。 需求分析 针对这个场景，我们可以先分析一下，访问者就如同上述图中的客户端(Client)，获取班级人数、成绩这些实际的功能是由实体对象实现，也就是图中的RealSubject。Client与RealSubject之间不能直接通信，它们只能通过中间的代理(Proxy)进行通信。而Proxy主要的职责就相当于一个控制开关，如果Client要访问班级人数，Proxy会检查这项数据所需要的权限，然后发现班级人数是非敏感数据可以直接访问，那么它会调用实体对象中的方法，返回结果。如果Client要访问某个学生的成绩，Proxy会检查这项数据是敏感数据，需要提供用户名和密码，如果Client提供的正确，则允许访问，否则拒绝访问。 编程实践 第一步，我们自定义一个异常处理类，当我们要访问的用户不在班级成绩列表时，则抛出异常， 123class NotFindError(Exception): def __init__(self, msg): self.msg = msg 第二步，实现实体类，实体类实现了具体的功能，针对这个场景就两个方法：获取班级人数、获取指定学生的成绩， 12345678910111213141516class RealSubject(object): def __init__(self): self.score = &#123; "张三": 90, "李四": 59, "王二": 61 &#125; def num_students(self): num = len(self.score.keys()) print("The number of students is &#123;num&#125;".format(num=num)) def get_score(self, user_name): _score = self.score.get(user_name) print("The score of &#123;user&#125; is &#123;score&#125;".format(user=user_name, score=_score)) 第三步，实现代理(Proxy)，它通过对应功能的访问权限来确定是接受这个访问，还是拒绝这个访问， 注意：在这个示例中，我把密码直接写在初始化方法中，实际的项目是不允许这样的，不能把密码写在代码中，另外也不能使用明文密码，需要使用加密工具对明文密码进行加密。 123456789101112131415161718class Proxy(object): def __init__(self): self.default_passwd = "9l0skjlsa" self.real_subject = RealSubject() def num_students(self): self.real_subject.num_students() def get_score(self, user_name): print("You are visiting &#123;&#125; score ...".format(user_name)) passwd = input("Please input password : ") if passwd == self.default_passwd: if user_name in self.real_subject.score.keys(): return self.real_subject.get_score(user_name) else: raise NotFindError("The student you are visiting not found.") else: raise ValueError("The password you provided is wrong!") 然后就是实现Client来调用对应的功能，为了测试上述代理的功能，使用3个测试样例， 密码错误，用户名正确； 密码正确，用户名错误； 密码正确，用户名正确； 密码错误，用户名正确 123456789101112def client(): proxy = Proxy() proxy.get_score("张三") client()# shellYou are visiting 张三 score ...Please input password : kdksla # 输出ValueError: The password you provided is wrong! 密码正确，用户名错误 123456789101112def client(): proxy = Proxy() proxy.get_score("李三") client()# shellYou are visiting 张三 score ...Please input password : 9l0skjlsa# 输出NotFindError: The student you are visiting not found. 密码正确，用户名正确 123456789101112def client(): proxy = Proxy() proxy.get_score("李四") client()# shellYou are visiting 张三 score ...Please input password : 9l0skjlsa# 输出The score of 李四 is 59 本讲完整代码如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class NotFindError(Exception): def __init__(self, msg): self.msg = msgclass RealSubject(object): def __init__(self): self.score = &#123; "张三": 90, "李四": 59, "王二": 61 &#125; def num_students(self): num = len(self.score.keys()) print("The number of students is &#123;num&#125;".format(num=num)) def get_score(self, user_name): _score = self.score.get(user_name) print("The score of &#123;user&#125; is &#123;score&#125;".format(user=user_name, score=_score))class Proxy(object): def __init__(self): self.default_passwd = "9l0skjlsa" self.real_subject = RealSubject() def num_students(self): self.real_subject.num_students() def get_score(self, user_name): print("You are visiting &#123;&#125; score ...".format(user_name)) passwd = input("Please input password : ") if passwd == self.default_passwd: if user_name in self.real_subject.score.keys(): return self.real_subject.get_score(user_name) else: raise NotFindError("The student you are visiting not found.") else: raise ValueError("The password you provided is wrong!")def client(): proxy = Proxy() proxy.get_score("张三")client()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 让动画从1080p秒变4k]]></title>
    <url>%2F2019%2F11%2F07%2Fanime4k%2F</url>
    <content type="text"><![CDATA[前言 视频是我们日常生活中经常接触到的一种媒体形式，电影、电视剧、短视频、动画等。其中动画占据着我们娱乐生活中的一个很大比重，以我个人为例，偶尔会去电影院看一下电影，但是大多数都是通过下载到电脑的方式进行观看。我们接触的动画大多数为720p，再清晰一些的有1080p，而至于4k，这个概念似乎只存在于新闻里，却从未接触过，本文就来介绍一个强大的开源工具—Anime4k，能够实时、高质量的放大动画视频，能够轻松使得1080p变成4k。下面就来介绍一下这个优质开源的项目。 Anime4k图像放大在学术领域研究的也很多，尤其是最近几年人工智能在图像处理领域的广泛应用，很多研究者利用机器学习、深度学习进行视频图像放大，虽然深度学习方法在放大效果方面取得了一些成绩，但是在速度方面并不是很理想，我们都知道，实时视频流对速度要求也很高，如果每一帧放大速度太慢，那样会严重影响观感，即便是清晰度方面得到了提升，但是也无法使用。 本文介绍的Anime4k没有使用深度学习这些基于训练的方法，但是它却能够同时实现图像放大的实时及高质量。以往也有一些动画放大算法，例如， NGU：6ms Waifu2x：1s 而本文介绍的Anime4k把一副1080p的图像放大至2160p只需要3ms左右(Vega 64 GPU平台)。 Anime4k的实时和高质量使得它在保证图像清晰的情况下保证视频的连贯性，能够完全满足我们日常观看动漫的需求。 由于本文主要是介绍这款开源工具的配置及使用，因此不详细展开Anime4k的实现过程，如果感兴趣可以访问Anime4k的项目地址。 https://github.com/bloc97/Anime4Kgithub.com 目前Anime4k可以支持多款主流、强大的视频播放器，只需要配置一下就可以让播放器具备实时图像放大功能，能够高效的提升动漫的清晰度，Anime4k支持的播放器如下， PotPlayer MPC-BE MPC-HC MPV madVR 上述5款播放器都是开源，本文就拿我个人认为比较不错的两款播放器来介绍一下如何配置Anime4k这款工具。 PotPlayerpotplayer是一款开源、简洁且倍受欢迎的离线视频播放器，它不仅功能强大，支持丰富的定制化功能，而且使用简单，没有广告，这相对于大多数国产视频播放器要良心很多，因此，我在这里也推荐大家尝试一下这款播放器。话不多说，现在开始介绍PotPlayer+Anime4k的配置过程。 1. 下载安装PotPlayer可以访问链接http://www.potplayer.org/下载PotPlayer安装包，然后双击安装即可。 2. 下载Anime4k HLSL文件打开链接， https://github.com/bloc97/Anime4K/releasesgithub.com 选择Anime4K_HLSL.zip下载。3. 解压并复制到指定文件夹解压第2步下载的压缩包，把其中HLSL文件移动到PotPlayer安装目录下PxShader文件夹里，如果安装PotPlayer没有修改安装路径的话，默认位置在C:\Program Files (x86)\DAUM\PotPlayer\PxShader。4. 配置PotPlayer打开PotPlayer播放器，依次点选如下选项， 视频 像素着色 调整尺寸后的着色集 一定要注意，是调整后的着色期，不要选择调整尺寸前的着色集。然后点击组合编辑，添加如下4个HLSL文件， 1234-Anime4K_ComputeLum -Anime4K_Push -Anime4K_ComputeGradient -Anime4K_PushGrad_Weak 如果原图像尺寸小于或等于1080p则选择Anime4K_PushGrad_Weak ，如果大于1080p，则选择Anime4K_PushGrad。 如果找不到这几个选择可以重启PotPlayer， 然后依次点击视频-&gt;像素着色-&gt;调整尺寸后的着色集-&gt;组合即可， MPC-BEMPC-BE是一款开源简洁的视频播放器，内容占用比PotPlayer还要小。MPC-BE+Anime4k的配置方式和PotPlayer非常相似，所以，有的过程就简略的介绍一下。下面就来看一下它的配置过程。 1. 下载安装MPC-BE 2. 下载Anime4k HLSL文件 打开链接，https://github.com/bloc97/Anime4K/releases 3. 解压复制到指定文件夹 把上一步下载HLSL压缩包解压，放到下面路径， C:\Users\YourUserName\AppData\Roaming\MPC-BE\Shaders 4. 配置MPC-BE 这一步和PotPlayer配置不同，MPC-BE配置需要依次选择如下选择， 播放 着色器 选择着色器 下面要做的和PotPlayer类似，添加HLSL文件， 这里记得勾选启用后调整大小像素着色器。 经过以上步骤配置即可使用MPC-BE+Anime4k。 工具获取 为了方便大家获取，我把PotPlayer、MPC-BE、Anime4k_HLSL进行了整理共享，如果需要可以公众号后台回复关键字”4k“获取下载链接。]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>影音娱乐</tag>
        <tag>工具</tag>
        <tag>实用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第七讲：接口与抽象基类]]></title>
    <url>%2F2019%2F10%2F27%2Fabstract%2F</url>
    <content type="text"><![CDATA[前言抽象基类(abstract base class,ABC)，提到这个概念应该会马上联想到面向对象、继承。作为继承的一种，它拥有继承中代码共享、提高代码的重用性等优点。例如，下面示例， 123456789101112131415161718class Animal(object): def eat(self, kind): print("&#123;&#125; eat food....".format(kind))class Dog(Animal): passclass Cat(Animal): pass dog = Dog()cat = Cat()dog.eat("dog")cat.eat("cat")# 输出dog eat food....cat eat food.... 狗(Dog)和猫(Cat)都属于动物(Animal)，它们有很多类似的属性和动作，我们可以在父类中实现这些方法，在子类中直接继承或者重载父类中的方法，这样减少了代码的重复性，提高了代码的共享能力。 作为继承的一种，抽象基类有用继承的上述这些优点，但是它与普通的继承也有不同之处， 抽象基类不能实例化 子类需要实现基类指定的抽象方法 看到这里应该会意识到，抽象基类有一种接口的感觉，没错，抽象基类的出现主要是功能就是类似于Java等编程语言中的接口。但是需要明确一点，Python语言中没有interface这个概念，只是这是一种约定俗成的编程规范，就如同Python也没有真实意义上的私有变量，我们在编程中可以规范的使用下划线来表示某个变量为私有变量。 尽管Python中没有接口这个关键字，但是抽象基类实现的功能主要围绕接口在展开，因此，首先类比Java来阐述一下编程语言中接口的概念，然后介绍一下Python中如何实现抽象基类。 接口接口(Interface)是对象公开方法的一种集合，在Java中通常以interface关键字来定义，接口虽然实现过程中和类相似，但是却具有不同的概念。具体而言，类与接口主要有以下几点不同之处： 类实现了对象的属性和方法，而接口指定了使用该接口需要实现哪些方法 类可以实例化，而接口不可以被实例化 类中的方法可以是实现，接口中的方法都是抽象方法 抽象方法：抽象方法的概念是父类中只负责声明该方法，但不具体实现这个方法，实现部分由继承该类的子类负责实现。 如果觉得上述描述有点云里雾里、对接口的概念依然不是非常清楚，不妨来试想一个场景：当你开发一个项目或者服务，你需要给上下游的组件提供接口，让别人来调用你的程序接口(Application Programming Interface，API)，上下游组件该怎么样才能达到想要的目的和你的组件无缝衔接？需要通过按照你接口中规定的抽象方法来实现，例如，你提供一个访问网络请求的接口，你不会去实现host、username、password的注册和发送请求，这些需要调用的用户去实现，你只需要规定：“调用者必须实现指定方法才能实现调用”即可。 抽象基类虽然Python中抽象基类和接口概念非常相近，但是它们还是有一些不同之处，例如， 接口需要被实现的子类完成接口中指定的所有方法，而抽象基类不是，抽象基类则没有这么严格的要求 接口需要所有方法都是抽象方法，而抽象基类中有抽象方法，也有自己实现的方法 正是因为抽象基类和接口的不同之处使得接口之所以称为接口、抽象基类之所以称为抽象基类。 为什么使用抽象基类？ 前面铺垫了这么多，话说回来，为什么需要抽象基类？ 存在的即是合理的，抽象基类的存在自然有它的价值。当你学会一种编程语言的语法时，你可以轻松的完成一项功能的开发，但是如果希望把代码完成的更加优美高效，那么就需要在设计模式等方面下一些功夫，抽象基类就是其中的一个选择，抽象基类具有以下优点： 处理继承问题方面更加规范、系统 明确调用之间的相互关系 使得继承层次更加清晰 限定子类实现的方法 什么是抽象基类？ 前面已经介绍了很多有关接口的概念，抽象基类和接口有很多相似之处，例如需要包含抽象方法，不能被实例化，如果更加确切的定义抽象基类：必须包含一个抽象函数(纯虚函数)，它是一个不完整的类，它有已经被实现的方法，也有需要子类重写的方法。 抽象基类使用场景一项功能只有具有应用场景才能体现出它的价值，如果仅仅是为了看上去高逼格，那么倒不如使用最简单的条件、循环语句，没必要花里胡哨，让代码变得难以维护、晦涩难懂。 抽象基类首先它具备普通继承的功能，因此，在代码可以共用，或者需要获取额外属性的时候可以考虑使用抽象基类，例如，狗、猫、牛、羊这些动物有很多共有的属性和方法，我们可以通过实现一个基类，让每个特定的对象来继承它，这样不仅可以实现多态，还可以提高代码的复用能力。 当然，上述说的这些场景都偏重于普通继承的优势，而抽象基类的特别之处更加偏向于接口的特点，因此，它的使用场景和接口也有很多相通之处，例如我们开发一个系统，下面有若干个组件，每个组件都需要按照指定的规范来实现特定的方法，这时候我可以发挥抽象基类的限定功能的优势。 下面就结合这个场景来介绍Python中抽象基类的实现方法。 Python抽象基类场景介绍 假如我们现在实现了一个数据中台的开发，我们对外提供一个接口让不同组件通过这个接口进行访问数据库，来读取数据，我们给数据接口主要有2个功能， 登录数据库 读取数据 执行SQL语句 可以想象，登录数据库这个功能在不同组件之间可以共用，不同组件只需要提供host、user、passwd即可，至于读取数据这是每个组件都必须单独实现的，可以声明为抽象方法，执行SQL语句也是每个子类需要实现的，可以声明为抽象的静态方法。 实现 Python标准库中有一个模块abc可以实现抽象基类和抽象方法，它们的实现方式如下： 抽象基类：通过继承abc模块中的ABC类来实现抽象基类。 抽象方法：通过装饰器的方法来调用abc模块中abstractmethod方法来注解抽象基类的方法。 abstractmethod注解除了可以实现抽象方法外，还可以注解类方法(@classmethod)、静态方法(@staticmethod)、属性(@property)。 下面就先实现抽象基类， 1234567891011121314151617181920212223from abc import ABCfrom abc import abstractmethodclass Database(ABC): def register(self, host, user, password): print("Host : &#123;&#125;".format(host)) print("User : &#123;&#125;".format(user)) print("Password : &#123;&#125;".format(password)) print("Register Success!") @abstractmethod def query(self, *args): """ 传入查询数据的SQL语句并执行 """ @staticmethod @abstractmethod def execute(sql_string): """ 执行SQL语句 """ 从抽象基类Database的实现可以看出，它共包含3个方法，其中register是每个子类都需要的，直接实现在抽象基类里，是一个普通的类方法。query和execute只是在基类中进行类声明，给出了描述，但并没有实现，它限定了继承Database的子类必须实现这两个方法。 下面就来实现两个组件(子类)， 1234567891011121314151617181920212223242526272829303132333435363738394041class Component1(Database): def __init__(self, host, user, password): self.register(host, user, password) @staticmethod def execute(sql_string): print(sql_string) def query(self, *args): sql_string = "SELECT ID FROM db_name" self.execute(sql_string)class Component2(Database): def __init__(self, host, user, password): self.register(host, user, password) @staticmethod def execute(sql_string): print(sql_string) def query(self, *args): sql_string = "SELECT NAME FROM db_name" self.execute(sql_string)comp1 = Component1("00.00.00.00", "abc", "000000")comp2 = Component2("11.11.11.11", "ABC", "111111")comp1.query()comp2.query()# 输出结果Host : 00.00.00.00User : abcPassword : 000000Register Success!Host : 11.11.11.11User : ABCPassword : 111111Register Success!SELECT ID FROM db_nameSELECT NAME FROM db_name 上述是通过Python标准库中abc模块实现了抽象基类，其实在Python中collections中也实现了抽象基类，numbers中也定义了有关数字对象的抽象基类，可见，抽象基类在Python中占据着至关重要的地位。 完整代码本文所涉及的完整代码可以查看github项目advance-python，也可以直接访问下方链接， https://github.com/Jackpopc/advance-python/blob/master/5-abstract%20.ipynb]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十七讲：卷积神经网络之VGG]]></title>
    <url>%2F2019%2F10%2F27%2Fvgg%2F</url>
    <content type="text"><![CDATA[前言 2014年对于计算机视觉领域是一个丰收的一年，在这一年的ImageNet图像识别挑战赛(ILSVRC,ImageNet Large Scale Visual Recognition Challenge)中出现了两个经典、响至深的卷积神经网络模型，其中第一名是GoogLeNet、第二名是VGG，都可以称得上是深度计算机视觉发展过程中的经典之作。 虽然在名次上GoogLeNet盖过了VGG，但是在可迁移性方面GoogLeNet对比于VGG却有很大的差距，而且在模型构建思想方面对比于它之前的AlexNet、LeNet做出了很大的改进，因此，VGG后来常作为后续卷积神经网络模型的基础模块，用于特征提取。直到5年后的今天，依然可以在很多新颖的CNN模型中可以见到VGG的身影，本文就来详细介绍一下这个经典的卷积神经网络模型。 VGG模型 VGG(VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION)，是由牛津大学的研究者提出，它的名称也是以作者所在实验室而命名(Visual Geometry Group)。 前一篇文章介绍了经典的AlexNet，虽然它在识别效果方面非常令人惊艳，但是这些都是建立在对超参数进行大量的调整的基础上，而它并没有提出一种明确的模型设计规则以便指导后续的新网络模型设计，这也限制了它的迁移能力。因此，虽然它很知名，但是在近几年的模型基础框架却很少出现AlexNet的身影，反观VGG则成为了很多新模型基础框架的必选项之一，这也是它相对于AlexNet的优势之一：VGG提出用基础块代替网络层的思想，这使得它在构建深度网络模型时可以重复使用这些基础块。 正如前面所说，VGG使用了块代替层的思想，具体的来说，它提出了构建基础的卷积块和全连接块来替代卷积层和全连接层，而这里的块是由多个输出通道相同的层组成。 VGG和AlexNet指代单一的模型不同，VGG其实包含多个不同的模型，从上图可以看出，它主要包括下列模型， VGG-11 VGG-13 VGG-16 VGG-19 其中，后面的数字11、13、16、19是网络层数。 从图中可以看出，VGG的特点是每个卷积块(由1个或多个卷积层组成)后面跟随一个最大池化层，整体架构和AlexNet非常类似，主要区别就是把层替换成了块。 从图中红框标记可以看出，每个卷积块中输出通道数相同，另外从横向维度来看，不同模型在相同卷积块中输出通道也相同。 下面就以比较常用的VGG-16这个模型为例来介绍一下VGG的模型架构。 VGG-16是由5个卷积块和3个全连接层共8部分组成(回想一下，AlexNet也是由8个部分组成，只不过AlexNet是由5个卷积层和3个全连接层组成)，下面详细介绍每一个部门的详细情况。 注意：前两篇文章我们在搭建LeNet和AlexNet时会发现，不同层的卷积核、步长均有差别，这也是迁移过程中比较困难的一点，而在VGG中就没有这样的困扰，VGG卷积块中统一采用的是3*3的卷积核，卷积层的步长均为1，而在池化层窗口大小统一采用2*2，步长为2。因为每个卷积层、池化层窗口大小、步长都是确定的，因此要搭建VGG我们只需要关注每一层输入输出的通道数即可。 卷积块1 包含2个卷积层，输入是224*224*3的图像，输入通道数为3，输出通道数为64。 卷积块2 包含2个卷积层，输入是上一个卷积块的输出，输入通道数为64，输出通道数为128。 卷积块3 包含3个卷积层，输入是上一个卷积块的输出，输入通道数为128，输出通道数为256。 卷积块4 包含3个卷积层，输入是上一个卷积块的输出，输入通道数为256，输出通道数为512。 卷积块5 包含3个卷积层，输入是上一个卷积块的输出，输入通道数为512，输出通道数为512。 全连接层1 输入为上一层的输出，输入通道数为前一卷积块输出reshape成一维的长度,输出通道数为4096。 全连接层2 输入为上一层的输出，输入通道数为4096,输出通道数为4096。 全连接层3 输入为上一层的输出，输入通道数为4096,输出通道数为1000。 激活函数 VGG中每层使用的激活函数为ReLU激活函数。 由于VGG非常经典，所以，网络上有关于VGG-16、VGG-19预训练的权重，为了为了展示一下每一层的架构，读取VGG-16预训练权重看一下， 1234567891011121314151617181920212223242526272829303132import numpy as nppath = "vgg16.npy"layers = ["conv1_1", "conv1_2", "conv2_1", "conv2_2", "conv3_1", "conv3_2", "conv3_3", "conv4_1", "conv4_2", "conv4_3", "conv5_1", "conv5_2", "conv5_3", "fc6", "fc7", "fc8"]data_dict = np.load(path, encoding='latin1').item()for layer in layers: print(data_dict[layer][0].shape) # 输出(3, 3, 3, 64)(3, 3, 64, 64)(3, 3, 64, 128)(3, 3, 128, 128)(3, 3, 128, 256)(3, 3, 256, 256)(3, 3, 256, 256)(3, 3, 256, 512)(3, 3, 512, 512)(3, 3, 512, 512)(3, 3, 512, 512)(3, 3, 512, 512)(3, 3, 512, 512)(25088, 4096)(4096, 4096)(4096, 1000) 网络共16层，卷积层部分为1*4维的，其中从前到后分别是卷积核高度、卷积核宽度、输入数据通道数、输出数据通道数。 到此为止，应该已经了解了VGG的模型结构，下面就开始使用tensorflow编程实现一下 VGG。 编程实践因为 VGG非常经典，所以网络上有VGG的预训练权重，我们可以直接读取预训练的权重去搭建模型，这样就可以忽略对输入和输出通道数的感知，要简单很多，但是为了更加清楚的理解网络模型，在这里还是从最基本的部分开始搭建，自己初始化权重和偏差，这样能够更加清楚每层输入和输出的结构。 卷积块 经过前面的介绍应该了解，VGG的主要特点就在于卷积块的使用，因此，我们首先来完成卷积块部分的编写。在完成一段代码的编写之前，我们应该首先弄明白两点：输入和输出。 输出当然很明确，就是经过每个卷积块(多个卷积层)卷积、激活后的tensor，我们要明确的就是应该输入哪些参数？ 最重要的3个输入：要进行运算的tensor、每个卷积块内卷积层的个数、输出通道数。 当然，我们为了更加规范的搭建模型，也需要对每一层规定一个命名空间，这样还需要输入每一层的名称。至于输入通道数，我们可以通过tensorflow的get_shape函数获取， 123456789101112def conv_block(self, X, num_layers, block_index, num_channels): in_channels = int(X.get_shape()[-1]) for i in range(num_layers): name = "conv&#123;&#125;_&#123;&#125;".format(block_index, i) with tf.variable_scope(name) as scope: weight = tf.get_variable("weight", [3, 3, in_channels, num_channels]) bias = tf.get_variable("bias", [num_channels]) conv = tf.nn.conv2d(X, weight, strides=[1, 1, 1, 1], padding="SAME") X = tf.nn.relu(tf.nn.bias_add(conv, bias)) in_channels = num_channels print(X.get_shape()) return X 从代码中可以看出，有几个参数是固定的： 卷积窗口大小 步长 填充方式 激活函数 到此为止，我们就完成了VGG最核心一部分的搭建。 池化层 之前看过前两篇关于AlexNet、LeNet的同学应该记得，池化层有两个重要的参数：窗口大小、步长。由于在VGG中这两个超参数是固定的，因此，不用再作为函数的入参，直接写在代码中即可。 12def max_pool(self, X): return tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") 全连接层 至于全连接层，和前面介绍的两个模型没有什么区别，我们只需要知道输出通道数即可，每一层的输出为上一层的输出， 1234567def full_connect_layer(self, X, out_filters, name): in_filters = X.get_shape()[-1] with tf.variable_scope(name) as scope: w_fc = tf.get_variable("weight", shape=[in_filters, out_filters]) b_fc = tf.get_variable("bias", shape=[out_filters], trainable=True) fc = tf.nn.xw_plus_b(X, w_fc, b_fc) return tf.nn.relu(fc) 由于不同网络模型之前主要的不同之处就在于模型的结构，至于训练和验证过程中需要的准确率、损失函数、优化函数等都大同小异，在前两篇文章中已经实现了训练和验证部分，所以这里就不再赘述。在本文里，我使用numpy生成一个随机的测试集测试一下网络模型是否搭建成功即可。 测试 首先使用numpy生成符合正态分布的随机数，形状为(5, 224, 224, 3)，5为批量数据的大小，244为输入图像的尺寸，3为输入图像的通道数，设定输出类别数为1000， 123456789101112131415161718192021222324252627282930def main(): X = np.random.normal(size=(5, 224, 224, 3)) images = tf.placeholder("float", [5, 224, 224, 3]) vgg = VGG(1000) writer = tf.summary.FileWriter("logs") with tf.Session() as sess: model = vgg.create(images) sess.run(tf.global_variables_initializer()) writer.add_graph(sess.graph) prob = sess.run(model, feed_dict=&#123;images: X&#125;) print(sess.run(tf.argmax(prob, 1)))# 输出(5, 224, 224, 64)(5, 224, 224, 64)(5, 112, 112, 128)(5, 112, 112, 128)(5, 56, 56, 256)(5, 56, 56, 256)(5, 56, 56, 256)(5, 28, 28, 512)(5, 28, 28, 512)(5, 28, 28, 512)(5, 14, 14, 512)(5, 14, 14, 512)(5, 14, 14, 512)(5, 4096)(5, 4096)(5, 1000)[862 862 862 862 862] 可以对比看出，每层网络的尺寸和前面加载的预训练模型是匹配的，下面在看一下tensorboard的结果， 1$ tensorboard --logdir="logs" 结果， 完整代码完整代码请查看github项目aiLearnNotes，也可以直接访问下面链接， https://github.com/Jackpopc/aiLearnNotes/blob/master/computer_vision/VGG-16.py]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第六讲：单例模式的妙用]]></title>
    <url>%2F2019%2F10%2F19%2Fsingleton%2F</url>
    <content type="text"><![CDATA[前言在第三讲：类的特殊方法(上篇)中我在讲解Python特殊方法__new__的使用时提及了一个概念—单例模式，这是一个软件设计中非常重要的概念，由于它不属于某一类特定的语言，既可以用于Java、也可以用于Python，因此在这些单一编程语言的书籍里很少特意花费篇幅介绍单例模式，因此，我准备用这整篇文章来介绍一下Python的单例模式的实现及使用场景。 本文，我将从如下3个方面阐述Python单例模式的使用， - 单例模式的概念 - Python单例模式的实现 - 单例模式的使用场景 # 单例模式 首先看一下维基百科对单例模式的解释， > 单例模式，也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 上述描述也许有点让人云里雾里，我来提炼一下维基百科关于单例模式解释的关键点， 1. 单例模式是一种软件设计模式，而不是专属于某种编程语言的语法； 2. 单例模式只有一个实例存在； 3. 单例模式有助于协调系统的整体性、统一性； 软件设计模式 我一直认为，对于一门编程语言“入门容易，精通不易”，哪怕是对于很多人都认为简单的Python语言。 我们学会一门语言的基本语法和基本使用也许只需要2个月、2个周，甚至2天或者2个小时，但是如果用一门编程语言开发出高性能的系统，却是一件日积月累的事情。 当使用一门编程语言时一定要认清一个问题，代码不仅是给机器看的，同时也要给人看。因此，我们实现一个工程项目，要同时兼顾代码的高效性和简洁易读性。在效率方面我们可以借助分而治之、动态规划、二叉树、B-树等算法设计模式和数据结构，但是要实现代码的简洁性和高效性还离不开一个好的软件设计模式，软件设计模式有很多种，例如， - 工厂模式 - 原型模式 - 单例模式 - 生成器模式 - ...... 使用合理的软件设计模式可以使得代码重用性更高、更易于理解、可靠性更高。 单例模式只有一个实例存在 这是单例模式的主要特征，也是设计单例模式的要求，和普通软件设计模式允许多个实例同时存在不同，单例模式只允许一个实例存在，首先来看一个示例， 123456789101112class Software(object): def __init__(self): passsoft1 = Software()soft2 = Software()print(id(soft1))print(id(soft2))# 输出25388466195762538846620024 上述给出的Python的一个普通软件设计模式，当我们定义一个名为Software的类后，我们先后实例化两个对象，分别是soft1和soft2，输出它们的地址可以看出，它们不是同一个示例，这就限制了它在某些场景下无法使用，后面关于单例模式的使用场景部分会专门介绍。 单例模式有助于协调系统的整体性、统一性 由于单例模式的设计要求使得每一个应用、活动只有一个实例，这使得不管我们怎么去调用、实例化，当前唯一存在一个实例，这在资源调度、日志管理、信息注册等应用场景下保证了只有一个实例对其进行操作，而避免了多个实例同时操作一个对象，这保证了协调系统的整体性和统一性。 Python单例模式其实，关于Python单例模式的实现，在第三讲：类的特殊方法(上篇)中已经有所提及，可以通过重写__new__方法来实现单例模式，但是Python实现单例模式不仅包含这一种方式，还可以使用装饰器来实现单例模式，下面来看一下两种实现Python单例模式的方式。 首先，定义一个名为Singleton的基类，在这个基类里面对new方法进行重写， 123456class Singleton(object): def __new__(cls, *args, **kw): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls) return cls._instance 然后，凡是继承Singleton基类的子类都属于单例模式，下面来看一下， 123456789101112class Books(Singleton): def __init__(self): pass book1 = Books()book2 = Books()print(id(book1))print(id(book2))# 输出25388474579682538847457968 可以从上面输出看得出来，我们虽然对Books类实例化两次，分别得到两个名为book1和book2的实例，但是id却是相同的，也就说这两个实例指向同一个地址，为同一个实例。 装饰器 在第二讲中我详细的介绍了Python装饰器的使用，简而言之，Python装饰器就是操作函数的函数，当然，它类也可以作为装饰器的输入。利用装饰器实现Python单例模式就是通过类进行操作实现单例模式， 首先，我们完成装饰器的编写， 1234567def singleton(cls, *args, **kw): instances = &#123;&#125; def wrapper(): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return wrapper 然后调用装饰器，实现单例模式， 12345678910111213@singletonclass Animal(object): def __init__(self): pass animal1 = Animal()animal2 = Animal()print(id(animal1))print(id(animal2))# 输出25388482085442538848208544 看一下上面的输出，和new方法实现的效果是相同的。 除此之外，还可以通过__metaclass__元类、共有属性等来实现，但是由于它本质上与上述两种方式并没有什么区别，也许看代码过程中会觉得有点不太明白，其实上述两种方式都是基于同一个思想进行实现的：创建实例(instance)时首先判断是否已经存在，如果已经存在则返回，否则创建。 单例模式的使用场景由于单例模式的特殊性，使得它具备整体性、统一性的优势，因此，它的使用场景大多数也是围绕这两点优势进行展开的，如果遇到以下场景，我们可以考虑是否能够使用单例模式来实现， 资源管理的场景 难以同步的场景 涉及共享的场景 有关认证的场景 以上述第四点展开进行讨论一下，结合代码更加容易理解单例模式的妙处所在。 场景描述 做项目开发过程中，大多数岗位都会和数据打交道，无论是前端还是后端。假如，我们存储数据工具是SQL Server，我们需要通过host、user、passwd来连接数据库进行读取数据，这时候就需要一次认证，多次调用，请注意这句话，很关键。 普通模式 我们首先来实现一个连接SQL的类， 123456789101112class SqlClient(object): def __init__(self, host, user, passwd): self.host = host self.user = user self.passwd = passwd self.register() def register(self): self.info = "&#123;&#125;--&#123;&#125;---&#123;&#125;".format(self.host, self.user, self.passwd) def select(self): print("SELECT * FROM &#123;&#125;".format(self.host)) SqlClient中有3个方法，__init__用于初始化参数，register是认证SQL客户端，select是执行SQL语句的操作。 到这里，我们完成了SQL的认证，后面我们会在不同的地方查找数据，也就是在多个地方需要调用SqlClient类的select方法，试想一下我们该怎么实现？ 有两种方法： 反复实例化、反复认证 把实例化后的对象作为参数传入到每个用到select的函数里 先看第一种， 1234567891011121314151617181920212223host = "10.293.291.19"user = "admin"passwd = "666666"def use_data_1(): sql_client = SqlClient(host, user, passwd) sql_client.select() def use_data_2(): sql_client = SqlClient(host, user, passwd) sql_client.select()def use_data_3(): sql_client = SqlClient(host, user, passwd) sql_client.select() use_data_1()use_data_2()use_data_3()# 输出SELECT * FROM 10.293.291.19SELECT * FROM 10.293.291.19SELECT * FROM 10.293.291.19 可以看到，我们在use_data_1、use_data_2、use_data_3三处使用到了SQL选择工具，每一次我们都要重新实例化SqlClient，显然，这是很麻烦的。 然后再看一下第二种方式， 12345678910111213141516host = "10.293.291.19"user = "admin"passwd = "666666"def use_data_1(sql_client): sql_client.select() def use_data_2(sql_client): sql_client.select()def use_data_3(sql_client): sql_client.select() sql_client = SqlClient(host, user, passwd)use_data_1(sql_client)use_data_2(sql_client)use_data_3(sql_client) 我们可以先对实例化SqlClient，然后作为参数传入到每一个用到SQL工具的地方。 这样看来显然比第一种要好很多，在代码简洁性方面比第一种方法优化了不少，但是，开发中我们应该意识到一个问题，尽量少传参数，尤其是链式调用的函数，只在其中某几个环境用到，我们却需要不断的把它当作参数一致往下传递，如果这样的话，我们会发现，我们会传递很多参数，例如下面这个示例， 123456789101112131415host = "10.293.291.19"user = "admin"passwd = "666666"def use_data_1(sql_client): sql_client.select() use_data_2(sql_client) def use_data_2(sql_client): use_data_3(sql_client)def use_data_3(sql_client): sql_client.select() sql_client = SqlClient(host, user, passwd)use_data_1(sql_client) 可以看到上述示例，use_data_1调用use_data_2，use_data_2调用use_data_3,而我们在use_data_1、use_data_3中需要用到SQL工具，但是在use_data_2这个中间环节用不到，但是为了让参数继续传递下去，sql_client却不得不作为use_data_2的一个入参。 单例模式 这时候我们就可以使用单例模式来轻松解决这个问题，我们只需要实例化一次用于认证，然后再每个位置调用即可， 123456789101112131415class Singleton(object): def __new__(cls, *args, **kw): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls) return cls._instance class SqlClient(Singleton): info = None def register(self, host, user, passwd): self.info = "&#123;&#125;--&#123;&#125;--&#123;&#125;".format(host, user, passwd) def select(self): print(self.info) 我们通过继承Singleton实现SqlClient的单例模式，我们只需要调用register一次，用于认证客户端，然后后期每次重新实例化都是指向的同一个实例，也就是已经认证过的示例，我们后面任何其他地方调用的地方直接使用select方法即可， 12345678910111213def use_data_1(): SqlClient().select()def use_data_2(): SqlClient().select() def use_data_3(): SqlClient().select() SqlClient().register(host, user, passwd)use_data_1()use_data_2()use_data_3() 依此可以发散思维一下，凡是类似的场景都可以考虑一下是否可以使用单例模式。 当然，凡事既有优点就会有缺点，单例模式也是，它可以实现系统的整体性和统一性，但是也不是在任何场景下都是适用的，例如， 多线程 可变对象 在这些场景下，它违背了单例模式单一性原则，而且很容易因此数据错误。 因此，使用单例模式之前需要考虑一下对应场景是否适合，如果适合，单例模式能够大大提高代码的效率，同时使得代码更加简洁，但是如果不适合而强行使用单例模式，那样会导致很多未知的问题。 完整代码我把完整代码已经放在github，感兴趣的可以点击下方链接，或者直接搜索项目advance-python， https://github.com/Jackpopc/advance-python/blob/master/4-Singleton.ipynb]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 2款强大的C盘清理工具]]></title>
    <url>%2F2019%2F10%2F16%2Fclean-c%2F</url>
    <content type="text"><![CDATA[前言 如果是我，绝大多数情况看到这样的标题我是不会点进来看的。 有几类软件在我心里被认为是毫无用处的鸡肋，其中就有C盘清理工具。 作为一个经常跟电脑打交道的开发者，当看到C盘空间从100G、80G、50G、30G、10G逐渐减小的过程中，就如同看到手机电量从100%逐渐降至80%、50%、20%、5%一样，让我心神不宁、坐立不安。 有时候会很疑惑，我明明没有安装任何软件，为什么C盘空间还在持续缩小？然后会打开C盘逐个的去看一下每个目录的空间占用情况，然后发现对于那些大文件无从下手、不知道有什么作用，最终还是灰溜溜的放弃。 有时无奈之余会想着求助C盘清理工具吧，平时在各种平台看到过太多各式各样的C盘清理工具，例如，比较知名的360、电脑管家、魔方等，但是，经过一段时间的对比使用之后会发现，效果可以说是微乎其微。就如同经过某些电脑管家清理内存之后它告诉你，“速度提升*%”，然而，当把它卸载后你会电脑运行的速度要远远超过\*%。 因此，久而久之就不得不改掉这种强迫症，不再去关注内存的缩小，直到前段时间我遇到两款工具，本来我没有抱太大情况，心想软件不大、免安装，倒不如试一下，用过之后我才发现，竟然有这么强大的C盘清理工具，能够轻松清理C盘N个G的工具，下面就来介绍一下这两款工具。 Windows Update Clean Tool在介绍这两款工具之前我们先看一下windows自带的清理工具的情况，然后能够做一下明显的对比， 首先，鼠标右键点击C盘盘符 然后，点击磁盘清理，同时点击清理系统文件 从上图可以看出，全选windows自带清理工具识别出来的垃圾空间占用量为135MB。 下面来看一下Windows Update Clean Tool的情况，没有占用空间综合，粗略估计至少有1.4GB, 经过对比，现在对Windows Update Clean Tool这款工具的强大之处已经一目了然，但是它的强大之处远不止我们静态看到的这些，它不仅能够识别如下类别垃圾： 临时文件 日志文件 冗余文件 安装源 缓存 …… 它还有一点是其他C盘清理工具是无法比拟的，那就是速度快！！！ 我们使用清理工具，当然也包括windows自带的工具会发现，扫描过程非常缓慢，少则几分钟，多则几十分钟都有可能。而Windows Update Clean Tool只需要1分钟内就可以完成垃圾的扫描，我说的1分钟是对绝大多数机型，如果性能好一些的电脑10秒内即可完成扫描。 最后还需要补充3点，Windows Update Clean Tool无需安装，解压即可使用！！！内存占用小，只有4.4MB！！！更重要的是免费！！！ WICleanupWindows Update Clean Tool是一款针对不同类型文件的清理工具，它能够清理安装源、缓存等类型的垃圾，而WICleanup则是一款纯粹的冗余文件清理工具，它能够更加深层次、专注的挖掘MSI补丁程序冗余文件，然后对其进行扫描和清理，它和Windows Update Clean Tool的侧重点不同，因此两款工具可以结合使用对C盘进行清理。 从上图可以看出经过Windows Update Clean Tool清理之后，WICleanup还可以扫描述几百兆的冗余文件，由于我有一个经常清理电脑的习惯，所以电脑垃圾相对较少，在办公电脑上我尝试了一下上述两款工具，能够迅速清理出5GB以上的垃圾。 同样需要补充3点，WICleanup内存占用更小，只要120KB！！！它同样是绿色免安装的，解压即可使用！！！WICleanup同样是一款完全免费的工具。 免责声明windows是一款庞大而复杂的系统，作为一个非系统开发者，我对它内部的很多文件功能也不是完全了解，只是我发现这两款工具非常好用，因此分享给大家。但是，强大的清理功能随之带来的就是更高的风险。因此，在删除的过程中切勿盲目点按，需要看清楚软件的提醒，以Windows Update Clean Tool为例，它会有推荐操作，推荐你是保留还是删除，为了保险起见还是应该慎重按照软件的提示使用。如果因为使用上述软件造成了系统损坏问题，请不要强加责备。当然，经过我在多台电脑上试用，目前还没有发生什么问题。 下载方式我已经把上述两款工具打包进行共享，如果需要可以关注公众号，后台回复关键字“clean”获取~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>清理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十六讲：卷积神经网络之AlexNet]]></title>
    <url>%2F2019%2F10%2F13%2Falexnet%2F</url>
    <content type="text"><![CDATA[前言 前文详细介绍了卷积神经网络的开山之作LeNet，虽然近几年卷积神经网络非常热门，但是在LeNet出现后的十几年里，在目标识别领域卷积神经网络一直被传统目标识别算法(特征提取+分类器)所压制，直到2012年AlexNet(ImageNet Classification with Deep ConvolutionalNeural Networks)在ImageNet挑战赛一举夺魁，使得卷积神经网络再次引起人们的重视，并因此而一发不可收拾，卷积神经网络的研究如雨后春笋一般不断涌现，推陈出新。 AlexNet是以它的第一作者Alex Krizhevsky而命名，这篇文章中也有深度学习领域三位大牛之一的Geoffrey Hinton的身影。AlexNet之所以这么有名气，不仅仅是因为获取比赛冠军这么简单。这么多年，目标识别、目标跟踪相关的比赛层出不穷，获得冠军的团队也变得非常庞大，但是反观一下能够像 AlexNet影响力这么大的，却是寥寥可数。 AlexNet相比于上一代的LeNet它首先在数据集上做了很多工作， 第一点：数据集 我们都知道，限制深度学习的两大因素分别输算力和数据集，AlexNet引入了数据增广技术，对图像进行颜色变换、裁剪、翻转等操作。 第二点：激活函数 在激活函数方面它采用ReLU函数代替Sigmoid函数，前面我用一篇文章详细的介绍了不同激活函数的优缺点，如果看过的同学应该清楚，ReLU激活函数不仅在计算方面比Sigmoid更加简单，而且可以克服Sigmoid函数在接近0和1时难以训练的问题。 第三点：Dropout 这也是AlexNet相对于LeNet比较大一点不同之处，AlexNet引入了Dropout用于解决模型训练过程中容易出现过拟合的问题，此后作者还发表几篇文章详细的介绍Dropout算法，它的引入使得卷积神经网络效果大大提升，直到如今Dropout在模型训练过程中依然被广泛使用。 第四点：模型结构 卷积神经网络的每次迭代，模型架构都会发生非常大的变化，卷积核大小、网络层数、跳跃连接等等，这也是不同卷积神经网络模型之间的区别最明显的一点，由于网络模型比较庞大，一言半语无法描述完整，下面我就来详细介绍一下AlexNet的网络模型。 AlexNet 如果读过前面一片文章应该了解，LeNet是一个5层的卷积神经网络模型，它有两个卷积层和3个全连接层。对比而言，AlexNet是一个8层的卷积升级网络模型，它有5个卷积层和3个全连接层。 我们在搭建一个网络模型的过程中，重点应该关注如下几点： 卷积核大小 输入输出通道数 步长 激活函数 关于AlexNet中使用的激活函数前面已经介绍过，它使用的是ReLU激活函数，它5层卷积层除了第一层卷积核为11*11、第二次为5*5之外，其余三层均为3*3，下面就详细介绍一下AlexNet的模型结构， 第一层：卷积层 卷积核大小11*11，输入通道数根据输入图像而定，输出通道数为96，步长为4。 池化层窗口大小为3*3，步长为2。 第二层：卷积层 卷积核大小5*5，输入通道数为96，输出通道数为256，步长为2。 池化层窗口大小为3*3，步长为2。 第三层：卷积层 卷积核大小3*3，输入通道数为256，输出通道数为384，步长为1。 第四层：卷积层 卷积核大小3*3，输入通道数为384，输出通道数为384，步长为1。 第五层：卷积层 卷积核大小3*3，输入通道数为384，输出通道数为256，步长为1。 池化层窗口大小为3*3，步长为2。 第六层：全连接层 输入大小为上一层的输出，输出大小为4096。 Dropout概率为0.5。 第七层：全连接层 输入大小为4096，输出大小为4096。 Dropout概率为0.5。 第八层：全连接层 输入大小为4096，输出大小为分类数。 注意：需要注意一点，5个卷积层中前2个卷积层后面都会紧跟一个池化层，而第3、4层卷积层后面没有池化层，而是连续3、4、5层三个卷积层后才加入一个池化层。 编程实践 在动手实践LeNet文章中，我介绍了网络搭建的过程，这种方式同样适用于除LeNet之外的其他模型的搭建，我们需要首先完成网络模型的搭建，然后再编写训练、验证函数部分。 在前面一篇文章为了让大家更加容易理解tensorflow的使用，更加清晰的看到网络搭建的过程，因此逐行编码进行模型搭建。但是，我们会发现，同类型的网络层之间很多参数是相同的，例如卷积核大小、输出通道数、变量作用于的名称，我们逐行搭建会有很多代码冗余，我们完全可以把这些通用参数作为传入参数提炼出来。因此，本文编程实践中会侧重于代码规范，提高代码的可读性。 编程实践中主要根据tensorflow接口的不同之处把网络架构分为如下4个模块： 卷积层 池化层 全连接层 Dropout 卷积层 针对卷积层，我们把输入、卷积核大小、输入通道数、步长、变量作用域作为入参，我们使用tensorflow时会发现，我们同样需要知道输入数据的通道数，关于这个变量，我们可以通过获取输入数据的尺寸获得， 12345678def conv_layer(self, X, ksize, out_filters, stride, name): in_filters = int(X.get_shape()[-1]) with tf.variable_scope(name) as scope: weight = tf.get_variable("weight", [ksize, ksize, in_filters, out_filters]) bias = tf.get_variable("bias", [out_filters]) conv = tf.nn.conv2d(X, weight, strides=[1, stride, stride, 1], padding="SAME") activation = tf.nn.relu(tf.nn.bias_add(conv, bias)) return activation 上面，我们经过获取权重、偏差，卷积运算，激活函数3个部分完成了卷积模块的实现。AlexNet有5个卷积层，不同层之间的主要区别就体现在conv_layer的入参上面，因此我们只需要修改函数的入参就可以完成不同卷积层的搭建。 池化层 12def pool_layer(self, X, ksize, stride): return tf.nn.max_pool(X, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding="SAME") 全连接层 1234567def full_connect_layer(self, X, out_filters, name): in_filters = X.get_shape()[-1] with tf.variable_scope(name) as scope: w_fc = tf.get_variable("weight", shape=[in_filters, out_filters]) b_fc = tf.get_variable("bias", shape=[out_filters], trainable=True) fc = tf.nn.xw_plus_b(X, w_fc, b_fc) return tf.nn.relu(fc) Dropout 12def dropout(self, X, keep_prob): return tf.nn.dropout(X, keep_prob) 到这里，我们就完成了卷积层、池化层、全连接层、Dropout四个模块的编写，下面我们只需要把不同模块按照AlexNet的模型累加在一起即可， 模型 123456789101112131415161718192021def create(self, X): X = tf.reshape(X, [-1, 28, 28, 1]) conv_layer1 = self.conv_layer(X, 11, 96, 4, "Layer1") pool_layer1 = self.pool_layer(conv_layer1, 3, 2) conv_layer2 = self.conv_layer(pool_layer1, 5, 256, 2, "Layer2") pool_layer2 = self.pool_layer(conv_layer2, 3, 2) conv_layer3 = self.conv_layer(pool_layer2, 3, 384, 1, "Layer3") conv_layer4 = self.conv_layer(conv_layer3, 3, 384, 1, "Layer4") conv_layer5 = self.conv_layer(conv_layer4, 3, 256, 1, "Layer5") pool_layer = self.pool_layer(conv_layer5, 3, 2) _, x, y, z = pool_layer.get_shape() full_connect_size = x * y * z flatten = tf.reshape(pool_layer, [-1, full_connect_size]) fc_1 = self.full_connect_layer(flatten, 4096, "fc_1") drop1 = self.dropout(fc_1, self.keep_prob) fc_2 = self.full_connect_layer(drop1, 4096, "fc_2") drop2 = self.dropout(fc_2, self.keep_prob) fc_3 = self.full_connect_layer(drop2, self.num_classes, "fc_3") return fc_3 返回结果是一个1*m维的向量，其中m是类别数，以本文使用的MNIST为例，输入是一个1*10的详细，每一个数字对应于索引数字的概率值。 上述就是完整模型的搭建过程，下面我们就需要把输入传入模型，然后获取预测输出，进而构建误差函数进行训练模型。 训练验证 训练验证部分入参有3个，分别是， 输入数据 标签 预测值 其中输入数据和标签为占位符，会在图启动运算时传入真实数据，预测值为模型的输出， 12345678910111213141516171819202122232425262728293031323334def train_val(X, y, y_): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)) optimizer = tf.train.AdamOptimizer(learning_rate=LR) train_op = optimizer.minimize(loss) tf.summary.scalar("loss", loss) correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) merged = tf.summary.merge_all() writer = tf.summary.FileWriter("logs") saver = tf.train.Saver() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) writer.add_graph(sess.graph) i = 0 for epoch in range(EPOCHS): for step in range(MAX_STEPS): batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE) summary, loss_val, _ = sess.run([merged, loss, train_op], feed_dict=&#123;X: batch_xs, y: batch_ys&#125;) print("epoch : &#123;&#125;----loss : &#123;&#125;".format(epoch, loss_val)) writer.add_summary(summary, i) i += 1 saver.save(sess, os.path.join("temp", "mode.ckpt")) test_acc = 0 test_count = 0 for _ in range(10): batch_xs, batch_ys = mnist.test.next_batch(BATCH_SIZE) acc = sess.run(accuracy, feed_dict=&#123;X: batch_xs, y: batch_ys&#125;) test_acc += acc test_count += 1 print("accuracy : &#123;&#125;".format(test_acc / test_count)) 上述就是AlexNet模型搭建和训练过程。 注意：同一个模型在不同的数据集上表现会存在很大差异，例如LeNet是在MNIST的基础上进行搭建和验证的，因此卷积核、步长等这些超参数都已经进行了精心的调节，因此只需要按照模型搭建完成即可得到99%以上的准确率。而AlexNet是在ImageNet的图像上进行调优的，ImageNet的图像相对于MNIST28*28的图像要大很多，因此卷积核、步长都要大很多，但是这样对于图像较小的MNIST来说就相对较大，很难提取细节特征，因此如果用默认的结构效果甚至比不上20年轻的LeNet。这也是为什么深度学习模型可复制性差的原因，尽管是两个非常类似的任务，同一个模型在两个任务上表现得效果也会存在很大的差异，这需要工程时对其进行反复的调节、优化。 完整代码如果需要完整代码可以在github搜索项目aiLearnNotes，或者复制下方链接直接打开， https://github.com/Jackpopc/aiLearnNotes/blob/master/computer_vision/AlexNet.py 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 推荐3款令人惊艳的截图工具]]></title>
    <url>%2F2019%2F10%2F11%2Fscreenshot%2F</url>
    <content type="text"><![CDATA[前言截图是我们日常生活中经常会用到的一种功能，windows虽然自带截图功能，但是功能相对单一，灵活度较差，而且连续截图非常不方便。因此很多人会选择微信、QQ等聊天工具自带的截图工具，尽管这些工具携带的截图工具相对于windows自带的好一些，但是功能依然谈不上强大，而且最致命的一点就是，为了使用一个截图功能还要安装或打开另外一个工具，这显然是非常不方便的。 平时我也会有很多场景下需要用到截图工具，因此对截图功能的依赖相对较强，所以一直都在不断的寻觅、尝试不同的截图工具，在这个过程中试用过很多截图工具，但是有3款给我留下印象最为深刻，把这些工具推荐给大家，各位可以根据自己的需求进行选择。 Snipaste 之所以第一个推荐snipaste这款截图工具，是因为这是我使用最长，也觉得最为顺手的一款截图工具，至今我依然在使用这款截图工具，虽然本文推荐3款工具，但是如果仅仅用于截图，我还是首推snipaste这款工具。 添加文字、马赛克、涂鸦等这些功能在其他很多截图工具中也非常常见，这里就不再赘述。每款工具如果深挖都能找出长篇大论的优点，我不喜欢这样繁琐而不实用的过程，我就提几点我使用过程中觉得比较实用的、具有特色的功能， 自动识别边界框 贴图 快捷键 精细化截图 自动识别边界框，Snipaste能够自动识别窗口和窗口内部的按钮，当鼠标箭头指向对应位置时不需要拖动鼠标，即可截图。 贴图，我认为这是Snipaste最大的特色之一，也是比较吸引我的一点，你能够把截取的图像贴到屏幕上，始终置定，这样无论你打开什么窗口贴图都会一直显示，这样就能够避免重复的在不同窗口之间切换，能够大大提高效率。 快捷键，这是最初吸引我使用这款工具的原因。很多截图工具，包括windows自带的截图工具都有快捷键，为什么却偏偏Snipaste吸引到我了？因为它的快捷键设置简单、容易记忆、使用方便，&lt;F1&gt;截图、&lt;F3&gt;贴图，不需要使用复杂的组合键，这对于我这种懒的记忆的人是一个非常具有吸引力的点。 精细化截图，截图过程中snipaste可以精确的显示鼠标所在点的坐标，颜色，它可以精确到像素级进行调整，能够精确的截图我们所需要的内容。 FastStone Capture Snipaste已经足够强大了，至少对于我这种仅限于静态截图的已经足够使用了。但是它并不是万能的。如果非要选择一款能够和Snipaste抗衡，能够弥补Snipaste不足的截图工具，那么一定非FastStone Capture莫属，这里提几点Snipaste所不具备的功能, 捕捉手绘区域 录屏 滚动截屏 捕捉手绘区域，你可以通过FastStone Capture捕捉任意你希望截取的形状，这相对于大多数截图工具仅限于截图矩形框而言，的确是一个特色。 录屏，和截图一样，录屏在很多场景下也是很常用的一个功能，例如，演示、教学、游戏等，虽然能够在网上找到很多录屏工具，但是大多数比较臃肿。相对而言，就体现出FastStone Capture的强大之处，单纯把它当作一个录屏工具它就非常强大了，在录屏时有很大的灵活性，你可以手动选择录屏区域，也可以选择窗口进行录屏，也可以选择录制全屏。 滚动截屏，我们都知道现在手机端已经实现了滚动截屏，这样在我们浏览网页、分享、收藏过程中非常实用，但是电脑端却很少有截图工具实现这项功能，FastStone Capture就可以实现电脑端的滚动截屏，只需要点击鼠标左键它能够自动滚动截屏，非常方便。 ShareX Snipaste、FastStone Capture这两款工具应该很多人都听说过或者自己也用过，因此前面就简单的介绍了一下，但是对于ShareX，我觉得需要大书特书一番，这是一款不仅限于截图的强大工具。 如果说snipaste和FastStone Capture主要围绕截图、录屏进行展开，那么ShareX可以说是一款强大的综合工具，甚至可以说它的功能能够让你觉得非常惊艳，可以把它称为截图领域的百宝箱。 截图 既然本文是以以截图展开的，那么提到ShareX还是首先讲一下它的截图功能，可以认为它是前面两款截图工具的结合体，它不仅有常见的全屏、窗口截图，它还有如下截图功能， 网页捕捉 文本捕捉 滚动捕捉 录制GIF 滚动捕捉 可以说是一款集截图和录制为一体的工具。 OCR 我们在日常工作中经常会遇到无法复制或者编辑PDF或者某些网站的文字内容，这时候我们就不得不借助一些PDF转换工具，但是效果差强人意。因此，剩下唯一的方法就是选择OCR识别文字。在之前的文章里我介绍过两款OCR工具，分别是电脑端的天若OCR和手机端的白描，其实，ShareX这款”不务正业”的截图工具也具备这项功能，它集成了OCR识别API，另外，它能够支持中文、英文、德语、法语等25种语言的识别。 工具箱 除了强大的OCR识别之外，ShareX还包含很多实用的工具集，例如， 拾色器 图像合成 二维码生成和解码 尺子 哈希值检查 …… 连续动作 我觉得这是ShareX在截图方面对比于snipaste和FastStone Capture的一大”杀器”。 首先想一下，我们截图之后会做哪些动作？ 发送给别人 保存到本地 上传到图床 以我为例，我平时需要维护我的个人主页，因此我截图前后需要经历以下几个步骤： 截图 保存到本地 上传到图床 拷贝URL 对于一副截图，我需要先后经历4个动作，因此，我在维护个人主页时最为头疼的点就是对于图片的处理，但是有了ShareX能够轻松的把这4个动作精简为一个动作。 ShareX可以定义截图后的动作和上传后的动作，例如截图后的动作包含打印、保存、复制到剪切板、上传，而上传后的动作包含分享网址、复制URL到剪切板。因此，通过配置连续的截图后的动作和上传后的动作就可以简单的截图后完成一系列的动作，我觉得这对于我来说真的解决了一个大问题。 当然，每个人都可以根据自己的工作场景自定义一个连续的动作，例如，你平时经常需要把截图打印出来，你可以定义截图后的动作为打印。如果你想快捷的使用OCR文字识别功能，你也可以把截图后的动作定义为文字识别。 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>截图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理 | Spark&HDFS集群配置及基本使用]]></title>
    <url>%2F2019%2F10%2F08%2Fspark%2F</url>
    <content type="text"><![CDATA[Spark简介 Spark是由加州伯克利大学AMP实验室于2009年开发并于2013年加入Apache的开源大数据并行计算框架，它凭借自身独有的优势迅速成为Apache三大分布式计算框架之一，对比于常用的hadoop，它具有低时延、速度快、通用性强等优点。此外，Spark具有完善的生态系统，在资源调度方面它拥有Mesos和YARN，在存储方面它支持本地文件系统、HDFS、Amazon S3、HBase等，在数据仓库方面它拥有Hive SQL、Spark SQL，在接口方面它拥有mlib、GraphX等。 除了运算和生态方面的优势，Spark在数据处理方式方面同时支持批计算和流计算，虽然Spark在流计算方面不如storm、flink能够支持毫秒级别，但是对于大多数对实时性要求不高的在线计算已经足够使用。 基于上述众多优点使得Spark成为一个非常热门和受欢迎的大数据处理框架，目前在很多大型公司被广泛使用。 Spark不仅可以支持集群模式，还可以支持单机模式，但是我认为之所以使用大数据处理框架，它的主要优势就体现在多机并行方面，随着数据集的增加和节点数量的增加，它的对比于传统并行模式和其他大数据处理框架的优势更加明显。单机Spark配置相对集群配置相对简单一些，也节省很多步骤，因此，本文就讲解一下集群Spark配置方式，本文的配置是建立在已经配制好JDK的基础上，所以不再详细介绍JDK的安装和配置。 Hadoop集群环境搭建 Spark可以读取多种数据源的数据，例如Amazon s3、HBase、HDFS、本地文件系统，由于数据存放在某一个节点路径下，在Spark集群的其他节点无法直接读取相应路径下的数据，而HBase、Amazon s3这些存储服务在很多场景下很难满足，例如学校实验室。因此本文就以HDFS为例来进行讲解。 Hadoop主要包括两个部门，HDFS文件存储系统用于存储数据源，MapReduce用于从文件存储系统读取数据并进行分布式处理，由于本文只用到文件存储系统HDFS，用不到MapReduce，所以本文就配置一下集群Hadoop，讲解HDFS的使用，不深入研究MapReduce的使用。 准备工作 首先要保证集群中不同节点能够互相通信，然后为每个节点配置对应的hostname，后面会用到， 12345$ vim /etc/hosts10.110.113.132 master10.110.113.133 slave010.110.113.134 slave110.110.113.135 slave2 上述master和slave是每个节点的hostname，可以作为IP的地带，通过ping的方式可以测试hostname是否正常通信，可以在master节点上测试是否连接到不同的slave节点， 123$ ping slave0或者$ ping 10.110.113.133 注：hostname的配置可以通过sudo vim /etc/hostname修改文件进行配置。 ssh无密码登陆集群机器 由于集群配置hadoop涉及多台机器，当在master节点启动或者关闭集群hadoop时需要输入所有slave节点的密码，这样显然太麻烦，因此需要配置无密码登陆，这样后续启动时就不需要输入密码， 首先，如果节点没有安装ssh需要安装ssh， 1$ sudo apt-get install ssh 然后，在每个节点上输入下面命令，测试是否能够正常登陆每个节点 1$ ssh localhost 为了保证master节点能够无密码登陆所有slave节点，需要首先生成master节点的公钥， 1$ ssh-keygen -t rsa 一直点击确定即可，然后会在home路径下生成两个文件，id_rsa和id_rsa.pub，这时需要把id_rsa.pub的内容追加到authorized_keys后面，然后把master节点id_rsa.pub拷贝到所有slave节点并追加到所有slave节点authorized_keys的后面， 首先在master节点执行操作， 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 然后把master节点生成的id_rsa.pub拷贝到所有slave节点， 123$ scp ~/.ssh/id_rsa.pub user_name@slave0:/home/user_name/$ scp ~/.ssh/id_rsa.pub user_name@slave1:/home/user_name/$ scp ~/.ssh/id_rsa.pub user_name@slave2:/home/user_name/ 上述user_name是slave节点的用户名，然后把id_rsa.pub追加到每个slave节点authorized_keys的后面， 1$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 然后可以在master节点上通过下方命令测试是否能够正常登陆每个slave节点， 123$ ssh slave0$ ssh slave1$ ssh slave2 注意：上述都是假设master和每个slave节点的用户名user_name一样的前提下，如果不一样需要修改~/.ssh/config文件。 安装Hadoop 打开下面链接进入到下载页面，点击下载binary文件，把hadoop-3.2.1.tar.gz文件下载到home路径下， https://hadoop.apache.org/releases.html 然后解压下载的文件到指定目录，同时需要修改对应目录的拥有者，因为hadoop在启动后会记录日志文件，如果不修改拥有者则没有权限写入文件，无法正常启动， 1234$ sudo tar -zxvf ~/下载/hadoop-3.2.1.tar.gz -C /usr/local $ cd /usr/local/$ sudo mv ./hadoop-3.2.1/ ./hadoop $ sudo chown -R user_name ./hadoop 然后，把hadoop路径加入到环境变量里，如果需要长期有效，需要修改~/.bashrc， 12export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 然后保存退出，执行下面命令让环境变量生效， 1$ source ~/.bashrc 集群配置 首先进入到Hadoop配置文件所在目录， 1$ cd /usr/local/hadoop/etc/hadoop 然后修改slave节点配置文件， 1234$ vim slavesslave0slave1slave2 注意：上述slave0~2不是IP地址，是前面配置的hostname。 首先配置core-site.xml， 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 上述内容只需要修改hdfs://master:9000这一句即可，其他的不需要修改，需要根据自己定义的master节点的hostname进行修改，例如你的master节点的hostname是hadoop，那么就需要修改成hdfs://hadoop:9000，端口默认为9000， 然后配置hdfs-site.xml， 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这里dfs.replication默认值是3，它的含义是数据副本备份的数量，如果slave节点数量小于这个数的话则会报错，所以为了防止报错可以把这个值改为1，当然如果slave节点大于等于3的话也可以不修改。 如果需要使用MapReduce还需要配置mapred-site.xml，因为本文只使用HDFS而不实用MapReduce，因此就不配置mapred-site.xml了。 另外，如果需要yarn资源调度的话，需要修改yarn-site.xml，yarn是一个资源调度和分配工具，Spark本身自带资源管理器，也可以与yarn、Mesos结合使用，本文就使用Spark自带的资源管理器。 最后，需要在hadoop中配置JAVA环境， 12$ vim /usr/local/hadoop/etc/hadoop/hadoop-env.shexport JAVA_HOME=/path/to/java 上述/path/to/java需要替换成自己JAVA安装的路径。 slave节点配置 上述全部操作都是在master节点进行的，下面需要配置slave节点。配置slave节点比较容易，只需要把文件打包复制到各个slave节点，解压即可， 1234$ tar -zcf ~/hadoop.tar.gz /usr/local/hadoop$ scp ~/hadoop.tar.gz slave0:~/$ scp ~/hadoop.tar.gz slave1:~/$ scp ~/hadoop.tar.gz slave2:~/ 然后在各个slave节点上都执行下面相同操作， 123$ sudo rm -rf /usr/local/hadoop$ sudo tar -zxvf ~/hadoop.tar.gz -C /usr/local$ sudo chown -R user_name /usr/local/hadoop 启动Hadoop集群 通过上述的配置，master节点和slave节点的Hadoop都配置好了，下面需要做的就是启动集群上每个节点的Hadoop， 123$ cd /usr/local/hadoop$ ./bin/hdfs namenode -format$ ./sbin/start-all.sh 注意：./bin/hdfs namenode -format这一句命令很重要，不能缺少。如果前面没有配置ssh免密登陆，执行./sbin/start-all.sh时会让输入密码。 然后在每个节点上执行下面命令，会在master节点上看到多了一个NameNode，slave节点上会多出DataNode， 1$ jps HDFS简单使用HDFS的使用和Linux命令非常相似，例如上传数据用put，创建目录用mkdir，查看目录内容用ls，删除目录用rm，但是也有不同之处，下面就来看一下简单的示例， 创建目录 1$ ./bin/hdfs dfs -mkdir -p /hello 上传文件到HDFS 首先先新建一个本地文件， 1234$ vim ~/test.txthello worldhello worldhello world 然后使用下面命令进行上传， 1$ ./bin/hdfs dfs -put ~/test.txt /hello 查看目录和文件内容 123456$ ./bin/hdfs dfs -ls /hello/hello/test.txt$ ./bin/hdfs dfs -cat /hello/test.txthello worldhello worldhello world 然后我们就可以用Spark访问hdfs文件系统的文件， 1text_file = sc.textFile(&quot;hdfs://master:9000/hello/text.txt&quot;) 文件的路径分为两部分，一部分是前面core-site.xml中配置的hostname:port，一部分是HDFS上传文件的相对路径。 Spark集群环境搭建 如果已经理解了上述Hadoop集群环境的搭建，那么学习Spark集群环境的搭建会容易很多，因为Hadoop和Spark不仅安装包目录结构非常相似，在配置方面也十分接近。均是在master节点上进行所有配置，然后打包复制到每个slave节点，然后启动集群Spark即可，下面就来详细介绍一下Spark集群环境的搭建。 下载安装 进入Spark的下载目录， https://spark.apache.org/downloads.html 可以看到Spark分多个版本，有基于Hadoop构建好的，有没基于Hadoop构建的，有基于Hadoop2.6之前版本构建的，也有基于Hadoop2.7以后版本构建的，由于前面讲解Hadoop集群环境搭建时采用的是Hadoop 3.2.1，因此，而且本文需要使用HDFS依赖Hadoop，因此需要下载Pre-built for Apache Hadoop 2.7 and later, 把spark-2.4.4-bin-hadoop2.7.tgz文件下载到home路径下，然后解压到指定目录， 1$ tar -zxvf ~/spark-2.4.4-bin-hadoop2.7.tgz -C /usr/local/ 然后进入目录并像Hadoop那样，修改Spark目录的拥有者， 123$ cd /usr/local$ sudo mv ./spark-2.4.4-bin-hadoop2.7 ./spark$ sudo chowm -R user_name ./spark 配置环境变量 修改bashrc，配置环境变量，把Spark的bin和sbin路径加入到环境变量， 12345$ vim ~/.bashrcexport SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATHexport PYSPARK_PYTHON=python3 Master节点配置 进入Spark目录，修改spark-env.sh文件， 12$ cd /usr/local/spark$ vim ./conf/spark-env.sh 在spark-env.sh中添加下面内容， 123export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopexport SPARK_MASTER_IP=10.110.113.132 SPARK_MASTER_IP指定的是master节点的IP，后面启动集群Spark时slave节点会注册到SPARK_MASTER_IP，如果这一项不配置，Spark集群则没有可使用资源， 修改slaves文件 配置完master节点信息之后需要配置slave节点信息，slave节点的信息配置在slaves文件里，由于Spark目录下没有这个文件，因此需要首先从slaves.template拷贝一下， 12$ cd /usr/local/spark/$ cp ./conf/slaves.template ./conf/slaves 然后添加如下内容， 123slave0slave0slave1 需要注意的是，slaves文件里配置的是运行作业任务的节点(worker)，这样的话master的节点只作为控制节点，而不作为工作节点，如果需要把master节点的资源也充分利用起来，需要把master节点也加入到slaves文件中。 slave节点配置 首先在master节点上把配制好的目录进行打包，拷贝到每个slave节点上， 12345$ cd /usr/local$ tar -zcf ~/spar.tar.gz ./spark$ scp ~/spark/tar.gz slave0:~/$ scp ~/spark/tar.gz slave1:~/$ scp ~/spark/tar.gz slave2:~/ 然后在每个slave节点上执行下方命令，把文件解压到相应路径下， 123$ sudo rm -rf /usr/local/spark$ sudo tar -zxvf ~/spark.tar.gz -C /usr/local$ sudo chown -R user_name /usr/local/spark 这样就完成了slave节点的配置。 启动Spark集群 如果要使用HDFS的话，在启动Spark集群前需要先启动Hadoop集群， 12$ cd /usr/local/hadoop/$ ./sbin/start-all.sh 然后进入Spark目录，启动Spark集群， 12$ cd /usr/local/spark$ ./sbin/start-all.sh 需要说明一下，前面配置Hadoop集群是提到，需要配置ssh免密登陆，对于Spark也是同样的道理，如果不配置ssh免密登陆的话，执行./sbin/start-all.sh会提示输入密码。 除了使用./sbin/start-all.sh启动Spark集群外，还可以分开启动，先启动master节点，然后启动slave节点， 12$ ./sbin/start-master.sh$ ./sbin/start-slaves.sh 如果前面没有完成Master节点配置指定master节点IP，那么执行./sbin/start-slaves.sh时则无法注册master节点的IP，这样集群计算资源则无法使用。除了配置spark-env.sh指定master节点IP外，还可以通过下面方式指定注册的master节点IP， 1$ ./sbin/start-slave.sh 10.110.113.132 然后分别在master节点和slave节点执行下面命令会看到分别多出一个Master进程和Worker进程。 Spark基本使用运行原理 如果使用过tensorflow的话，应该对Spark的使用很容易理解，Spark的计算过程和tensorflow有相似之处。 回忆一下，我们在使用tensorflow时需要首先构造一个计算图，然后实例化一个session，然后用session.run来启动图运算。 其实Spark也是这样，RDD(弹性分布式数据集)是Spark中最重要的概念之一，它提供了一个共享内存模型。Saprk的执行过程中主要包括两个动作：转换与行动。其中转换操作就如同tensorflow中的构造计算图的过程，在这个过程中Spark构造一个有向无环图(DAG)，但是不进行运算，输入为RDD输出则是一个不同的RDD，当执行行动操作时就如同tensorflow中的session.run，开始执行运算。 Spark中有很多转换操作，例如， groupByKey reduceByKey sortByKey map filter join …… 行动操作包括， count collect first foreach reduce take …… 运行模式 Spark中通过master url来执行Spark的运行模式，Spark的运行模式包括本地运行、集群运行、yarn集群等，关于Spark master url的指定不同运行模式的含义如下， URL值运行模式local使用1个线程本地化运行local[K]使用K个线程本地化运行local[*]使用逻辑CPU个数数量的线程来本地化运行spark://HOST:PORT指定集群模式运行Sparkyarn-cluster集群模式连接YARN集群yarn-client客户端模式连接YARN集群mesos://HOST:PORT连接到指定的Mesos集群 示例下面就以一个简单的示例把前面Hadoop和Spark串联在一起，讲解一下HDFS+Spark的使用方法。 上传数据到HDFS 新建一个hello_world.txt的本地文件，并在文件中添加3行hello world，然后上传至HDFS， 12345$ cd /usr/local/hadoop/$ ./bin/hdfs dfs -mkdir -p /usr/hadoop$ touch hello_world.txt$ echo -e &quot;hello world \nhello world \nhello world&quot; &gt;&gt; hello_world.txt$ ./bin/hdfs dfs -put ./hello_world.txt /usr/hadoop 编写Spark程序 新建一个spark.py的Python文件， 1$ vim spark.py 添加如下内容， 1234567from pyspark import SparkConffrom pyspark import SparkContextconf = SparkConf().setAppName(&quot;FirstProject&quot;).setMaster(&quot;local[*]&quot;)sc = SparkContext.getOrCreate(conf)rdd = sc.textFile(&quot;hdfs:///master:9000/usr/hadoop/hello_world.txt&quot;)rdd.map(lambda line: line).foreach(print) 然后运行程序， 1234$ python spark.pyhello worldhello worldhello world 以上就是Spark的集群配置过程和基本使用方法。 往期内容开发工具 | 你真的会用jupyter吗？ 【动手学计算机视觉】第十四讲：正则化之Dropout 【动手学计算机视觉】第十五讲：卷积神经网络之LeNet 更多精彩内容，请关注公众号【平凡而诗意】，或者收藏我的个人主页~]]></content>
      <categories>
        <category>IT技术</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第五讲：迭代器与生成器]]></title>
    <url>%2F2019%2F10%2F08%2Fgenerator-iterator%2F</url>
    <content type="text"><![CDATA[前言迭代是Python中常用且非常强大的一个功能，它可以用于访问集合、列表、字符串、字典等数据结构的元素。我们经常使用循环和条件语句，我们也清楚哪些是可以迭代访问，但是具体它们之间有什么有什么异同之处？有哪些特点？什么是迭代器、什么是生成器、什么是可迭代对象？这些问题对于初学者而言却是很少去细致的研究，本文就来详细阐述一下它们之间的关系已经它们的特别之处。 可迭代对象 在讲解迭代器和生成器之前，先介绍一下可迭代对象。 可迭代对象是Python中一个非常庞大的概念，它主要包括如下三类： 迭代器 序列 字典 从上图可以看出不同概念之间的关系，迭代器是可迭代对象的一个子集，而生成器又是迭代器的一个子集，是一种特殊的迭代器。除了迭代器之外，Python中还有序列、字典等可迭代对象。 现在已经直观的了解了可迭代对象与迭代器、生成器之间的关系，那么用Python语言怎么表述它们的区别呢？ 可迭代对象需要实现iter方法 迭代器不仅要实现iter方法，还需要实现next方法 在使用层面，可迭代对象可以通过in和not in访问对象中的元素，举一个例子， 123456789101112131415161718X = set([1,2,3,4,5])print(X)print(type(X))print(1 in X)print(2 not in X)for x in X: print(x) # 输出&#123;1, 2, 3, 4, 5&#125;&lt;class &apos;set&apos;&gt;TrueFalse12345 前面提到，可迭代对象实现了iter方法，但是它没有实现next，这也是判定迭代器和其他可迭代对象的关键之处，可以看一下通过next访问上述示例中可迭代对象X会报错， 1234next(X)# 输出TypeError: &apos;set&apos; object is not an iterator 报的错误是‘set’ object is not an iterator，它指明了set集合是一个可迭代对象，但不是迭代器，下面就来介绍一下迭代器。 迭代器迭代器是可迭代对象的一个子集，它是一个可以记住遍历的位置的对象，它与列表、元组、集合、字符串这些可迭代对象的区别就在于next方法的实现，其他列表、元组、集合、字符串这些可迭代对象可以很简单的转化成迭代器，通过Python内置的iter函数能够轻松把可迭代对象转化为迭代器，下面来看一个例子， 1234567891011121314X = [1,2,3,4,5]print(type(X))Y = iter(X)print(type(Y))print(next(Y))print(next(Y))print(next(Y))# 输出&lt;class &apos;list&apos;&gt;&lt;class &apos;list_iterator&apos;&gt;123 从上述示例中我们可以看出两点： 通过iter函数把list转化成了迭代器 可迭代器能够记住遍历位置，能够通过next方法不断从前往后访问 除了Python内置的iter之外，还可以通过Python内置的工具包itertools创建迭代器，其中函数包括， count cycle repeat accumulate chain compress dropwhile islice product permutations combinations …… itertools中包含很多用于创建迭代器的实用方法，如果感兴趣嗯可以访问官方文档进行详细了解。 当然，也可以自己通过实现iter和next方法来定义迭代器， 12345678910111213141516171819202122232425262728class Iterator(object): def __init__(self, array): self.x = array self.index = 0 def __iter__(self): return self def __next__(self): if self.index &lt; len(self.x): value = self.x[self.index] self.index += 1 else: raise StopIteration return value it = Iterator([1,2,3,4,5])print(type(it))for i in it: print(i)# 输出&lt;class &apos;__main__.Iterator&apos;&gt;12345 生成器从文章开头的流程图可以直观的看出，生成器是迭代器的子集，换句话说，生成器一定是迭代器，但是迭代器不全是生成器对象。 提及生成器就不得不提及一个Python中的关键字yiled，在Python中一个函数可以用yiled替代return返回值，这样的话这个函数就变成了一个生成器对象，举个例子对比一下， 123456789def generator(array): for i in array: return i gen = generator([1,2,3,4,5])print(type(gen))# 输出&lt;class &apos;int&apos;&gt; 这是我们常见的return返回方式，这样的话generator函数获取的是一个int型对象，下面看一下换成yield关键字， 123456789def generator(array): for i in array: yield(i) gen = generator([1,2,3,4,5])print(type(gen))# 输出&lt;class &apos;generator&apos;&gt; 这样的话获取的是一个生成器generator，除了yield之外，在Python3.3之后还加入了yield from获取生成器，允许一个生成器将其部分操作委派给另一个生成器，使得生成器的用法变得更加简洁，yield from后面需要加上可迭代对象，这样可以把可迭代对象变成生成器，当然，这里的可迭代对象不仅包含列表、元组，还包含迭代器、生成器。yield from相对于yield的有几个主要优点： 代码更加简洁 可以用于生成器嵌套 易于异常处理 下面就从简洁代码方面举个例子说明一下， 1234567891011121314def generator(array): for sub_array in array: yield from sub_arraygen = generator([(1,2,3), (4,5,6,7)])# 输出1234567 当我们需要访问多层/多维可迭代对象时，我们就不需要逐层的去用for … in …去访问，可以简单的通过yiled from把生成器委派给子生成器，除此之外还可以通过生成器表达式的方法得到生成式，后面会介绍。 123456print(next(gen))print(next(gen))# 输出12 通过上面示例可以看出，生成器可以像迭代器那样使用iter和next方法。 读到这里可以会有疑惑，从这个示例看来生成器和迭代器并没有什么区别啊？为什么生成器还可以称得上是Python中的一大亮点？ 首先它对比于迭代器在编码方面更加简洁，这是显而易见的，其次生成器运行速度更快，最后一点，也是需要着重说明的一点：节省内存。 也许在一些理论性实验、学术论文阶段可以不考虑这些工程化的问题，但是在公司做项目时，内存和资源占用是无法逃避的问题 。如果我们使用其他可迭代对象处理庞大的数据时，当创建或者返回值时会申请用于存储整个可迭代对象的内存，显然这是非常浪费的，因为有的元素当前我们用不到，也不会去访问，但它却一直占用这内存。这时候就体现了生成器的优点，它不是一次性把所有的结果都返回，而是当我们每读取一次，它会返回一个结果，当我们不读取时，它就是一个生成器表达式，几乎不占用内存。 生成器表达式首先来看一个对比示例， 1234567891011X = [1, 2, 3, 4, 5]it = [i for i in X]gen = (i for i in X)print(type(X))print(type(it))print(type(gen))# 输出&lt;class &apos;list&apos;&gt;&lt;class &apos;list&apos;&gt;&lt;class &apos;generator&apos;&gt; 首先说一下it = [i for i in X]，这种用法叫做列表生成式，在很多编程规范中非常推崇的一种替代for循环的方式，仔细看一下代码会发现，it = [i for i in X]与gen = (i for i in X)的区别非常小，只是一个用了中括号，一个用了小括号，但是它们的区别缺失非常大的，使用中括号的叫做列表生成式，获得的返回值是一个列表，而使用小括号叫做生成器表达式，获得的返回结果是一个生成器，这也是前面提到的，除了使用yield和yield from两个关键字外还可以使用生成器表达式获得生成器。 往期内容开发工具 | 你真的会用jupyter吗？ 【动手学计算机视觉】第十四讲：正则化之Dropout 【动手学计算机视觉】第十五讲：卷积神经网络之LeNet 更多精彩内容，请关注公众号【平凡而诗意】，也可以收藏我的个人主页~]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 推荐10款令人惊艳的免费手机APP]]></title>
    <url>%2F2019%2F09%2F22%2Fapp%2F</url>
    <content type="text"><![CDATA[前言我是一个对各种软件、APP充满着好奇心的人，空闲的时候就会打开应用市场，花费上几分钟时间看一下最近有没有什么新鲜的APP。 我也是一个非常挑剔的人，尽管一个APP相对于整部手机的内存可以忽略不计，但是当体验APP时不足以令我惊艳时我还是会毫不犹豫的把它卸载。 久而久之，我手机上的APP并没有多起来，但是经过各种筛选留下来的我认为都是有很多亮点的。本文就推荐10款不错的APP，后续如果遇到什么好用的APP会再次推荐。 备注：个人使用的是荣耀手机，所以下述APP均可在华为应用市场下载，其他Android系统的品牌手机应该也可以找到对应的APP。 1. 一个木涵 我认为这款APP可以称得上”APP中的百宝箱”，它一款APP集合了几十甚至上百款APP的功能，它的包括但不限于如下功能， OCR图片文字识别 文件清理 植物识别 动物识别 菜品识别 果蔬识别 历史上的今天 进制转换 画板 时间屏幕 汇率转换 快递查询 网速测试 指南针 二维码生成 GIF合成 表情制作 获取壁纸 …… 2. 全历史 当我体验一段时间全历史这款APP时我被惊艳了，令人耳目一新。 我是一个历史爱好者，也是我坚持时间最长的一个兴趣爱好，但是这么多年来有关历史的书籍、工具从没有让我眼前一亮的感觉，大多数都是枯燥乏味、千篇一律、高深莫测，也许这就是越来越少的人喜欢历史的原因吧。 当我看到这一款APP时，我感觉它真的与众不同，它包含历史地图、关系图谱、国家历史、古书籍、画作、疆域变迁等内容。以时间轴的方式直观展现历史的变迁和上下承接关系，非常吸引人。另外，关系图谱这项功能引起了我的极大兴趣，它直观的展现你所关注人物的关系网络，甚至可以推至几百年前，让你觉得原来历史这么奇妙。 3. 咔嗒 多媒体盛行的时代，图像在里面占据着不可替代的地位。我同样也是一个摄影爱好者，每当去到一个地方喜欢拍着一些体现不同风土人情、自然风貌的照片。但是我又是一个对修图一窍不通的人，因此每当和朋友谈起不修图时我美其名曰”我喜欢自然、原始的样貌”。其实，适当的修图能够让一幅图像更加有意境，更加吸引人。但是PS这些技术太难学了，虽然近几年所谓的“一键美化”工具层出不穷，但是我觉得99%的APP都是噱头，与其说一键美化，倒不如说是“一键不伦不类”。 当体验咔嗒这款APP时我并没有抱多大希望，但是体验后顿时让我改观，效果真的让其他美化APP黯然失色。它是一款基于AI的美化工具，能够精准识别图像中的物体，例如汽车、天空、海、建筑物等，然后根据不同的场景选择对应的滤镜对其进行美化，真正可以称得上“一键美化”，上图中左图为原图，右图为一键美化的图片，没有经过任何手动调整的图片。 4. 不做手机控 手机在我们生活中的地位越来越无法取代，学习时、工作时、走路时、吃饭时、排队时…无论何时何地总是不乏低头族。慢慢的手机成了让人又爱又恨的东西，它的确给生活带来了很大的便捷和乐趣，但是也的确占据了很多宝贵的时光。可是没办法，自制力是一件很难做到的事情，仅凭自己的意志太困难了。 和美化工具一样，近几年批判手机负面影响的人越来越多的同时，习惯养成类APP也层出不穷，但是我认为大多数都是在向用户进行妥协，在一些不痛不痒的地带进行约束，制约权还是在用户自身。我觉得不做手机控是一个另类，它相比于其他APP要求更高的权限，当然，它的控制力度更强，当你在计划工作时间内它可以约束除白名单外的任何APP，连自己都无法解开，除非消耗积分。此外，它还有睡眠计划、白噪声、手机使用统计等实用功能，让你对自己的使用情况一目了然。 5. Stork Stork中文名文献鸟，是一款由斯坦福大学研究人员开发的一款文献追踪神器。 对于高校学生、科研人员，或者从事偏研究性质岗位的工作人员而言，需要不断的跟踪自己所在领域的前言成果。当然这有很多种方法，可以用谷歌学术订阅对应作者的动态，可以每天关注arXiv，但是我个人并不喜欢这样的方式，以谷歌学术为例，假如我订阅李飞飞的相关文章，她每年挂名的文章可能多达几十篇，而且我认为其中90%是不值得花费时间看的，换句话说，通过arXiv和谷歌学术追踪到的信息很多是没价值的。 而Stork不同，你可以灵活使用这款APP，可以像谷歌学术那样通过作者名进行订阅，也可以通过关键字进行订阅，它会按照你自己规定的推送频率给你邮件推送相关文章，按照影响因子进行排序，同时还会提供相应文章的下载链接，非常方便。 6. 句读 优美的句子总是在不经意之间直击人的心灵深处， 人在无端微笑时，不是百无聊赖，就是痛苦难当。(王小波《黄金时代》) 我只所以继续句读这款APP是因为它的特立独行，信息爆炸的时代，各种文字类相关的APP都变了味道，每天推送数不清的文章，而大多数都是没有任何印象的内容，这就锻炼一个人从嘈杂内容中过滤有价值信息的能力。还好有这款句读APP，它每天精选一句话，仅仅一句话，从社区中提炼出一句热门、优美的句子。也许每天你只需要停留5秒钟，久而久之就会发现自己收获了很多。 7. 白描 图片和PDF转换成可编辑的文字一直以来都是一件令人困扰的事情，转化后的文本混乱，质量差。因此，一直以来我并不信任各种所谓的OCR工具，直到遇到白描这款APP，能够精准、高效的识别图片中的文字，转换后的文本整洁、干净，同时速度非常快，可以看一下上图中识别图片中文字的效果。 另外需要说明一下，普通用户每天限制识别5次，我觉得这对于大多数用户已经足够使用甚至用不完。 8. 夜间护眼 我们对手机的依赖越来越重，看书、追剧、看新闻、社交等等，每天眼睛对着屏幕的时间占比非常高，久而久之，对眼睛的伤害自然很严重。一些手机制造商逐渐的开始在手机中加入夜间模式，其实完全没有必要苦苦等待一个定制化的系统功能，不如尝试一下夜间护眼这款APP，它支持夜间、助眠、阅读、游戏等模式，还支持手动调节屏幕颜色，能够针对不同场景设置不同的屏幕颜色，这样在保证我们使用手机的舒适感的情况下同时保护了我们的眼睛。 此外，它还有很多实用的小功能，例如强制横屏让你的手机秒变平板、视力测试等。 9. 讯飞语记 我认为养成记录是一个很不错的习惯，俗话说“好记忆不如一个烂笔头”，但是道理都懂，记录是一件让人觉得很麻烦的事情。讯飞语记让这件事情变的简单起来，讯飞在语音识别领域的积淀自然不用多讲，非常厉害，它能够通过对你说话的语音进行快速、准确的识别，然后做好记事记录。此外，它不仅限于一款语音记事APP，它还支持文字识别、待办事项，同时还可以用于笔记排版。 此外，语言方面讯飞语记在语言/方言方面支持普通话、英语、粤语、四川话、东北话、河南话、山东话、上海话、武汉话、闽南语。 10. 藏书馆 也许，电子阅读有很多利润可图，因此，越来越多的互联网、传统出版公司开始进入这一块领域。那些被夸赞的天花烂坠的阅读APP试用了很多，无一例外，均采用少量免费诱导，然后让你不得不付费的手段，直到我遇到藏书馆这款APP，可以说让我豁然开朗，和其他电子阅读千篇一律的模式不同，藏书馆采用开放、共享的运营模式，其他书友可以在这里分享他们的书籍，然后你可以免费的借阅，这就给这款阅读APP带来两个无法比拟的优势： 免费 资源丰富 例如，前段时间朋友向我推荐了一本曼昆的《经济学原理》，我在多款APP上都没有找到这本书籍，最终却在藏书馆这款APP上找到很多阅读资源。]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>APP</tag>
        <tag>手机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通用技术 | 正则表达式]]></title>
    <url>%2F2019%2F09%2F21%2Fregex%2F</url>
    <content type="text"><![CDATA[前言正则表达式(Regular Expression)是一种既简单，又使用非常广泛的一种技术，我在题目中称其为”必会技术”，这并不夸张，无论是从事开发工作还是做一些文案类工作，甚至日常使用电脑搜索一个文件，都会接触到正则表达式。当然，在很多场景下不使用正则表达式也可以完成同样的工作，但是相对要麻烦很多。 正则表达式，不熟悉的人第一眼看上去很容易把它认为是一种数学或者其他理工科中的一种表达公式，其实它和公式并没有太大关系。正则表达式是一种文本匹配模式，它能够使用一个或多个简单的字符，去描述匹配一些复杂句法的规则，例如网页、身份证号、出生年月日、家庭住址等。 正是因为它的简单且不失强大的特性，它在很多地方都有着应用，一下举几个例子， - everything文件搜索 - Python、JS、PHP相关开发 - pycharm、sublime等编辑器及IDE - 爬虫 - ...... 也许说到这里还是有一些同学感觉云里雾里，下面就以一个简单的例子来说明一下。 假如，给你一个长篇的文章或者上万字的网页，你想从其中匹配出里面的信息，例如身份证号，你会怎么办？当然，这有很多种方法，甚至可以采用最笨的遍历方法去寻找，但是这并不是一个聪明的选择，我们这时就可以使用一个简短的正则表达式来进行匹配， 假设下面是要匹配的文本，在陆游的《钗头凤·红酥手》和唐婉的《钗头凤·世情薄》之间插入一个18位、末位为X的假身份证号， 12345红酥手，黄縢酒，满城春色宫墙柳。东风恶，欢情薄。一怀愁绪，几年离索。错、错、错。春如旧，人空瘦，泪痕红浥鲛绡透。桃花落，闲池阁。山盟虽在，锦书难托。莫、莫、莫！23453419901011908X世情薄，人情恶，雨送黄昏花易落。晓风干，泪痕残，欲笺心事，独语斜阑。难，难，难！人成各，今非昨，病魂常似秋千索。角声寒，夜阑珊，怕人寻问，咽泪装欢。瞒，瞒，瞒！ 对于文本中身份证号的匹配就可以使用下面这段正则表达式， 1(\d&#123;18&#125;)|([0-9]&#123;17&#125;(X|x$)) 看一下匹配结果， 在解释上述正则表达式含义之前，我们首先想一下身份证号有哪些特点， 身份证号长度为15位或者18位 末位为数字或者大写X或者小写x 以上述文本为例，身份证号长度为18、末位为X，下面来看一下上述正则表达式， 正则表达式中竖线 | 表示“或”的意思，可以在多个片段中进行选择，所以上述正则表达式可以分为2部分， (\d{18}) ([0-9]{17}(X|x$)) 其中(\d{18})中\d匹配任意数字，后面大括号是对前面子表达式的限定符，括号里为整数，表示的含义是匹配前面子表达式的次数，因此这个表达式的含义就是匹配\d18次，换句话说就是匹配数字18次，而有些身份证号就是18位的纯数字，因此这个表达式可以匹配18位纯数字的身份证号。 然后再看([0-9]{17}(X|x$))，这里面[0-9]和\d的含义是相同的，匹配0-9之间的数字，后面的17的含义是匹配前面数字17次，然后X|x$的含义是以X或x结尾，$是匹配结尾的含义，因此这个表达式的含义就是匹配另外一种身份证号，前面为数字末位为X或x的身份证号。 到这里，就应该了解了上述正则表达式的含义，匹配两种身份号，一种为纯数字，一种末位为X或x。 当然，正则表达式的价值远不止于匹配身份证号这么简单，它可以用于一些自然语言处理的数据粗提取，可以用于网页爬虫去匹配一些网页内容、网页链接，同时还可以用于linux命令行的字符、文件匹配。 读到这里应该就明白，正则表达式就是一些具有特殊含义字符以不同方式组合在一起形成的文本匹配模式，因此，学习正则表达式的关键点也就很明确了，需要了解不同字符的含义，下面就来详细介绍一下。 字符分类我把正则表达式的字符分为如下三类分别进行讲解， 元字符 限定字符 特殊字符 元字符开头和结尾 字符 描述 ^ 匹配字符字符串的开始位置 $ 匹配字符字符串的结尾位置 假如待匹配的文本如下， 123helloellohllohe 正则表达式及匹配结果为， 12345^h.+ # 正则表达式hello # 匹配结果.+e$ # 匹配结果llohe # 匹配结果 匹配次数相关字符 字符 描述 * 匹配前面子表达式0次或多次 + 匹配前面子表达式1次或多次 ？ 匹配前面子表达式0次或1次 假设待匹配文本为， 1helloooo 正则表达式及匹配结果为， 12345678910lo+ # 正则表达式loooo # 匹配结果lo* # 正则表达式l # 匹配结果loooo lo? # 正则表达式l # 匹配结果lo 限定字符前面讲到的可以用一些元字符来匹配“次数”相关的字符，但是这些都是模糊的匹配，零次或多次、零次或1次、1次或多次，不够精确。除了这些元字符外还可以通过限定符来匹配准确数量的字符，限定符是以大括号进行标识， 字符 描述 {n} 匹配前面子表达式n次 {n,} 匹配前面子表达式至少n次 {n,m} 匹配前面子表达式至少n次，最多m次 假设待匹配为， 11919190380384083 正则表达式及匹配结果为， 12345[0-9]&#123;4&#125; # 正则表达式1919 # 匹配结果190380384083 特殊字符有一些特殊字符在不同的场景下含义不尽相同，例如？紧跟一个字表达式，含义是匹配0次或者1次，如果跟随其他次数限定相关的字符，它就变成了非贪婪的，也就是说它就会使得匹配次数限定为“较少”的一方，例如的含义是匹配0次或者多次，那么\？就是匹配0次，而不能匹配多次，这就是非贪婪的含义。此外，^和正常字符在一起使用时表示匹配字符的开头，但是在中括号内使用时就成为反向范围的含义，例如[abc]匹配包括a或者b或者c，而[\^abc]的含义是匹配除a\b\c以外的任意字符，下面对特殊字符进行详细说明一下， 字符 含义 a\ b 匹配a或b [abc] 字符集合，匹配a或b或c abc 反向范围，匹配a、b、c以外任意字符 [a-z] 匹配a-z26个字母 a-z 匹配不在a-z之间的任意字符 \b 匹配单词边界，和^、$类似 \B \b的反义，匹配非单词边界 \d 匹配数字 \D 匹配非数字 \s 匹配任何空白字符，包括空格、制表符、换页符 \S 匹配任何非空白字符 \w 匹配字母、数字、下划线 \W 匹配非字母、数字、下划线 \f 匹配换页符 \n 匹配换行符 \r 匹配回车符 \t 匹配制表符 上述就是我们常用的正则表达式符号，可以根据我们的需求，对上述这些字符进行任意的组合，来完成我们的匹配、查询、替换等工作。]]></content>
      <categories>
        <category>IT技术</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>正则表达式</tag>
        <tag>开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发工具 | 你真的会用jupyter吗？]]></title>
    <url>%2F2019%2F09%2F14%2Fjupyter%2F</url>
    <content type="text"><![CDATA[前言 提起jupyter notebook，应该很多学习过Python的同学都不模型。虽然用jupyter notebook的同学相对较少，但是提及这款开发工具，很多人都会赞不绝口，“jupyter很强大，交互式、富文本”，很多人都知道jupyter notebook的这几个优点，但是，试问一下，你真的会用jupyter吗？ 以Python开发为例，我们只需要在windows命令行或者linux/Mac终端输入“jupyter notebook”或者“ipython notebook”即可使用默认浏览器打开一个在线IDE， 首先说一下交互式， jupyter notebook中一个非常重要的概念就是cell，每个cell能够单独进行运算，这样适合于代码调试。我们开发一个完整的脚本时变量会随着代码执行的结束而从内存中释放，如果我们想看中间的变量或者结构，我们只能通过断点或者输出日志信息的方式进行调试，这样无疑是非常繁琐的，如果一个程序运行很多这种方式还可行，如果运行时间长达几个小时，这样我们调试一圈耗费的时间就太长了。 而在jupyter notebook中我们可以把代码分隔到不同的cell里逐个进行调试，这样它会持续化变量的值，我们可以交互式的在不同cell里获取到我们想要测试的变量值和类型。 然后说一下富文本， 开发代码不仅是给机器去“阅读”，也需要让其他的同事、同学能够很容易的阅读，因此，注释就在开发过程中变的非常重要，一个完善的注释能够让周围人更加容易理解，协作效率也更高，避免重复性劳动。在大多数IDE中都可以进行注释，但是几乎都是相同的，只支持一些简单的文本格式注释，这显然是不够的，jupyter notebook支持Markdown编辑，它的cell不仅可以用于编码，还可以用于书写文本，Markdown可以轻松完成标题、数学公式等格式的编辑，更加有助于解释代码，适用于教学等场景。 最后在说一下轻量、触手可及， 开发过程中我经常需要测试一个小的代码块或者函数，这时候有两个选择：在IDE中新建一个测试脚本；打开命令行下的Python。我觉的这两个都不是好的选择，如果在项目下新建一个脚本后续还需要记住把它清理掉，如果写一个完善的测试脚本用于Alpha、beta测试这显然是低效不现实的。而选择在命令行下，界面不友好，操作不灵活，体验更差。 这时候就显现出jupyter notebook的优势，只需要输入jupyter notebook就会在流量器中打开一个网页，能轻量、快捷的进行开发验证，效率很好。此外，我们还可以通过搭建jupyter notebook服务使得它一直在服务器下运行来避免每次需要时都要在命令行下重复打开，我们只需要在浏览器打开对应的网页即可，这一点下文会详细介绍。 其实，除了这些我们耳熟能详的优点之外，jupyter还有很多令人惊叹的亮点： 丰富的插件 主题修改 多语言支持 下面就针对这3点分别介绍一下，介绍下面3个功能的前提条件是已经通过下方命令成功安装jupyter notebook， 1$ pip install jupyter notebook 丰富的插件安装插件管理器 如果没有安装插件管理器，打开jupyter notebook后菜单栏只有如下3项， Files Running Clusters 我们需要安装插件管理器来管理我们需要的插件， 第一步：用pip安装插件管理包， 12$ pip install jupyter_contrib_nbextensions$ pip install jupyter_nbextensions_configurator 第二步：安装一些插件并启用插件管理器， 12$ jupyter nbextensions_configurator install --user$ jupyter nbextensions_configurator enable --user 然后再次打开jupyter notebook会发现菜单栏多了一个选项Nbextensions, 记得勾选disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development)，否则下方插件是不可选状态。 我们可以通过命令来管理开启或关闭某个插件，但是我觉得还是通过直接勾选我们需要的插件效率更高。 选择插件 我们从上面可以看出，jupyter notebook有很多插件，我们该用哪一个呢？我推荐5款个人认为不错的插件。 Table of Contents Execute Time Nofity Codefolding Hinterland 下面分别介绍一下它们的功能， Table of Contents是一款自动生成目录的工具，它能够通过我们我们富文本中定义的标题自动生成目录，这样我们能够通过点击左侧目录快速定位到我们想要的到达的代码片段。 Execute Time顾名思义，执行时间，我觉得这是一款非常实用的插件，在企业项目开发中，效率是永远无法越过的一个门槛，和学术上理论效果足够优秀即可不同，在企业项目中对效率要求也很高，因此，我们需要统计代码的运行时间，其中最初级的用法就是在每个函数开始和结尾处写一个计时语句，这样比较繁琐。然后再高阶一些的用法就是通过装饰器写一些计时器，在每个函数上调用这个装饰器。其实，如果用jupyter notebook完全没必要这么麻烦。我们只需要打开Execute Time，它就能统计每个cell的运行耗费时间，结束时间等，非常详细，一目了然。 Nofity同样是一款非常实用的插件，当我们运行一个耗时较长的代码时，我们不可能一直盯着屏幕等待，但是我们又希望及时知道它运行结束了，Notify这款插件就可以实现这个功能，它能够在代码运行结束时发出通知，及时告知你代码运行结束了。 Codefolding是一款代码折叠工具，有时候我们写的一个函数非常长，但是我们又不关注 ，这样在阅读过程中会使得效率很低，代码折叠就是一个不错的选择，折叠我们不关注的代码块，Codefolding能够像其他IDE那样让你轻松自如的折叠代码块。 Hinterland是一款自动补全插件，称一个IDE“优秀”，如果没有自动补全显然是说不过去的。jupyter notebook自带补全功能，但是每次都需要点击tab键来补全，这样效率比较低，我们可以通过勾选Hinterland让jupyter notebook具备自动补全功能，当我们输入几个包含字母后它能够快速补全我们想要的函数，补全速度堪比pycharm。 主题修改很多同学使用jupyter notebook都会觉得，这款开发工具界面太单调了，只有纯白色的主题，其实并不是这样，jupyter notebook也支持主题修改，而且非常方便。 首先在命令行下输入下面命令安装主题， 1$ pip install jupyterthemes jupyter notebook的主题管理工具叫做jt，我们可以通过下面命令查看可用主题， 1234567891011$ jt -lAvailable Themes: chesterish grade3 gruvboxd gruvboxl monokai oceans16 onedork solarizedd solarizedl 然后通过下面命令设置主题， 1$ jt -t &lt;theme_name&gt; 其中theme_name为主题名称。 如果觉得不满意，想退回默认主题，可以通过下方命令实现， 1$ jt -r 多语言支持很多同学是因为Python而解除到jupyter notebook的，因此会认为这就是一款Python专属的开发工具，如果这样的话，那么也不足以我专门用一篇文章来介绍这款开发工具。 它更像是eclipse、IDEA、vscode，是一款综合的开发工具，它不仅支持Python，还支持C++、julia、R、ruby、Go、Scala、C#、Perl、PHP、Octave、Matlab、Lua、Tcl、等多种编程语言，功能十分强大，支持语言详情，请查看下方链接， https://github.com/jupyter/jupyter/wiki/Jupyter-kernels 不同语言的配置方式各不相同，这里不再一一介绍，可以根据自己需要的语言自行在网上搜索相关配置资料进行配置。 jupyter notebook服务如果非要找出使用jupyter notebook的缺点，我认为就是每次启动的时候相对繁琐，我们启动本地安装的IDE，一个命令或者点击一下图标即可，但是如果启动jupyter notebook就需要进入命令行或终端，输入“jupyter notebook”进行打开，如果使用的是虚拟环境，首先还要激活虚拟环境，这无疑是非常繁琐的，而且启动后它会占用一个终端或命令行窗口，如果意外关闭则会终止jupyter notebook服务。其实，这也是有解决方法的，我们搭建一个持续化的jupyter notebook服务，让它一直在服务器后台运行，这样既不占用窗口，也不需要繁琐的打开步骤，我们只需要把对应的URL收藏，每次需要时能够秒级速度打开，下面就来介绍一下jupyter notebook的搭建步骤。 第一步：获取sha1密码 在命令行下输入ipython， 12345In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password:Verify password:Out[2]: 'sha1:746cf729d33f:0af9cda409de9791f237a6c46c3c76a3237962fc' 导入passwd函数，调用后会让你输入密码，你可以设置一个明文密码，例如123，然后它会生成一个sha1密码串，这个很重要，后面会用到。 修改jupyter配置文件，linux系统配置文件路径为~/.jupyter/jupyter_notebook_config.py，windows系统配置文件路径为C:\\Users\\User\.jupyter\\jupyter_notebook_config.py，如果没有这个文件，可以使用下面命令生成， 1$ jupyter notebook --generate-config 这个配置文件很长，以linux为例，主要关注的是如下几项， 12345c.NotebookApp.ip = '*' c.NotebookApp.password = u'sha1:xxx:xxx' c.NotebookApp.open_browser = False c.NotebookApp.port = 8888c.NotebookApp.enable_mathjax = True c.NotebookApp.ip、c.NotebookApp.port，ip要和服务器保持一致，端口可以自行设定，不和其他端口冲突即可，后续访问时在浏览器输入ip:port即可。 c.NotebookApp.password就是前面生成的sha1密码串，复制过来即可。 c.NotebookApp.open_browser = False 的含义为是每次启动命令时是否打开浏览器，由于我们用的时候直接输入URL即可，所以这里不需要打开浏览器。 c.NotebookApp.enable_mathjax的含义为是否用mathjax，它是一种用于数学公式显示的工具，这里选True。 配置好这几项之后保存退出，输入下面命令即可启动， 1$ nohup jupyter notebook &gt; /dev/null 2&gt;&amp;1 &amp; nohup的含义是后台运行，这样就不用占用一个窗口来了。 配置好之后只要服务器不关机，jupyter notebook的服务会一直处于运行状态，我们随时可以使用，只需要打开ip:port即可。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十五讲：卷积神经网络之LeNet]]></title>
    <url>%2F2019%2F09%2F13%2Flenet%2F</url>
    <content type="text"><![CDATA[前言 提起卷积神经网络，也许可以避开VGG、GoogleNet，甚至可以忽略AleNet，但是很难不提及LeNet。 LeNet是由2019年图灵奖获得者、深度学习三位顶级大牛之二的Yann LeCun、Yoshua Bengio于1998年提出(Gradient-based learning applied to document recognition)，它也被认为被认为是最早的卷积神经网络模型。但是，由于算力和数据集的限制，卷积神经网络提出之后一直都被传统目标识别算法(特征提取+分类器)所压制。终于在沉寂了14年之后的2012年，AlexNet在ImageNet挑战赛上一骑绝尘，使得卷积神经网络又一次成为了研究的热点。 近几年入门计算机视觉的同学大多数都是从AlexNet甚至更新的网络模型入手，了解比较多的就是R-CNN系列和YOLO系列，在很多知名的课程中对LeNet的介绍也是非常浅显或者没有介绍。虽然近几年卷积神经网络模型在LeNet的基础上加入了很多新的单元，在效果方面要明显优于LeNet，但是作为卷积神经网络的基础和源头，它的很多思想对后来的卷积神经网络模型具有很深的影响，因此，我认为了解一下LeNet还是非常有必要的。 本文首先介绍一下LeNet的网络模型，然后使用tensorflow来一步一步实现LeNet。 LeNet 上图就是LeNet的网络结构，LeNet又被称为LeNet-5，其之所以称为这个名称是由于原始的LeNet是一个5层的卷积神经网络，它主要包括两部分： 卷积层 全连接层 其中卷积层数为2，全连接层数为3。 这里需要注意一下，之前在介绍卷积、池化时特意提到，在网络层计数中池化和卷积往往是被算作一层的，虽然池化层也被称为”层”，但是它不是一个独立的运算，往往都是紧跟着卷积层使用，因此它不单独计数。在LeNet中也是这样，卷积层块其实是包括两个单元：卷积层与池化层。 在网络模型的搭建过程中，我们关注的除了网络层的结构，还需要关注一些超参数的设定，例如，卷积层中使用卷积核的大小、池化层的步幅等，下面就来介绍一下LeNet详细的网络结构和参数。 第一层：卷积层 卷积核大小为5*5，输入通道数根据图像而定，例如灰度图像为单通道，那么通道数为1，彩色图像为三通道，那么通道数为3。虽然输入通道数是一个变量，但是输出通道数是固定的为6。 池化层中窗口大小为2*2，步幅为2。 第二层：卷积层 卷积核大小为5*5，输入通道数即为上一层的输出通道数6，输出通道数为16。 池化层和第一层相同，窗口大小为2*2，步幅为2。 第三层：全连接层 全连接层顾名思义，就是把卷积层的输出进行展开，变为一个二维的矩阵(第一维是批量样本数，第二位是前一层输出的特征展开后的向量)，输入大小为上一层的输出16，输出大小为120。 第四层：全连接层 输入大小为120，输出大小为84。 第五层：全连接层 输入大小为84，输出大小为类别个数，这个根据不同任务而定，假如是二分类问题，那么输出就是2，对于手写字识别是一个10分类问题，那么输出就是10。 激活函数 前面文章中详细的介绍了激活函数的作用和使用方法，本文就不再赘述。激活函数有很多，例如Sigmoid、relu、双曲正切等，在LeNet中选取的激活函数为Sigmoid。 模型构建 如果已经了解一个卷积神经网络模型的结构，知道它有哪些层、每一层长什么样，那样借助目前成熟的机器学习平台是非常容易的，例如tensorflow、pytorch、mxnet、caffe这些都是高度集成的深度学习框架，虽然在强化学习、图神经网络中表现一般，但是在卷积神经网络方面还是很不错的。 我绘制了模型构建的过程，详细的可以看一下上图，很多刚入门的同学会把tensorflow使用、网络搭建看成已经非常困难的事情，其实理清楚之后发现并没有那么复杂，它主要包括如下几个部分： 数据输入 网络模型 训练预测 其中，重点之处就在于网络模型的搭建，需要逐层的去搭建一个卷积神经网络，复杂程度因不同的模型而异。训练测试过程相对简单一些，可以通过交叉熵、均方差等构建损失函数，然后使用深度学习框架自带的优化函数进行优化即可，代码量非常少。 LeNet、AlexNet、VGG、ResNet等，各种卷积神经网络模型主要的区别之处就在于网络模型，但是网络搭建的过程是相同的，均是通过上述流程进行搭建，因此，本文单独用一块内容介绍模型搭建的过程，后续内容不再介绍网络模型的搭建，会直接使用tensorflow进行编程实践。 编程实践完整代码请查看github项目： aiLearnNotes 首先需要说明一下，后续的内容中涉及网络模型搭建的均会选择tensorflow进行编写。虽然近几年pytorch的势头非常迅猛，关于tensorflow的批评之声不绝于耳，但是我一向认为，灵活性和易用性总是成反比的，tensorflow虽然相对复杂，但是它的灵活性非常强，而且支持强大的可视化tensorboard，虽然pytorch也可以借助tensorboard实现可视化，但是这样让我觉得有一些”不伦不类”的感觉，我更加倾向于一体化的框架。此外，有很多同学认为Gluon、keras非常好用，的确，这些在tensorflow、mxnet之上进一步封装的高级深度学习框架非常易用，很多参数甚至不需要开发者去定义，但是正是因为这样，它们已经强行的预先定义在框架里了，可想而知，它的灵活性是非常差的。因此，综合灵活性、一体化、丰富性等方面的考虑，本系列会采用tensorflow进行编程实践。 其次，需要说明的是本系列重点关注的是网络模型，因此，关于数据方面会采用MNIST进行实践。MNIST是一个成熟的手写字数据集，它提供了易用的接口，方便读取和处理。 在使用tensorflow接口读取MNIST时，如果本地有数据，它会从本地加载，否则它会从官网下载数据，如果由于代理或者网速限制的原因自动下载数据失败，可以手动从官网下载数据放在MNIST目录下，数据包括4个文件，分别是： train-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz 它们分别是训练数据集和标签，测试数据集和标签。 可能会有人有疑问，手写体识别不是图像吗？为什么是gz的压缩包？因为作者对手写体进行了序列化处理，方便读取，数据原本是衣服单通道28*28的灰度图像，处理后是784的向量，我们可以通过一段代码对它可视化一下， 12345678910from matplotlib import pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST", one_hot=True)for i in range(12): plt.subplot(3, 4, i+1) img = mnist.train.images[i + 1] img = img.reshape(28, 28) plt.imshow(img)plt.show() 通过读取训练集中的12副图像，然后把它修改成28*28的图像，显示之后会发现和我们常见的图像一样， 下面开始一步一步进行搭建网络LeNet，由前面介绍的模型构建过程可以知道，其中最为核心的就是搭建模型的网络架构，所以，首先先搭建网络模型， y=wx+b卷积的运算是符合上述公式的，因此，首先构造第一层网络，输入为批量784维的向量，需要首先把它转化为28*28的图像，然后初始化卷积核，进行卷积、激活、池化运算， 123456X = tf.reshape(X, [-1, 28, 28, 1])w_1 = tf.get_variable("weights", shape=[5, 5, 1, 6])b_1 = tf.get_variable("bias", shape=[6])conv_1 = tf.nn.conv2d(X, w_1, strides=[1, 1, 1, 1], padding="SAME")act_1 = tf.sigmoid(tf.nn.bias_add(conv_1, b_1))max_pool_1 = tf.nn.max_pool(act_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") 然后构建第二层网络， 12345w_2 = tf.get_variable("weights", shape=[5, 5, 6, 16])b_2 = tf.get_variable("bias", shape=[16])conv_2 = tf.nn.conv2d(max_pool_1, w_2, strides=[1, 1, 1, 1], padding="SAME")act_2 = tf.nn.sigmoid(tf.nn.bias_add(conv_2, b_2))max_pool_2 = tf.nn.max_pool(act_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") 到这里，卷积层就搭建完了，下面就开始搭建全连接层。 首先需要把卷积层的输出进行展开成向量， 1flatten = tf.reshape(max_pool_2, shape=[-1, 2 * 2 * 16]) 然后紧接着是3个全连接层， 12345678910111213141516171819# 全连接层1with tf.variable_scope("fc_1") as scope: w_fc_1 = tf.get_variable("weight", shape=[2 * 2 * 16, 120]) b_fc_1 = tf.get_variable("bias", shape=[120], trainable=True)fc_1 = tf.nn.xw_plus_b(flatten, w_fc_1, b_fc_1)act_fc_1 = tf.nn.sigmoid(fc_1)# 全连接层2with tf.variable_scope("fc_2") as scope: w_fc_2 = tf.get_variable("weight", shape=[120, 84]) b_fc_2 = tf.get_variable("bias", shape=[84], trainable=True)fc_2 = tf.nn.xw_plus_b(act_fc_1, w_fc_2, b_fc_2)act_fc_2 = tf.nn.sigmoid(fc_2)# 全连接层3with tf.variable_scope("fc_3") as scope: w_fc_3 = tf.get_variable("weight", shape=[84, 10]) b_fc_3 = tf.get_variable("bias", shape=[10], trainable=True)fc_3 = tf.nn.xw_plus_b(act_fc_2, w_fc_3, b_fc_3) 这样就把整个网络模型搭完成了，输入是批量图像X，输出是预测的图像，输出是一个10维向量，每一维的含义是当前数字的概率，选择概率最大的位置，就是图像对应的数字。 完成了网络模型的搭建，它能够将输入图像转化成预测标签进行输出，接下来要做的就是训练和测试部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def train(): # 1. 输入数据的占位符 x = tf.placeholder(tf.float32, [None, 784]) y = tf.placeholder(tf.float32, [BATCH_SIZE, 10]) # 2. 初始化LeNet模型，构造输出标签y_ le = LeNet() y_ = le.create(x) # 3. 损失函数，使用交叉熵作为损失函数 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y)) # 4. 优化函数，首先声明I个优化函数，然后调用minimize去最小化损失函数 optimizer = tf.train.AdamOptimizer() train_op = optimizer.minimize(loss) # 5. summary用于数据保存，用于tensorboard可视化 tf.summary.scalar("loss", loss) merged = tf.summary.merge_all() writer = tf.summary.FileWriter("logs") # 6. 构造验证函数，如果对应位置相同则返回true，否则返回false correct_pred = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)) # 7. 通过tf.cast把true、false布尔型的值转化为数值型，分别转化为1和0，然后相加就是判断正确的数量 accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # 8. 初始化一个saver，用于后面保存训练好的模型 saver = tf.train.Saver() with tf.Session() as sess: # 9. 初始化变量 sess.run((tf.global_variables_initializer())) writer.add_graph(sess.graph) i = 0 for epoch in range(5): for step in range(1000): # 10. feed_dict把数据传递给前面定义的占位符x、y batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE) summary, loss_value, _ = sess.run(([merged, loss, train_op]), feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) print("epoch : &#123;&#125;----loss : &#123;&#125;".format(epoch, loss_value)) # 11. 记录数据点 writer.add_summary(summary, i) i += 1 # 验证准确率 test_acc = 0 test_count = 0 for _ in range(10): batch_xs, batch_ys = mnist.test.next_batch(BATCH_SIZE) acc = sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) test_acc += acc test_count += 1 print("accuracy : &#123;&#125;".format(test_acc / test_count)) saver.save(sess, os.path.join("temp", "mode.ckpt")) 上述就是训练部分的完整代码，在代码中已经详细的注释了每个部分的功能，分别包含数据记录、损失函数、优化函数、验证函数、训练过程等，然后运行代码可以看到效果， 12345678...epoch : 4----loss : 0.07602085173130035epoch : 4----loss : 0.05565792694687843epoch : 4----loss : 0.08458487689495087epoch : 4----loss : 0.012194767594337463epoch : 4----loss : 0.026294417679309845epoch : 4----loss : 0.04952147603034973accuracy : 0.9953125 准确率为99.5%，可以看得出，在效果方面，LeNet在某些任务方面并不比深度卷积神经网络差。 打开tensorboard可以直观的看到网络的结构、训练的过程以及训练中数据的变换， 1$ tensorboard --logdir=logs 通过损失函数的变化过程可以看出，训练过程在2000步左右基本达到了最优解， 更多精彩内容请关注公众号【平凡而诗意】，或者加入我的知识星球【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 神器AutoHotkey的使用教程]]></title>
    <url>%2F2019%2F09%2F06%2Fautohotkey%2F</url>
    <content type="text"><![CDATA[简单示例AutoHotkey是一款强大、开源的热键脚本工具。在以往的文章里，我介绍了很多强大的效率提升工具，其中包括Wox、Listary、QuickLook等。如果说这些软件在某些领域独树一帜，那么AutoHotkey则是在则是“无所不能”的强大工具。 易用性和功能的灵活性往往是成反比的，举一个极端的例子，开发语言这类工具在使用方面非常不友好，但是经过开发语言的各种组合可以实现各种手机、PC端软件数不胜数的功能。当然，由于它的使用偏于专业，所以更多人选择定制化较强、界面友好的工具，但是却要牺牲一些灵活性。 Autohotkey是一款介于纯编程语言和分发软件之间的一款工具，你可以使用它写一些简单的脚本语言，用内置的一些函数或者自定义的函数去单独或组合使用，以达到我们期望的功能，下面先来看一个示例。 先看一个简单的AutoHotkey脚本， 12345678^j::Send, This is a Hotkey!return::hs::This is a HotString!^#s::Run, D:\Sublime Text 3\sublime_text.exe 然后鼠标右键点击编译，或者双击脚本运行，然后点击对应的快捷键， 这个演示包括三个动作： 快捷键启动sublime 快捷键输入“This is a Hotkey!” 快捷字符输入“This is a HotString!” 回到前面给出的AutoHotkey脚本，来一步一步的解释怎么实现的。 第一个动作：快捷键启动sublime 12^#s::Run, D:\Sublime Text 3\sublime_text.exe 这句命令实现的是快捷键启动sublime。 第一行中^和#是两个代表按键的符号，分别代表ctrl和windows，s就是键盘上的s键，::可以认为是命令的结束符号。 第二行中Run是一个内置函数，用于运行一个工具或者打开一个网页，后面跟的是要打开的网页或者软件路径(如果已经加入到环境变量，就不需要完整的安装路径)。 因此，上述两行脚本的功能就是ctrl+win+s就可以打开sublime这款软件。 第二个动作：快捷键输入“This is a Hotkey!” 123^j::Send, This is a Hotkey!return 看完第一个动作的介绍，应该很容易理解这个动作， 第一行的含义是ctrl+j快捷键。 第二行的含义是发送一段字符串。 第三个动作：快捷字符输入“This is a HotString!” 1::hs::This is a HotString! 这个和前两个动作不同，介绍这个动作之前需要先简单的了解一下AutoHotkey，它主要包括两个概念： hotkey hotstring 其中hotkey并不陌生，就是热键、快捷键，前两个动作实现的就是快捷键。 这里需要说一下hotstring，顾名思义，就是通过一段字符串实现一个快捷功能，第三个动作实现的就是一个hotstring功能。 现在回过头来解释一下第三个动作的脚本的含义， 两个::之间定义的是快键字符串，后面跟随的是要输入的完整字符串，这样的话在文本框输入hs然后点击Tab键即可在编辑器中输入This is a HotString!这个完整的字符串。 很多人用惯了快捷键会疑问，hostring有什么价值？我认为它对于开发者或者文本编辑相关的工作者是非常有意义的。我们可以在脚本中预先用一些hostring定义好我们常用的代码块或者文字内容，这样，当输入对应的hostring时就可以快速补全我们想要的内容，速度和资源消耗要远远小于常用的代码补全工具。 热键符号通过前面的简单示例，想必应该对AutoHotkey有了简单的了解，它就是通过一些内置的符号、函数、自定义函数来任意组合，定制化的实现我们想要的功能。 通过示例中第一个动作^#s代表快捷键ctrl+win+s可以看出，热键符号具有至关重要的作用，我们怎么就知道^代表ctrl、#代表win呢？因为官网给出了不同符号的对应关系，下面是各个符号对应的热键， 符号 描述 # win ！ Alt ^ Ctrl + Shift &amp; 可以在两个组合键之间使用 &lt; 指定快捷键的位置在左边 &gt; 指定快捷键的位置在右边 注：我们都知道，alt、ctrl、shift这些键都有两个，左右两边均有，&lt;和&gt;两个符号就指定使用左边的符号还是右边的符号，例如&lt;!指定使用左边的Alt键。 标签在示例中，我们也看到使用了Run、Send，这两个称为AutoHotkey的标签，虽然我是以大写字母开头，但是标签名称其实是部分不区分大小写的，可以由空格、制表符、逗号、转义符以外的任何字符组成，但是由于样式的约定，通常最好使用字母、数字、下划线，AutoHotkey常用的内置标签有如下几个， 标签 描述 Send 向编辑器发送一段字符 SendInput 同上 MsgBox 弹出对话框 Run 运行一个工具或打开网页 WinActivate 窗口激活 WinWaitActive 窗口等待激活 下面看一个例子， 1234^j::Run, https://www.baidu.comMsgBox, 已经打开网页！return 有了前面的基础，应该很容易理解这个脚本，它是一个组合功能，分别是打开网页和弹出对话框，下面看看效果， 函数想要更加灵活，仅仅使用上述这些符号、标签显然是不够的。和大多数编程语言一样，AutoHotkey也支持自定义函数，这才是它的强大之处。 AutoHotkey内置了一些常用的函数，如下， 函数 描述 FileExist 检查文件或文件夹是否存在，并返回其属性 GetKeyState 获取按键状态，向下返回true，向上返回false InStr 从左或右搜索字符串的给定出现项 RegExMatch 确定字符串是否包含正则表达式匹配模式 RegExReplace 替换字符串中出现的模式(正则表达式) StrLen 获取字符串长度 StrReplace 替换字符串 StrSplit 用指定的分隔符分割字符串 SubStr 按指定位置返回子字符串 当然，AutoHotkey内置的函数远不止这些，它还包括以下类型的函数： 文件读取 数学计算 条件判断 异常处理 状态获取 鼠标键盘 屏幕状态 声音 进程管理 窗口状态 …… 没错，AutoHotkey在功能和丰富性方面丝毫不亚于一些老的脚本语言，但是它的优点是更加实用。我们可以即写即用，能够辅助我们日常生活中多种场景的工作。 使用场景快捷启动 这方面它可以替代Wox、Listary这些快速启动工具，我们可以把常用的网站、软件用脚本的方式定义不同的hotkey或者hotstring，当我们需要打开一个网页或者软件时就不需要再繁琐、多步骤的去寻找、打开。 例如用下面这个脚本，能够快速打开优酷、B站、直播吧， 1234567891011^y::Run, https://www.youku.com/return^b::Run, https://www.bilibili.com/return^z::Run, https://www.zhibo8.cc/return 自动补全 我们在日常开发或者文本编辑时，会有大量重复的工作，以编程为例，我们会有很多重复的代码块，因此，才有各种各样的补全工具，但是目前大多数补全工具可以说是差强人意，速度方面甚至不及自己手动敲代码的速度，但是通过AutoHotkey把我们常用的代码块、文本用hotkey、hotstring代替，这样能够快速的补全我们想要的内容， 例如，用下面几行脚本补全我们常用的代码片段， 123::np::import numpy as np::plt::from matplotlib import pyplt as plt::tf::import tensorflow as tf 效率提升 关于效率提升，这就因人而异，不同的人工作内容不同，因此常用的操作和功能也截然不同，这方面就需要发散思维，总结一下平时自己常用的操作，例如管理进程、取色、文件读取、编程、数学运算等，可以根据自己的需求，使用内置的或者自定义的函数来组合成自己想要的功能，当然，AutoHotkey内置了很多标签、函数，这足以满足日常大多数场景的需求。 软件分发 我们经常在能够在开源平台寻找到很多别人写的软件，其实自己也可以利用AutoHotkey实现一些比较有趣、高效的工具，它不像C++、Java那么难以入门，而且在代码规范方面要求没那么严格，因此门槛相对较低。此外，AutoHotkey脚本的编译非常迅速，资源消耗低，能够一键编译成我们常见的exe软件，这样的话可以把它分享给周围的同学、同事，或者更多的人，这样在提高周围人效率的同时能够锻炼自己的产品思维和开发规范。 安装与基本操作下载安装包 AutoHotkey是一款开源免费的工具，能够直接从官网下载AutoHotkey的安装包， https://www.autohotkey.com/ 如果觉得麻烦，也可以在公众号后台回复关键字hot获取安装包。 安装 双击下载的安装包，会弹出安装界面，直接一步步往下点击即可，如果需要更改安装路径，可以在location页面进行更改。 新建脚本 成功安装后，在桌面或者其他空白处点击鼠标右键-新建-AutoHotkey Script，来创建一个AutoHotkey脚本。 编辑脚本 创建脚本后，鼠标右键点击脚本，可以看到有三个选项：Run Script、Compile Script、Edit Script。 可以先点击Edit Script，它会用记事本打开，当然也可以用sublime、UE、notepad++等文本编辑器打开，然后进行编辑。 编辑之后可以双击脚本直接运行，也可以点击Run Script运行脚本，这样就可以使用我们脚本中定义的快捷键或者快捷字符串，此外，还可以点击Compile Script把脚本编译成exe文件，这样的话脚本会被加密，可以用于分发，其他使用者就无法看到工具源码。 更多精彩内容请关注公众号【平凡而诗意】，或者加入我的知识星球【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>文件查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十四讲：正则化之Dropout]]></title>
    <url>%2F2019%2F09%2F01%2Fcnn-dropout%2F</url>
    <content type="text"><![CDATA[本文完整代码请查看：aiLearnNotes 前言在前几讲里已经介绍了卷积神经网络中常用的一些单元，例如， 卷积层 池化层 填充 激活函数 批量归一化 本文会介绍最后一个卷积神经网络中常用的单元Dropout，可以称之为“丢弃法”，或者“随机失活”。它在2012年由Alex Krizhevsky、Geoffrey Hinton提出的那个大名鼎鼎的卷积神经网络模型AlexNet中首次提出并使用，Dropout的使用也是AlexNet与20世纪90年代提出的LeNet的最大不同之处。随后，Krizhevsky和Hinton在文章《Dropout: A Simple Way to Prevent Neural Networks from Over tting》又详细的介绍了介绍了Dropout的原理。发展至今，Dropout已经成为深度学习领域一个举足轻重的技术，它的价值主要体现在解决模型的过拟合问题，虽然它不是唯一的解决过拟合的手段，但它却是兼备轻量化和高效两点做的最好的一个手段。 “丢弃法”，从字面意思很好理解，就是丢弃网络中的一些东西。丢弃的是什么？神经元，有放回的随机丢弃一些神经元。 很多刚接触或者使用过Dropout的同学都会觉得“这有什么好讲的？这是一个非常简单的东西啊。”，如果仅仅从使用角度来讲，这的确非常简单。以目前主流的机器学习平台而言，tensorflow、mxnet、pytorch，均是传入一个概率值即可，一行代码即可完成。但是，我认为学习深度学习如果仅仅是为了会使用，那么真的没什么可以学习的，抽空把tensorflow教程看一下既可以称得上入门深度学习。如果剖开表象看一下Dropout的原理，会发现，它的理论基础是非常深的，从作者先后用《Improving neural networks by preventing co-adaptation of feature detectors》《Dropout: A Simple Way to Prevent Neural Networks from Over tting》等多篇文章来阐述这个算法就可以看出它的不可小觑的价值。 和往常先讲理论再讲用法不同，本文先介绍一下它在tensorflow中的用法，然后做一个了解后带着问题去介绍它的理论知识，本文主要包括如下几块内容， - tensorflow中Dropout的使用 - 优化与机器学习的区别 - 过拟合 - Dropout理论知识 # tensorflow中Dropout的使用 在tensorflow中Dropout的函数为， 1tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) 函数中的参数分别是： x：Dropout的输入，即为上一层网络的输出 keep_prob：和x尺寸相同的张量(多维向量)，它用于定义每个神经元的被保留的概率，假如keep_prob=0.8，那么它被保留的概率为0.8，换个角度说，它有20%的概率被丢弃。 noise_shape：表示随机生成的保存/删除标志的形状，默认值为None。 seed：一个用于创建随机种子的整数，默认值为None。 name：运算或操作的名称，可自行定义，默认值为None。 上述5个参数中x和keep_prob为必须参数，其他很少用到，所以不多介绍。x不难理解，就是上一层网络的输出。这里主要提示一下keep_prob，它是Dropout使用中最为重要的参数。 注意：keep_prob是网络中每个神经元被保留的概率，并非是神经网络中神经元被保留个数的概率。举个例子，加入有个3层神经网络，共100个神经元，定义keep_prob=0.6，那么并不是说要保留60个神经元而丢弃40个。而是每个神经元将会有60%的概率被保留，40%的概率被丢弃，所以最终剩余的神经元并不是确切的60个，可能多于60也可能少于60。 策略：深度学习是一门经验主义非常重的方向，Dropout的使用同样需要很多经验。一般情况下建议靠近输入层的把keep_prob设置大一些，就是靠近输入层经历多保留神经元。 优化与机器学习的区别讲完Dropout的使用，话说回来，为什么要用Dropout？ 提到这个问题，就不得不先做一下铺垫，先谈一下优化与机器学习的区别。 机器学习主要包括如下几块内容： 数据集 模型 损失函数 优化算法 其中优化算法直接决定着最终模型效果的好坏，因此，很多人都肆意的扩大优化算法的价值，认为“机器学习就是优化算法”。 我认为这是不严谨的说法机器学习与优化算法有这本质的区别。优化算法主要用于已知或未知数学模型的优化问题，它主要关注的是在既定模型上的误差，而不关注它的泛化误差。而机器学习则不同，它是在训练集上训练书模型，训练过程中与优化算法类似，考虑在训练集上的误差，但是它对比于优化算法还要多一步，要考虑在测试集上的泛化误差。 过拟合 (图片截取自吴恩达《深度学习工程师》) 在训练集和测试集这两个数据集上的精确度就引出几种情况： 在训练集上效果不好，在测试集上效果也不好：欠拟合(上图中第一种情况) 在训练集上效果很好，在测试集上效果不好：过拟合(上图中第三种情况) 在实际项目中，这两种情况是非常常见的，显然，这并不是我们想要的，我们追求的是第三种情况： 在训练集和测试集上的效果都相对较好(上图中第二种情况) 但是，事与愿违，欠拟合和过拟合是机器学习中非常常见的现象，尤其是过拟合。 过拟合的形成原因主要包括如下几点： 训练数据集太少 参数过多 特征维度过高 模型过于复杂 噪声多 很多研究者把目光和精力都聚焦在解决过拟合这个问题上，目前用于解决过拟合问题的算法有很多，例如， 权重衰减 Early stopping 批量归一化(没错，就是前一讲讲的BN，它也带有一些正则化的功能) 在解决过拟合问题的算法中最为知名的词汇莫过于正则化。 提到正则化，很多同学会想到L1、L2正则化，其实它是一类方法的统称，并非仅限于L1、L2正则化，目前用于结果过拟合的正则化方法主要包括如下几种： 数据扩充 L1、L2正则化 Dropout 没错，Dropout也是正则化方法中的一种！铺垫这么多，终于引出本文的主角了。 数据扩充解决过拟合，这一点不难理解，因为数据缺少是引起过拟合的主要原因之一，由于数据的却是导致模型学习过程中不能学到全局特征，只能通过少量数据学习到一个类别的部分特征，通过数据的扩充能够让模型学习到全局特征，减少过拟合现象。 L1、L2正则化主要为损失函数添加一个L1或L2的正则化惩罚项，防止学习过程中过于偏向于某一个方向引起过拟合。 最后就轮到Dropout，下面来详细讲一下Dropout的原理。 Dropout如何使用Dropout？ (图片来自于《Dropout: A Simple Way to Prevent Neural Networks from Over tting》) 上图中左图为一个标准的神经网络，右图是采用Dropout之后的神经网络，其中的区别一目了然，就是丢弃了一些神经元。 前面已经讲解了在tensorflow中如何使用Dropout，已经清楚，对于Dropout最为核心的就是保留概率或者丢弃概率，简述它的原理就是：遍历神经网络的每一层中每一个神经元，以一定概率丢弃或保留某个神经元。用数学语言描述如下， 假设某一个神经元的输入有4个，那么神经元的计算表达式为， h_{i}=\phi\left(x_{1} w_{1 i}+x_{2} w_{2 i}+x_{3} w_{3 i}+x_{4} w_{4 i}+b_{i}\right)其中$x_i$是输入，$w$是权重，$b$是偏差。 假设保留概率为$p$，那么丢弃改为就为$1-p$，那么神经元$h_i$就有$1-p$的概率被丢弃，那么经过Dropout运算后的神经元为， h_{i}^{\prime}=\frac{\xi_{i}}{1-p} h_{i}其中$\xi_{i}$为0或者1，它为0或者1的概率分别为$1-p$和$p$，如果为0,则这个神经元被清零，临时被丢弃，一定要注意，是临时的丢弃，Dropout是有放回的采样。在一轮训练中前向或反向传播都会用丢弃后的神经网络，下一轮训练又会随机丢弃，用一个新的网络去训练。 编程实现Dropout其实只需要几行代码，下面结合代码来解释，会更容易理解， 123456def dropout(X, keep_prob): assert 0 &lt; keep_prob &lt; 1 if keep_prob == 0: return np.zeros(X.shape) mask = np.random.uniform(0, 1, X.shape) &lt; keep_prob return mask * X / keep_prob 输入参数为上一层的激活值和保留概率， 第3行：如果保留概率为0，也就是不保留的话，则全部元素都丢弃。 第5行：生成一个随机的掩码，掩码和输入X形状相同，每个位置非0即1，然后用这个掩码与X相乘，如果对应位置乘以0，则这个神经元被丢弃，反之保留。 Dropout为什么起作用？ (图片来自《深度学习》) 这里不得不提一下Bagging集成集成学习方法，在Bagging集成学习方法中，预先会定义k的模型，然后采用k个训练集，然后分别训练k个模型，然后以各种方式去评价、选取最终学习的模型。 Dropout的训练过程中与Bagging集成学习方法类似，以上图为例，有一个三层神经网络(两个输入神经元、两个隐藏神经元、一个输出神经元)，从这个基础的网络中随机删除非输出神经元来构建一些子集。这样每一次训练就如同在这些子集中随机选择一个不同的网络模型进行训练，最后通过”投票”或者平均等策略而选择出一个最好的模型。 其实这个思想并不陌生，在传统机器学习中Adaboost、随机森林都是采用集成学习的思想，效果非常好。采用Dropout后的深度神经网络和这类思想也是类似的，这样能够结合不同网络模型的训练结果对最终学习的模型进行评价，能够综合多数，筛选掉少数，即便是某个网络模型出现了过拟合，最终经过综合也会过滤掉，效果自然会好很多。 需要说明一点的是，虽然采用Dropout和其他集成学习方法思想有异曲同工之处，但是也有一些细节的差异。在Bagging中所有模型都是独立的，但是在Dropout中所有模型是共享参数的，每个子模型会继承父网络的网络参数，这样可以有效的节省内存的占用。 需要注意的点到这里，Dropout的内容就讲解完了，总结一些本文，需要有几点需要注意， keep_prob是每个神经元被保留的概率 Dropout和L1、L2、数据扩充都属于正则化方法 Dropout的丢弃是有放回的 参考文献 Dropout: A Simple Way to Prevent Neural Networks from Over tting Dropout as data augmentation Improving Neural Networks with Dropout Improving neural networks by preventing co-adaptation of feature detectors 《深度学习》 资源获取我把参考文献中列出的4片文章和《深度学习》这本书籍的电子版进行整理共享了，感兴趣的可以关注公众号，回复关键字“dl”获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第四讲：类的特殊方法(下篇)]]></title>
    <url>%2F2019%2F08%2F28%2Fpython-4%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言特殊方法是为我们定义的类添加上某些特殊功能的方法，上一讲分组讲解了Python的几对特殊方法(或者成为魔术方法)，分别是， __new__与__init__ __enter__与__exit__ __str__与__repr__ __setattr__、__getattr__、__getattribute__与__delattr__ 这些都是相对较为常用的。Python中类的特殊方法远不止这些，其中还有一些不太常用，或者在某些特定场景下用到的特殊方法。 本讲会按照功能对剩余的特殊方法进行分类，不再详细的把每个特殊方法的使用都展开阐述，会着重的从每种功能中挑选出具有代表性的特殊方法进行实现、详细讲解。 # 函数调用 假如我们定义一个用于算数运算的类， 12345678910111213class Operation(object): def __init__(self, x, y): self.x = x self.y = y def add(self): print("The result is &#123;&#125;".format(self.x + self.y)) opt = Operation(2, 2)opt.add()# 输出The result is 4 从这段代码可以看出 ，这是我们一贯使用类及类方法的方式，实例化—调用，其实Python提供有特殊方法__call__能够让类的调用像调用函数的方式一样。 这句话听着似乎很绕口，具体什么含义呢？用一段代码来说明， 123456789101112131415161718class Operation(object): def __init__(self): self.x = None self.y = None def add(self): print("The result is &#123;&#125;".format(self.x + self.y)) def __call__(self, x, y): self.x = x self.y = y self.add() opt = Operation()opt(3, 3)# 输出The result is 6 当我们给类添加特殊方法__call__后，我们可以直接使用实例名(opt)来调用类的方法，就不用在用instance.method的方法去调用。换句话说就是，当我们定义__call__后，我们使用实例名进行调用时，它会首先进入__call__方法，执行__call__中的程序。 容器与序列容器和序列分别涉及2个特殊方法：__contains__、__len__。 从__len__名称就可以看出它的功能，给类添加一个获取序列长度的功能，所以这里着重讲解一下容器，顺带讲解一下__len__。 我们在条件语句中经常会用到这样的语句if … in、if … not in，其中__contains__就可以给类添加这样一个功能，可以通过if … in、if … not in来调用类的实例，以一段代码来举例， 123456789101112131415161718192021222324class Contain(object): def __init__(self, data): self.data = data def __contains__(self, x): return x in self.data def __len__(self): return len(self.data) contain = Contain([2,3,4,5])if 2 in contain: print("222222") if 6 not in contain: print("666666") len(contain) # 输出2222226666664 从代码中可以看出，当我们调用if 2 in contain时会调用__contains__这个特殊方法，通过方法中的语句返回一个布尔型的值。 此外，可以看到代码中有这样一句调用len(contain)，它就是前面提到的特殊方法__len__的功能，它可以给类添加一个获取序列长度的功能，当使用len(instance)时会调用__len__方法中的程序。 算数运算用于实现算数运算的有以下类的特殊方法的有以下几个， 运算 代码 特殊方法 加法 a + b __add__ 减法 a - b __sub__ 乘法 a * b __mul__ 除法 a / b __truediv__ 向下取整除法 a // b __floordiv__ 取余 a % b __mod__ 以一段代码举例说名加法与乘法的使用， 123456789101112131415161718192021class Operation(object): def __init__(self, value): self.value = value def __add__(self, other): return Operation(self.value + other.value) def __mul__(self, other): return Operation(self.value * other.value) def __str__(self): return "the value if &#123;&#125;".format(self.value) a = Operation(3)b = Operation(5)print(a + b)print(a * b)# 输出the value if 8the value if 15 同理，其他几种算法运算的使用方法同加法、乘法相同。 比较运算类的特殊方法不仅提供了算术运算，还提供了比较运算的特殊方法，它们分别是，| 运算 | 代码 | 特殊方法 || —————— | ——— | ———————— || 等于 | a == b | __eq__ || 不等 | a != b | __ne__ || 大于 | a &gt; b | __gt__ || 小于 | a &lt; b | __lt__ || 大于等于 | a &gt;= b | __ge__ || 小于等于 | a &lt;= b | __le__ | 以一段代码解释比较运算符的使用， 123456789101112131415class Cmp(object): def __init__(self, value): self.value = value def __eq__(self, other): return self.value == other.value def __gt__(self, other): return self.value &gt; other.valuea = Cmp(3)b = Cmp(3)a == b# 输出True 可以看出，比较运算和算术运算的使用非常相似。 字典功能我们可以通过如下几个特殊方法为类添加如同字典一样的功能，| 运算 | 代码 | 特殊方法 || —————— | ——— | ———————— || 取值 | x[key] | __setitem__ || 设置值 | x[key]=value | __getitem__ || 删除值 | del x[key] | __delitem__ | 下面以一段代码举例说明， 123456789101112131415161718192021222324252627282930313233class Dictionaries(object): def __setitem__(self, key, value): self.__dict__[key] = value def __getitem__(self, key): return self.__dict__[key] def __delitem__(self, key): del self.__dict__[key]diction = Dictionaries()diction["one"] = 1diction["two"] = 2diction["three"] = 3diction['three']del diction['three']diction['three']# 输出3---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-57-dfe1a566046b&gt; in &lt;module&gt;()----&gt; 1 diction['three']&lt;ipython-input-55-21dcfd1e91cb&gt; in __getitem__(self, key) 4 5 def __getitem__(self, key):----&gt; 6 return self.__dict__[key] 7 8 def __delitem__(self, key):KeyError: 'three' 可以看出，当删除键值为three的值之后再次去获取会报错。 其他除了上述提到的特殊方法之后，Python还有很多特殊方法，这里不一一举例说明，下面列举出这些特殊方法以及它们的功能和使用方法，如果感兴趣的可以对应的去查找文档学习。 运算 代码 特殊方法 类析构函数 del instant __del__ 格式化字符串 format(x, format_spec) __format__ 遍历迭代器 iter(list) __iter__ 取迭代器下一个值 next(list) __next__ 列出类的所有属性和方法 dir(instance) __dir__ 自定义散列值 hash(instance) __hash__ 自定义拷贝 copy.copy(instance) __copy__ 自定义深层拷贝 copy.deepcopy(instance) __deepcopy__ 上下文环境布尔值 if instance: __bool__ 当然，除了这些，Python还有其他的特殊方法，例如逻辑运算、按位运算等，感兴趣的可以参考官方文档仔细学习一下，本文仅列举一些相对常用的一些特殊方法。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号【平凡而诗意】回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十三讲：批量归一化]]></title>
    <url>%2F2019%2F08%2F24%2Fcnn-bn%2F</url>
    <content type="text"><![CDATA[前言当我们用一些数据做一个预测系统时，我们首先需要对数据进行预处理，例如标准化、正则化、滑动窗口等，比如常用的Z-score、最大最小标准化，它能将数据转化为同一个量级，这样的话能够保证数据的稳定性、可比性。 这些标准化方法在浅层神经网络中已经足够使用，效果已经很不错。但是在深度学习中，网络越来越深，使用这些标准化方法就难以解决相应的问题。 为什么需要批量归一化？ 在训练过程中，每层输入的分布不断的变化，这使得下一层需要不断的去适应新的数据分布，在深度神经网络中，这让训练变得非常复杂而且缓慢。对于这样，往往需要设置更小的学习率、更严格的参数初始化。通过使用批量归一化(Batch Normalization, BN)，在模型的训练过程中利用小批量的均值和方差调整神经网络中间的输出，从而使得各层之间的输出都符合均值、方差相同高斯分布，这样的话会使得数据更加稳定，无论隐藏层的参数如何变化，可以确定的是前一层网络输出数据的均值、方差是已知的、固定的，这样就解决了数据分布不断改变带来的训练缓慢、小学习率等问题。 在哪里使用批量归一化？ 批量归一化是卷积神经网络中一个可选单元，如果使用BN能够保证训练速度更快，同时还可以具备一些正则化功能。 在卷积神经网络中卷积层和全连接层都可以使用批量归一化。 对于卷积层，它的位置是在卷积计算之后、激活函数之前。对于全连接层，它是在仿射变换之后，激活函数之前，如下所示： 1234conv_1 = tf.nn.conv2d()norm_1 = tf.nn.batch_normalization(conv_1)relu_1 = tf.nn.relu(norm_1)pool_1 = tf.nn.max_pool(relu_1) 以卷积层为例，网络架构的流程为： 卷积运算 批量归一化 激活函数 池化 批量归一化 在讲批量归一化之前，首先讲一下数据标准化处理算法Z-score。 Z-score标准化也成为标准差标准化，它是将数据处理成均值为0，方差为1的标准正态分布，它的转化公式为， x^{*}=\frac{x-\overline{x}}{\sigma}其中$x$是处理前的数据，$x^{*}$是处理后的数据，$\overline{x}$是原始数据的均值，$\sigma$是原始的标准差。这样的话就可以把数据进行标准化。 其实批量归一化在思想上和Z-score是有很多共通之处的。 在深度学习训练过程中会选取一个小批量，然后计算小批量数据的均值和方差， \boldsymbol{\mu}_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)}\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}然后对数据进行归一化处理， \hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}}\boldsymbol{y}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}经过这样处理，就可以使得数据符合均值为$\boldsymbol{\mu}$、方差为$\sigma_{\mathcal{B}}^{2}$的高斯分布。 下面看一下原文中批量归一化的算法步骤： 获取每次训练过程中的样本 就算小批量样本的均值、方差 归一化 拉伸和偏移 这里要着重介绍一下最后一步尺度变换(scale and shift)，前面3步已经对数据进行了归一化，为什么还需要拉伸和偏移呢？ 因为经过前三步的计算使得数据被严格的限制为均值为0、方差为1的正态分布之下，这样虽然一定程度上解决了训练困难的问题，但是这样的严格限制网络的表达能力，通过加入$\gamma$和$\beta$这两个参数可以使得数据分布的自由度更高，网络表达能力更强。另外，这两个参数和其他参数相同，通过不断的学习得出。 tensorflow中BN的使用在tensorflow中可以直接调用批量归一化对数据进行处理，它的函数为， 1tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None) 来解释一下函数中参数的含义： x：输入的数据，可以是卷积层的输出，也可以是全连接层的输出 mean：输出数据的均值 variance：输出数据的方差 offset：偏移，就是前面提到的beta scale：缩放，前面提到的gamma variance_epsilon：一个极小的值，避免分母为0 更多内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用资源 | 推荐6个高赞有趣的Github项目]]></title>
    <url>%2F2019%2F08%2F23%2Fgithub-project%2F</url>
    <content type="text"><![CDATA[前言Github，应该很多同学都听过这个鼎鼎大名的开源项目托管平台。 刚开始接触github，我和大多数同学一样，仅仅把它当作一个开源项目搜索工具。比如当看到一篇文章，会去搜索一下看看作者有没有开源源代码，仅此而已，因此一直以来对github的依赖都不太强。 最近几个月以来，我每天会特意抽出一段时间去github看一下，看看近期有没有什么热门有趣的项目，慢慢的，在github上发现了很多不错的项目，久而久之就对其产生了依赖。现在每天都会去看一下，在这个过程总的确从这些优质的开源项目上学到了很多，收获了很多，下面我就推荐5个我个人觉得不错的开源项目。 free-programming-books-zh_CN项目地址：https://github.com/justjavac/free-programming-books-zh_CN star：54k+ 在从事IT、互联网相关的开发过程中，我们会用到各种各样的知识，linux、数据库、编程语言、虚拟化等。这样就需要我们不断的去学习，很多时间和金钱比较充足的可以选择一些课程或者买一些书籍，但是大多数人是不会把所用到知识相关的书籍都购买一遍。这样就面临一个问题：我们该去哪获取相应的学习资源？ free-programming-books-zh_CN收集了计算机领域很多知名的书籍，它包括但不限于以下种类： 各种编程语言 版本控制 数据库 大数据 操作系统 编译原理 web … 有了这个项目就再也不用担心学习资源的问题了，它手机了计算机各个分支相关的经典书籍和优质学习资源，避免了在茫茫的网络中去搜索的麻烦。 sherlock项目地址：https://github.com/sherlock-project/sherlock star：6.8k+ 在到处充斥着互联网、社交的时代，用户名是每个人都不陌生的词汇，当我们注册一个社交网站，例如知乎、微博等，需要起一个名称。很多人都喜欢独一无二、与众不同，但是在这么多用户名字，一不小心就和别人重复了，sherlock这个项目就可以解决这个问题，它能够在不同的社交网站上搜索是否存在指定的用户名，这样的话你就可以看到自己起的用户名有没有重复？有哪些重复。 weekly项目地址：http://link.zhihu.com/?target=https%3A//github.com/ruanyf/weeklystar：8.1k+ 这是一个科技爱好者周刊。 现在是一个信息爆炸的社会，各种新媒体、自媒体，每天各个APP有看不完、层出不穷的新闻，但是，我个人认为大多数自媒体的水平有待商榷，在他们看来，一个吸引人的标题比实际的内容还要重要。所以我很苦恼，每天喜欢看看新闻，但是花费几十分钟后发现都是一些乱七八糟没有价值的新闻。 直到几个月前我在github看到weekly这个项目，它每周五定期更新一次，慢慢的，周五在我心里有了一些期待，期待着这个项目更新周刊。 为什么它如此吸引我？ 它与众不同，而且都是经过筛选的一些新奇有趣的新闻，我觉得称其为新闻，但它不仅限于新闻，它包含如下内容： 新奇的资讯 优质中文、英文文章 高效工具 资源 精选图片 文摘 言论 UnblockNeteaseMusic项目地址：http://link.zhihu.com/?target=https%3A//github.com/nondanee/UnblockNeteaseMusic star：4.4k+ 从这个项目的名称即可看出它的功能，unblock netease music。 网易云音乐是很多人喜欢的一款音乐播放器，我也不例外，但是发现它上面的音乐越来越少，当你想听一首歌时发现，它竟然是灰色的，也就是不能听。 有了这个项目，它可以从QQ / 虾米 / 百度 / 酷狗 / 酷我 / 咕咪 / JOOX等音乐源寻找资源进行替换，也就是说，有了UnblockNeteaseMusic+网易云音乐，你可以听来自不同音乐源的歌曲。 ChineseBQB项目地址：https://github.com/zhaoolee/ChineseBQB star：5k+ 这个一个表情包博物馆，目前共收录了3319个表情包，包含各种各样热门、搞笑的表情包，有了这个github项目，再也不用为”斗图”担心了。 先来几个看一下， LeetCodeAnimation项目地址：https://github.com/MisterBooo/LeetCodeAnimation star：38k+ 娱乐之后不得不说一些沉重的话题，已经是8月底了，大批量的校招马上就要开始了 ~ 不知道关注我的同学里有多少位要参加今年的校园招聘，如果有，首先预祝各位找到理想的工作！ 其次，就是推荐一份不错的学习资源。 LeetCode，这个大名鼎鼎的平台应该很多参加过校招或者即将参加校招的同学应该都有所耳闻，是很多参加互联网、IT方向校招同学的必经之路，甚至周围有同事说”没有刷过LeetCode的校招，是不完整的！”。 github上有关leetcode的项目有很多，但是99%都是如出一辙，把leetcode上的题目做一下或者寻找一些答案，然后上传到github，只有静态的代码和简短的文字描述，这对于很多初学者是很难以理解的。 LeetCodeAnimation这个项目却与众不同，它通过动画的形式来阐述不同算法的解题思路，更加生动形象，下面就看一个例子—无重复字符的最长子串， 更多内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十二讲：激活函数]]></title>
    <url>%2F2019%2F08%2F21%2Factivation%2F</url>
    <content type="text"><![CDATA[完整代码链接：aiLearnNotes 前言激活函数不仅对于卷积神经网络非常重要，在传统机器学习中也具备着举足轻重的地位，是卷积神经网络模型中必不可少的一个单元，要理解激活函数，需要从2个方面进行讨论： 什么是激活函数？ 为什么需要激活函数？ 什么是激活函数？ 对于神经网络，一层的输入通过加权求和之后输入到一个函数，被这个函数作用之后它的非线性性增强，这个作用的函数即是激活函数。 为什么需要激活函数？ 试想一下，对于神经网络而言，如果没有激活函数，每一层对输入进行加权求和后输入到下一层，直到从第一层输入到最后一层一直采用的就是线性组合的方式，根据线性代数的知识可以得知，第一层的输入和最后一层的输出也是呈线性关系的，换句话说，这样的话无论中加了多少层都没有任何价值，这是第一点。 第二点是由于如果没有激活函数，输入和输出是呈线性关系的，但是现实中很多模型都是非线性的，通过引入激活函数可以增加模型的非线性，使得它更好的拟合非线性空间。 目前激活函数有很多，例如阶跃函数、逻辑函数、双曲正切函数、ReLU函数、Leaky ReLU函数、高斯函数、softmax函数等，虽然函数有很多，但是比较常用的主要就是逻辑函数和ReLU函数，在大多数卷积神经网络模型中都是采用这两种，当然也有部分会采用Leaky ReLU函数和双曲正切函数，本文就介绍一下这4个激活函数长什么样？有什么优缺点？在tensorflow中怎么使用？ SigmoidSigmoid函数的方程式为： f(x)=\sigma(x)=\frac{1}{1+e^{-x}} 绘图程序： 123456def sigmoid(): x = np.arange(-10, 10, 0.1) y = 1 / (1+np.exp(-x)) plt.plot(x, y) plt.grid() plt.show() Sigmoid函数就是前面所讲的逻辑函数，它的主要优点如下： 能够将函数压缩至区间[0, 1]之间，保证数据稳定，波动幅度小 容易求导 缺点： 函数在两端的饱和区梯度趋近于0，当反向传播时容易出现梯度消失或梯度爆炸 输出不是0均值(zero-centered)，这样会导致，如果输入为正，那么导数总为正，反向传播总往正方向更新，如果输入为负，那么导数总为负，反向传播总往负方向更新，收敛速度缓慢 对于幂运算和规模较大的网络运算量较大 双曲正切函数双曲正切函数方程式： f(x)=\tanh (x)=\frac{\left(e^{x}-e^{-x}\right)}{\left(e^{x}+e^{-x}\right)} 绘图程序： 123456def tanh(): x = np.arange(-10, 10, 0.1) y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) plt.plot(x, y) plt.grid() plt.show() 可以看出，从图形上看双曲正切和Sigmoid函数非常类似，但是从纵坐标可以看出，Sigmoid被压缩在[0, 1]之间，而双曲正切函数在[-1, 1]之间，两者的不同之处在于，Sigmoid是非0均值(zero-centered)，而双曲是0均值的，它的相对于Sigmoid的优点就很明显了： 提高了训练效率 虽然双曲正切函数解决了Sigmoid函数非0均值的问题，但是它依然没有解决Sigmoid的两位两个问题，这也是tanh的缺点： 梯度消失和梯度爆炸 对于幂运算和规模较大的网络运算量较大 ReLUReLU函数方程式： f(x)=\left\{\begin{array}{ll}{0} & {\text { for } x]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第三讲：类的特殊方法(上篇)]]></title>
    <url>%2F2019%2F08%2F18%2Fpython-3%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言Python是一种面向对象的语言，而特殊方法又是Python类中一个重点，因此学习Python类的特殊方法能够有助于设计出更加简洁、规范的代码架构。 Python类的特殊方法又称为魔术方法，它是以双下划线包裹一个词的形式出现，例如__init__。特殊方法不仅可以可以实现构造和初始化，而且可以实现比较、算数运算，此外，它还可以让类像一个字典、迭代器一样使用，可以设计出一些高级的代码，例如单例模式。 面向对象这个词大家应该都不陌生，在C++、Java等面向对象的语言中也经常出现，要想理解面向对象，首先要理解4个概念之间的关系：类、对象、实例、方法。 类：类是一种由不同属性、不同数据组成的一个集合。用直白的话来描述，它是由多种对象组成的一个组合，例如人是一个类，那么它包含男人、女人、儿童等对象。例如三角形是一个类，那么它包含等腰三角形、直角三角形、等边三角形等对象。 对象：前面介绍类中已经提到了对象这个词汇，一句话总结：对象具有具体状态和行为。例如直角三角形，它具有特定的状态和属性。 实例：对象就是类的一个实例。也许这有点绕，的确对象与实例之间的概念非常模糊。你可以理解为对象是一个概念性的存在，而实例是采取行为、动作的载体，以一段代码举例， 1234class Animal(object): passanimal = Animal() 其中Animal是一个类，而animal是一个实例，它可以访问类内的方法，实施“动作”和“行为”。 方法：定义在类外部的函数叫做函数，定义在类内部的函数称为方法。 这些概念在Python面向对象编程中非常概念，只有理解这些概念才能在后续学习中更加容易理解，对上述这些概念有一个简单的了解，在后续的讲解中会更加轻松。 __new__与__init__之所以把这个放在第一个，因为这个不仅非常常用，而且很容易被误解，甚至很多知名的书籍中都把这个特殊方法弄错。 很多博客和个别书籍中都把__init__当作类似于C++的构造方法，其实这个理解是错误的。 1234567891011121314class Animal(object): def __new__(cls, *args, **kargs): instance = object.__new__(cls, *args, **kargs) print("&#123;&#125; in new method.".format(instance))# return instance # 不返回实例 def __init__(self): print("&#123;&#125; in init method.".format(self)) animal = Animal()# 输出&lt;__main__.Animal object at 0x000002BB03001CF8&gt; in new method. 以上面为例，我们对基类中的__new__进行重构，不让它返回实例，可以从输出结果可以看出，程序没有进入__init__方法。这是因为__new__是用来构造实例的，而__init__只是用来对返回的实例进行一些属性的初始化，我们在写一个类的时候首先都会写一个__init__方法去初始化变量，却很少使用__new__，因此就容易忽略__new__，其实在我们继承基类object(例如，class Animal(object))时同时就从基类中继承了__new__方法，所以就不需要重新在子类中实现，如果把上述注释取消掉，再看一下， 123456789101112131415class Animal(object): def __new__(cls, *args, **kargs): instance = object.__new__(cls, *args, **kargs) print("&#123;&#125; in new method.".format(instance)) return instance def __init__(self): print("&#123;&#125; in init method.".format(self)) animal = Animal()# 输出&lt;__main__.Animal object at 0x000002BB03001B00&gt; in new method.&lt;__main__.Animal object at 0x000002BB03001B00&gt; in init method. 可以看出，程序先运行到new中，然后进入init方法。 对于__init__应该都很熟悉，为什么很少使用__new__呢？因为大多数情况下我们是用不到它的。但是存在的即是合理的，它自然有自己的价值。 __new__在哪些场景能够用到呢？ 当实现一些高级的软件设计模式可能会用到__new__方法，它主要有以下几点用处， 重构一些不可变方法，例如，int, str, tuple 实现单例模式(Singleton Pattern) 这里着重介绍一下单例模式。 单例模式是一种常用的软件设计模式，有时候我们需要严格的限制一个类只有一个实例存在，一个系统只有一个全局对象，这样有利于协调系统的整体行为。 先看一下我们常用的写法， 1234567891011class NewInt(object): passnew1 = NewInt()new2 = NewInt()print(new1)print(new2)# 输出&lt;__main__.NewInt object at 0x000002BB03001390&gt;&lt;__main__.NewInt object at 0x000002BB02FF4080&gt; 从输出可以看出，上述两个实例new1、new2地址不同，是两个实例。 然后通过__new__实现单例模式， 12345678910111213141516class NewInt(object): _singleton = None def __new__(cls, *args, **kwargs): if not cls._singleton: cls._singleton = object.__new__(cls, *args, **kwargs) return cls._singletonnew1 = NewInt()new2 = NewInt()print(new1)print(new2)# 输出&lt;__main__.NewInt object at 0x000002BB02FF6080&gt;&lt;__main__.NewInt object at 0x000002BB02FF6080&gt; 地址相同，指向同一个对象，所以每次实例化产生的实例都是完全相同的。 __enter__与__exit__在介绍这两个特殊方法之前我们首先讲一下with语句。 with语句主要用于对资源进行访问的场景，例如读取文件。以读取文件为例，我们可以使用open、close的方法，但是使用with语句有着无法比拟的优势。 首先就是简洁，你不需要再写file.close的语句去关闭文件。 其次，也是最重要的，它能够很好的做到异常处理，当处理过程中发生异常，它能够自动关闭、自动释放资源。 以读取文件来对比一下两个功能， 如果使用open、close方式需要打开、读取、关闭3个过程， 12345# file.txtfp = open("file.txt", "rb")fp.readline()fp.close() 而使用with语句只需要打开、读取两个过程，当执行完毕会自动关闭， 12with open("file.txt", "rb") as fp: fp.readline() 说了这么多with语句的好处，这和__enter__与__exit__有什么关系？ __enter__与__exit__就是实现with的类特殊方法。 以一段代码来解释这两个特殊方法的使用， 12345678910111213141516171819202122232425class FileReader(object): def __init__(self): print("in init method") def __enter__(self): print("int enter method") return self def __exit__(self, exc_type, exc_val, exc_tb): print("in exit method") del self def read(self): print("in read")# with语句调用with FileReader() as fr: fr.read() # 输出in init methodint enter methodin readin exit method 从上面输出可以看出，程序先进去init方法进行初始化，然后进入enter特殊方法，然后通过fr.read调用read()方法，最后退出时调用exit方法。 这就是enter与exit的调用过程， __enter__：初始化后返回实例 __exit__：退出时做处理，例如清理内存，关闭文件，删除冗余等 __str__与__repr__一句话描述这两个特殊方法的功能：把类的实例变为字符串。 我们都知道，我们可以用这种方法输出一个字符串， 1print("Hello world!") 那我们怎么能够像字符串一样把实例输出出来？ 可以通过__str__与__repr__来实现， 12345678910111213141516class Person(object): def __init__(self, name, age): self.name = name self.age = age def __str__(self): return "str: &#123;&#125; now year is &#123;&#125; years old.".format(self.name, self.age) def __repr__(self): return "repr: &#123;&#125; now year is &#123;&#125; years old.".format(self.name, self.age)person = Person("Li", 27)print(person)# 输出str: Li now year is 27 years old. 可以看出，当使用print语句打印实例person时，能够像输出字符串那样把实例信息输出出来。 但是可以看出，程序进入__str__方法，并没有进入__repr__，这就引出了这两个方法的不同之处， __str__：用于用户调用 __repr__：用于开发人员调用 这似乎不太好理解，因为对于写程序的我们无法理解，何为用户？何为开发人员？ 简单的来说，__str__是用些Python脚本(.py)时使用，用print语句输出字符串信息。__repr__是我们在交互式环境下测试使用，例如cmd下的Python、ipython，例如在交互式环境下调用， 123&gt;&gt;&gt; person = Person("li", 27)&gt;&gt;&gt; personrepr: li now year is 27 years old. 更为简单的理解就是：__str__需要用print语句打印，__repr__只需输入实例名称即可。 __setattr__、__getattr__、__getattribute__与__delattr__了解上述这4个方法之前，我们先来解释一下什么是属性？ 也许很多同学已经清楚，但是我觉得还是有必要介绍一下，因为这是要讲的这3个特殊方法的关键。 1234567891011121314class Person(object): def __init__(self, name, age, home, work): self.name = name self.age = age self.home = home self.work = workperson = Person("Li", 27, "China", "Python")print(person.name)print(person.age)# 输出Li27 例如上面我们定义一个Person类，name、age、home、work就是它的属性，当实例化之后我们可以通过点.来访问它的属性。 我可以可以通过传入参数，赋值给self来定义类的属性，但是这样未免太固定了，当实例化之后就不能更改它的属性了，如果我们想获取、添加、删除属性怎么办？这就用到这里要讲的4个特殊方法，__setattr__、__getattr__、__getattribute__与__delattr__，它们的功能分别是， __setattr__：设置属性 __getattr__：访问不存的属性时调用，可能会有同学有疑问，访问不存的属性要它干吗？可以用来做异常处理！ __getattribute__：访问存在的属性，如果访问属性不存在的时候随后会调用__getattr__ __delattr__：删除属性 以一个例子来说明一下， 1234567891011121314151617181920212223242526272829303132class Person(object): def __init__(self, name): self.name = name def __setattr__(self, key, value): object.__setattr__(self, key, value) def __getattribute__(self, item): print("in getattribute") return object.__getattribute__(self, item) def __getattr__(self, item): try: print("in getattr") return object.__getattribute__(self, item) except: return "Not find attribute: &#123;&#125;".format(item) def __delattr__(self, item): object.__delattr__(item)person = Person("Li")print(person.name)print(person.age)# 输出in getattributeLiin getattributein getattrNot find attribute: age 从上面输出来看一下就可以明白，当获取属性name时，由于已经有了，则进入__getattribute__中，获取对应的属性，当获取属性age时，由于没有这个属性，则先进入__getattribute__，然后进入__getattr__，没有找到属性返回异常信息。 然后再来看一下看一下设置属性和删除属性， 123456789101112person.age = 27print(person.age)delattr(person, "age")print(person.age)# 输出in getattribute27in delattrin getattributein getattrNot find attribute: age 从输出结果可以看出，通过instance.attribute的方式可以设置属性，通过delattr可以删除属性。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号【平凡而诗意】回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十一讲：卷积层、池化层与填充]]></title>
    <url>%2F2019%2F08%2F16%2Fcv-cnn-pool%2F</url>
    <content type="text"><![CDATA[前言从2012年AlexNet成名之后，CNN如同雨后春笋一样，出现了各种各样的Net，其中也有很多知名的，例如VGG、GoogleNet、Faster R-CNN等，每个算法都在前面研究工作的基础上做出了很大的改进，但是这些CNN模型中主要使用的组件却有很多重叠之处，这个组件主要有： 卷积层 池化层 激活函数 优化函数 全连接层 Dropout 批量正则化 填充padding …… 其实一个CNN网络的模型搭建过程非常容易，现在有很多优秀的机器学习框架，例如tensorflow、pytorch、mxnet、caffe、keras等，借助这些机器学习框架搭建一个CNN网络模型只需要几十行代码即可完成，而且使用到的函数屈指可数，难度并不大。而上述提到的这些组件却是CNN中非常核心的概念，了解它们是什么？有什么价值？在哪里起作用？掌握这些之后再回头看这些CNN模型就会发现轻而易举，因此，这几节会先把上述这些技术介绍一下，然后逐个讲解如何一步一步搭建那些成熟优秀的CNN模型。 由于上述每个技术都涉及很多知识点，本文为了效率就用简单的语言介绍它是什么？有什么价值？具体详细的内容可以阅读文章或者外网资料详细了解，本文主要介绍3点： 卷积层 池化层 填充padding 卷积层介绍 卷积神经网络(convolutional neural network)，从它的名称就可以看出，卷积是其中最为关键的部分。在前面讲解图像去噪和图像分割中提到了一些用于分割和去噪的算法，例如sobel算子、中值滤波，其实卷积的概念和这些有相同之处。 把输入图像看作是一个n维矩阵，然后拿一个mm维(m&lt;n)的卷积核(或者称为滤波器)，从图像的左上角开始沿着从左至右、*从上之下进行”扫描”，每当移动到一个窗口后和对应的窗口做卷积运算(严格的说是互相关运算)，用直白的话来说就是对应元素相乘之后加和。 移动过程中涉及一个重要的概念—步长(stride)，它的意思就是”扫描”过程中每次移动几个像素，如果步长stride=1，那么从左至右、从上之下逐个像素的移动。 以上图二维卷积运算为例，输入图像为一个5*5的矩阵，卷积核为3*3，以步长stride=1进行卷积运算，在左上角这个窗口每个对应元素先相乘再加和，即， 0*0+1*1+2*2+1*5+2*6+0*7+2*0+1*1+0*2=23以这种方式逐个窗口进行计算，就得到图中等号右边的输出结果。 tensorflow使用 在tensorflow中关于卷积层的函数为， 1tensorflow.nn. conv2d(input, filter, strides, padding) 其中参数分别为： input：输入数据或者上一层网络输出的结果 filter：卷积核，它的是一个1*4维的参数，例如filter=[5, 5, 3, 96]，这4个数字的概念分别是卷积核高度、卷积核宽度、输入数据通道数、输出数据通道数 strides：这是前面所讲的步伐，同卷积核一样，它也是一个1*4维的参数，例如strides=[1, 2, 2, 1]，这4个数字分别是batch方向移动的步长、水平方向移动的步长、垂直方向移动的步长、通道方向移动的步长，由于在运算过程中是不跳过batch和通道的，所以通常情况下第1个和第4个数字都是1 padding：是填充方式，主要有两种方式，SAME, VALID，后面会讲什么是填充 池化层介绍 池化层和卷积层一样，是CNN模型必不可少的一个部分，在很多卷积层后会紧跟一个池化层，而且在统计卷积神经网络时，池化层是不单独称为网络层的，它与卷积层、激活函数、正则化同时使用时共同称为1个卷积层。 池化层又成为下采样或者欠采样，它的主要功能是对于特征进行降维，压缩数据和参数量，避免过拟合，常用的池化方式有两种： 最大池化 平均池化 以最大池化为例介绍一下它是怎么实现的， 和卷积层类似，池化层也有窗口和步长的概念，其中步长在里面的作用也是完全相同的，就是窗口每次移动的像素个数，所以不再赘述。 池化层的窗口概念和卷积层中是截然不同的，在卷积层中每移动到一个窗口，对应的卷积核和输入图像做卷积运算。而在池化层中，窗口每移动到一个位置，就选择出这个窗口中的最大值输出，如果是平均池化就输出这个窗口内的平均值。 tensorflow使用 tensorflow中池化运算的函数为， 1tensorflow.nn.max_pool(value, ksize, strides, padding) 从函数的参数即可看出来，它和卷积层非常相似，它的参数概念分别是， value：输入数据或者上一层网络输出的结果 ksize：卷积核，它的是一个1*4维的参数，例如ksize=[1, 3, 3, 1]，这4个数字的概念分别是batch维度池化窗口、池化窗口高度、池化窗口宽度、通道维度窗口尺寸，由于在batch和通道维度不进行池化，所以通常情况下第1和第4个元素为1 strides：这和卷积层中相同 padding：这和卷积层中的也相同 填充在前面讲解卷积层和池化层时都提到了一个概念—填充，可见它是非常重要的。什么是填充？SAME, VALID这两种填充方式又有什么区别？下面来介绍一下。 从前面卷积层和池化层可以看出，卷积层和池化层的输出尺寸大小和选取的窗口大小有着密切关系，以卷积层为例，上述输入为5*5，但是输出为3*3，输出尺寸变小了，而且在输入图像的四周的元素只被卷积了一次，中间的元素却被利用多次，也就是说，如果是一副图像，图像四周的信息未被充分提取，这就体现了填充的价值， 保持边界信息 使得输入输出图像尺寸一致 那怎么样达到上述这2个目的？就是通过填充，一般情况下是在图像周围填充0，如下， 如上图所示，在输入图像周围填充0，然后通过卷积运算，输入和输出的尺寸都为5*5。当然，这是针对卷积核为3*3情况下，外层填充1层，具体填充几层，要根据卷积核大小而定。 然后回到前面所提到的，tensorflow中填充padding参数有两个选项：SAME, VALID，它们有什么区别呢 ？ VALID：不进行填充 SAME：填充0，使得输出和输入的尺寸相同，就如同上面这个例子。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 推荐一款提高Python编程效率的神器]]></title>
    <url>%2F2019%2F08%2F13%2Fkite%2F</url>
    <content type="text"><![CDATA[前言“AI自动补全工具”，这个其实很久之前就有所耳闻，但是我却始终没有去尝试，因为，在我看来这两年人工智能泡沫太严重，各行各业都在蹭AI的热度，我想，也许”AI自动补全工具”也只不过是一个噱头吧。 在工作中，对于Python开发我一直都是以pycharm为主力。它也是Python开发中非常知名的一款IDE，支持DEBUG、格式提示、快速补全等等，有着非常吸引人的优点。尽管它非常臃肿、启动速度非常缓慢，但是对于追求补全速度的我来说，我还是选择忍受它的种种不足。 直到前不久在开发过程中发生的几次问题让我忍无可忍，我决心换掉这款工具，主要有如下几个原因： 内存占用大：16G的内存，pycharm占据了1G以上，使得电脑卡顿 license服务器崩溃：购买的license总是莫名其妙的出问题 臃肿：pycharm很强大，但是它的强大是建立在开启了很多辅助工具的基础上，这使得它非常臃肿卡顿 于是，我开始尝试不同的工具，VIM、vscode、sublime等。其中VIM在补全速度方面还可以，但是在windows下无法使用，而我有时在服务器下开发、有时会在windows下开发。至于vscode和sublime，界面和启动速度等都没的说，但是补全功能太弱，虽然配置了几款所谓的强大插件，但是依然跟不上编码的速度，于是，我又回到了pycharm，直到我遇到这款神奇的工具—kite，让我有一种柳暗花明的感觉，实在太强大了。 甚至Python之父Guido van Rossum都说I really love the line-of-code completions in the new kite.com，可见这款工具多么强大。 有了这一款工具，再也不用繁琐的配置sublime、vscode中各种插件和设置项了。 kite安装 kite是一款安装包+插件的工具，首先需要到官网下载kite的安装包，安装作为引擎，安装之后打开相应的编辑器或IDE安装kite的插件，然后就可以使用了，不用像sublime、vscode那样需要安装一堆插件还要到设置中配置Python路径之类的。 安装包下载可以直接到官网进行下载： https://www.kite.com/download/ 我把安装包进行共享了，如果访问官网速度比较慢，无法下载的话，可以在公众号后台回复kite获取。 双击安装 为什么推荐这款工具？ 一款好的编程工具能够让编码效率事半功倍，它不仅避免我们逐个敲击代码，还避免我们去记忆一些函数的名称。目前有很多有名气的IDE\编辑器，pycharm、eclipse、spyder、Atom、sublime、vscode等，每个人都有自己的习惯和偏好，所以每个人心中都有自己最认可的工具。但是不可否认，pycharm在Python开发方面是使用最为广泛的一款，它最吸引我的一点就是补全速度。虽然sublime、vscode等也可以通过配置插件来实现Python自动补全，但是速度和效果等方面始终和pycharm有着巨大差距。 所以长久以来，尽管我也体会到它的种种缺点，我还是在坚持使用pycharm，直到最近我遇到这款kite之后。它是一款基于人工智能的代码补全和文档查询工具。我觉得完全可以脱离臃肿的pycharm，利用sublime、vscode这些轻量的编辑器与kite结合使用，即可以避免缓慢的开启速度，还可以实现不亚于pycharm的补全速度。 当然，kite的功能不仅限于补全，它主要包括： 代码自动补全 文档查询 代码自动补全 直接来看一下它的补全速度，非常快。 目前的代码自动补全工具大多数都是通过上下文匹配、扫描第三方库的方式实现补全，这样都是通过你输入一个单词，它去扫描，可想而知，速度自然会很慢。但是kite则不同，它是通过人工智能的方式进行补全，当你属于一个单词，它能够像谷歌搜索那样，预测你接下来会输入什么，并按相关性进行排序。 它不仅支持Python内置函数补全，还支持第三方工具包的补全。此外，它还支持一些模块的补全，例如if…main…，能够极大的节省编码的时间，提升编码效率，经过统计，Kite的人工智能可以帮助减少47%的击键次数。 文档查询 当我们使用一个第三方库时，例如numpy、tensorflow、scipy等，我们对其中很多函数怎么使用？需要传入哪些参数并不清楚。当然你可以上网搜索一下，但是我认为现在网上的学习资料鱼龙混杂，最好的方法还是看文档，这样比较权威、严谨。 但是问题是去哪看文档？而且，找文档也很耗时间啊。 kite不仅可以自动补全的问题，它还可以解决文档查询的问题。 打开kite，输入你想搜索的模块，即可找到你想要看的文档。而且它非常简洁， 怎么使用 传入参数 返回值 以最简单明了的几句话概括这个模块的使用方法。 支持平台 kite是一块完全免费的工具，它目前支持以下两个平台： windows linux 支持工具 kite支持以下几种IDE\编辑器： pycharm Atom vscode sublime vim 因此，你有多种可选项，可以根据自己的喜好进行配置。即便你对目前所使用的编辑工具补全速度已经很满意了，我认为也不妨使用一下kite，用它作为一款文档查询工具，能够使得阅读文档效率大大提升。 支持语言 官方把它定义为一款Python自动补全工具，但是我在使用vscode开发javascript时发现kite同样能够实现补全，而且效果也不错，至于C++、Java等其他语言，我没有尝试，暂不清楚，感兴趣的可以试一下。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第二讲：装饰器]]></title>
    <url>%2F2019%2F08%2F10%2F1-decorators%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言前段时间我发了一篇讲解Python调试工具PySnooper的文章，在那篇文章开始一部分我简单的介绍了一下装饰器，文章发出之后有几位同学说”终于了解装饰器的用法了”，可见有不少同学对装饰器感兴趣。但是那篇文章主要的目的是在介绍PySnooper，所以没有太深入的展开讲解装饰器，于是在这里就详细的介绍一些装饰器的使用。 装饰器是Python中非常重要的一个概念，如果你会Python的基本语法，你可以写出能够跑通的代码，但是如果你想写出高效、简洁的代码，我认为离不开这些高级用法，当然也包括本文要讲解的装饰器，就如同前面提到的代码调试神器PySnooper一样，它就是主要通过装饰器调用的方式对Python代码进行调试。 什么是Python装饰器？ 顾名思义，从字面意思就可以理解，它是用来"装饰"Python的工具，使得代码更具有Python简洁的风格。换句话说，它是一种函数的函数，因为装饰器传入的参数就是一个函数，然后通过实现各种功能来对这个函数的功能进行增强。 为什么用装饰器？ 前面提到了，装饰器是通过某种方式来增强函数的功能。当然，我们可以通过很多方式来增强函数的功能，只是装饰器有一个无法替代的优势--简洁。 你只需要在每个函数上方加一个@就可以对这个函数进行增强。 在哪里用装饰器？ 装饰器最大的优势是用于解决重复性的操作，其主要使用的场景有如下几个： - 计算函数运行时间 - 给函数打日志 - 类型检查 当然，如果遇到其他重复操作的场景也可以类比使用装饰器。 简单示例 前面都是文字描述，不管说的怎么天花烂坠，可能都无法体会到它的价值，下面就以一个简单的例子来看一下它的作用。 如果你要对多个函数进行统计运行时间，不使用装饰器会是这样的， 12345678910111213141516171819202122from time import time, sleepdef fun_one(): start = time() sleep(1) end = time() cost_time = end - start print("func one run time &#123;&#125;".format(cost_time)) def fun_two(): start = time() sleep(1) end = time() cost_time = end - start print("func two run time &#123;&#125;".format(cost_time)) def fun_three(): start = time() sleep(1) end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) 在每个函数里都需要获取开始时间start、结束时间end、计算耗费时间cost_time、加上一个输出语句。 使用装饰器的方法是这样的， 1234567891011121314151617181920def run_time(func): def wrapper(): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): sleep(1) @run_timedef fun_two(): sleep(1) @run_timedef fun_three(): sleep(1) 通过编写一个统计时间的装饰器run_time，函数的作为装饰器的参数，然后返回一个统计时间的函数wrapper，这就是装饰器的写法，用专业属于来说这叫闭包，简单来说就是函数内嵌套函数。然后再每个函数上面加上@run_time来调用这个装饰器对不同的函数进行统计时间。 可见，统计时间这4行代码是重复的，一个函数需要4行，如果100个函数就需要400行，而使用装饰器，只需要几行代码实现一个装饰器，然后每个函数前面加一句命令即可，如果是100个函数，能少300行左右的代码量。 带参数的装饰器通过前面简单的例子应该已经明白装饰器的价值和它的简单用法：通过闭包来实现装饰器，函数作为外层函数的传入参数，然后在内层函数中运行、附加功能，随后把内层函数作为结果返回。 除了上述简单的用法还有一些更高级的用法，比如用装饰器进行类型检查、添加带参数的的装饰器等。它们的用法大同小异，关于高级用法，这里以带参数的装饰器为例进行介绍。 不要把问题想的太复杂，带参数的装饰器其实就是在上述基本的装饰器的基础上在外面套一层接收参数的函数，下面通过一个例子说明一下。 以上述例子为基础，前面的简单示例输出的信息是， 123func three run time 1.0003271102905273func three run time 1.0006263256072998func three run time 1.000312328338623 现在我认为这样的信息太单薄，需要它携带更多的信息，例如函数名称、日志等级等，这时候可以把函数名称和日志等级作为装饰器的参数，下面来时实现以下。 1234567891011121314151617181920212223242526def logger(msg=None): def run_time(func): def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("[&#123;&#125;] func three run time &#123;&#125;".format(msg, cost_time)) return wrapper return run_time@logger(msg="One")def fun_one(): sleep(1) @logger(msg="Two")def fun_two(): sleep(1) @logger(msg="Three")def fun_three(): sleep(1) fun_one()fun_two()fun_three() 可以看出，我在示例基本用法里编写的装饰器外层又嵌套了一层函数用来接收参数msg，这样的话在每个函数(func_one、func_two、func_three)前面调用时可以给装饰器传入参数，这样的输出结果是， 123[One] func three run time 1.0013229846954346[Two] func three run time 1.000720500946045[Three] func three run time 1.0001459121704102 自定义属性的装饰器上述介绍的几种用法中其实有一个问题，就是装饰器不够灵活，我们预先定义了装饰器run_time，它就会按照我们定义的流程去工作，只具备这固定的一种功能，当然，我们前面介绍的通过带参数的装饰器让它具备了一定的灵活性，但是依然不够灵活。其实，我们还可以对装饰器添加一些属性，就如同给一个类定义实现不同功能的方法那样。 以输出日志为例，初学Python的同学都习惯用print打印输出信息，其实这不是一个好习惯，当开发商业工程时，你很用意把一些信息暴露给用户。在开发过程中，我更加鼓励使用日志进行输出，通过定义WARNING、DEBUG、INFO等不同等级来控制信息的输出，比如INFO是可以给用户看到的，让用户直到当前程序跑到哪一个阶段了。DEBUG是用于开发人员调试和定位问题时使用。WARING是用于告警和提示。 那么问题来了，如果我们预先定义一个打印日志的装饰器， 123456def logger_info(func): logmsg = func.__name__ def wrapper(): func() log.log(logging.INFO, "&#123;&#125; if over.".format(logmsg)) return wrapper logging.INFO是打印日志的等级，如果我们仅仅写一个基本的日志装饰器logger_info，那么它的灵活度太差了，因为如果我们要输出DEBUG、WARING等级的日志，还需要重新写一个装饰器。 解决这个问题，有两个解决方法： 利用前面所讲的带参数装饰器，把日志等级传入装饰器 利用自定义属性来修改日志等级 由于第一种已经以统计函数运行时间的方式进行讲解，这里主要讲解第二种方法。 先看一下代码， 123456789101112131415161718192021222324252627282930313233343536import loggingfrom functools import partialdef wrapper_property(obj, func=None): if func is None: return partial(wrapper_property, obj) setattr(obj, func.__name__, func) return funcdef logger_info(level, name=None, message=None): def decorate(func): logmsg = message if message else func.__name__ def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) @wrapper_property(wrapper) def set_level(newlevel): nonlocal level level = newlevel @wrapper_property(wrapper) def set_message(newmsg): nonlocal logmsg logmsg = newmsg return wrapper return decorate@logger_info(logging.WARNING)def main(x, y): return x + y 这里面最重要的是wrapper_property这个函数，它的功能是把一个函数func编程一个对象obj的属性，然后通过调用wrapper_property，给装饰器添加了两个属性set_message和set_level，分别用于改变输出日志的内容和改变输出日志的等级。 看一下输出结果， 12345main(3, 3)# 输出# WARNING:Test:main# 6 来改改变一下输出日志等级， 123456main.set_level(logging.ERROR)main(5, 5)# 输出# ERROR:Test:main# 10 输出日志等级改成了ERROR。 保留元信息的装饰器很多教程中都会介绍装饰器，但是大多数都是千篇一律的围绕基本用法在展开，少部分会讲一下带参数的装饰器，但是有一个细节很少有教程提及，那就是保留元信息的装饰器。 什么是函数的元信息？ 就是函数携带的一些基本信息，例如函数名、函数文档等，我们可以通过func.__name__获取函数名、可以通过func.__doc__获取函数的文档信息，用户也可以通过注解等方式为函数添加元信息。 例如下面代码， 1234567891011121314151617181920212223242526from time import timedef run_time(func): def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): ''' func one doc. ''' sleep(1) fun_one()print(fun_one.__name__)print(fun_one.__doc__)# 输出# wrapper# None 可以看出，通过使用装饰器，函数fun_one的元信息都丢失了，那怎么样才能保留装饰器的元信息呢？ 可以通过使用Python自带模块functools中的wraps来保留函数的元信息， 12345678910111213141516171819202122232425262728from time import timefrom functools import wrapsdef run_time(func): @wraps(func) # &lt;- 这里加 wraps(func) 即可 def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): ''' func one doc. ''' sleep(1) fun_one()print(fun_one.__name__)print(fun_one.__doc__)# 输出# fun_one # func one doc. 只需要在代码中加入箭头所指的一行即可保留函数的元信息。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号回复回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第一讲：开篇]]></title>
    <url>%2F2019%2F08%2F04%2Fpython-one%2F</url>
    <content type="text"><![CDATA[前言 2015年，本科毕业的那个暑假我开始疯狂的投递简历，希望找一份数据分析与数据挖掘相关的实习工作。直到有一家公司的面试官问到我：“你会Python吗？”，我当时一脸疑惑，因为，大学期间只系统的学习过C语言，后期开发系统中用到过少量的C#。于是我问面试官：“你能给我拼写一下这么语言的名字吗”？多年之后回想起来还会觉得很尴尬，真的是孤陋寡闻。 从那以后，“Python”这么语言经常出现在我耳边。读研之后我开始之后我主要研究的方向是传统目标识别和图像处理，主要使用的语言就是C++和Matlab，所以Python在我读研第一年并不是主力工具。研二开始后我开始进入深度学习这个领域，开始用到很多第三方的工具包，例如caffe、tensorflow已经CNN，那以后开始以Python语言为主。 因为之前有一些编程基础，在加上当初面试实习时时间紧迫，所以我就抽了一天的时间把Python基础教程看了一遍，了解了基本用法之后就成功的面试上了一份实习工作。那时候我认为Python是简单的，因为它不像C++、Java那样有严格的语法规范、有变量类型的概念，你只需要记住缩进正确即可。而且在做自然语言和计算机视觉过程中很多部分的代码都是依托第三方工具包完成，真正自己开发的只是一些数据预处理、文本处理以及用一些条件循环语句对逻辑进行串联。 直到后来从事工作以后，做了更多有严格交付要求的项目之后才发现，Python并没有想象的那么简单，“会用Python容易，用好Python不易”，这是我使用几年Python之后的感触。 当你做一个项目要考虑到代码的复用性、易读性、运行效率、后期维护成本以及面对一些复杂的数据结构时，你会发现Python绝对不是简简单单利用那些基本知识能够实现的。 这也是我开始这个系列分享的原因，第一：把自己开发过程中的一些心得和经验总结下来。第二：如果能够帮助更多的Python学习者，那就更加荣幸了。 为什么要用Python？近几年唱衰Python的声音不拘于耳，有些人是的确发现并感受到了Python的缺点，但是更多的人是跟风式的唱衰Python。“Python效率低”，很多人都这样说，这显然有一些以偏概全的感觉，如果做游戏、软件，Python的确不占优势，但是如果作为算法工程师，进行算法验证，我想没有几个人会选择C/C++。口说无凭，先看几组数据对比。 PYPL 通过分析在谷歌上搜索语言教程的频率，创建了编程语言索引的PYPL流行度。 首先看一下PYPL最新编程语言流行程度， Python居于第一，力压Java、JS、PHP这些名气非常大的编程语言，而且前10名中2~9名都出现了负增长，而Python却4.5%的正向增长率。 如果觉得一个平台不够具有说服力，可以再看看另外一个知名的编程社区的排名。 TIOBE编程社区 TIOBE编程社区指数是编程语言受欢迎程度的一个指标。该指数每月更新一次。这些排名是基于全球熟练工程师、课程和第三方供应商的数量。流行的搜索引擎，如谷歌，必应，雅虎!美国、维基百科(Wikipedia)、亚马逊(Amazon)、YouTube和百度被用来计算收视率。值得注意的是，TIOBE索引不是关于最好的编程语言，也不是编写大多数代码行的语言。 来看一下TIOBE社区7月的编程语言排名， Python仅次于Java和C，排在第三名，而且对比去年同期，前10名中Python增长速度最快，达到2.9%。 从这里可以看出，Python一直被唱衰、一直很坚挺，尽管几年量关于Go、julia、Rust的呼声很高，但是依然无法撼动Python的地位，而且这些编程语言到底好不好用？有没有炒作的成分在里面？现在还是一个问号。 话说回来为什么Python如此受欢迎？ 我认为存在的即是合理的，如果它真的一无是处、漏洞百出，是经不住众人的考验的。它之所以如此受欢迎，自然有很多吸引人的方面： 简单易用、节省时间 丰富的第三方工具包 强大的社区 应用场景丰富 其他三个方面暂且不说，就说一些第一点，简单易用、节省时间，我觉得有这一个理由就足以吸引很多人。尤其是对于算法、测试等岗位，真正的耗费心思的并不在编程、开发这一块，编程语言是用来验证算法的可靠性的，但是没有这个编程语言，自然无法验证，这就体现出有一个简单易用的语言有多么重要了。 吴恩达在《机器学习》这么课程里提到“硅谷的工程师大多数都会选择一个简单的编程语言对自己的算法进行验证，当确认有效之后会用c/c++等语言重新实现一遍”，这足以体现Python语言简单易用的优点。 Python距离第一个版本发布以及有28年，唱衰的言论从未间断，但是依旧坚挺。 尤其是机器学习的大规模应用、国家把人工智能智能技术上升到战略层次，使得Python称为独树一帜的编程语言，虽然这两年Go、Julia号称性能更好、更加易用，但是一直无法撼动Python在机器学习领域的地位，很难望其项背，为什么？我认为最主要的原因就是拥有强大的用户基础。现在在大多数企业，从事算法相关岗位的清一色的使用Python，更别说计算机视觉、自然语言这些强依赖Python第三方库的方向。 Python该怎么学习？我认为大多数编程语言的学习都可以简化为3个过程： 入门 进阶 强化 入门阶段网上的教程已经很多了，关于入门我个人是不推荐参加培训班的，因为就如同前面所说的那样，Python基础语法非常简单，尤其是有一些C、Matlab、C++等编程基础的同学来说，Python中的很多概念虽然和恰语言不是完全相同，但是相似度还是非常高的，可以达到触类旁通。我个人更倾向于使用在线教程，这里推荐两个不错的入门教程， 菜鸟教程 https://www.runoob.com/python/python-tutorial.html 廖雪峰Python https://www.liaoxuefeng.com/wiki/1016959663602400 强化阶段我认为需要在实际的项目和工作中去得到提升，就如同计算机视觉、自然语言处理一样，你从文章和练手项目中所能获取的只有那么多，如果像进一步得到升华就需要在项目中去面对困难、解决困难，这时候就会想尽方法去解决各种难题，不知不觉中会得到很大的提升。 本系列的主要目的是介绍进阶阶段，讲解一些Python的高级用法，对于入门和强化阶段自己可以私下完成。 书籍推荐如果时间比较冲突，我觉得可以系统的看一些Python书籍，因为书籍的严谨性和条理性更加有保障，在这我推荐3本我个人认为不错的书籍， 1.《Python编程 从入门到实践》 如果时间有限，我认为入门阶段可以通过菜鸟教程、廖雪峰Python进行学习。如果时间充足，我认为可以看一下入门书籍，因为更加严谨一些。 《Python编程 从入门到实践》是一本比较适合入门的书籍，环境配置、变量、列表、if语句、函数等基础的概念都会详细的展开介绍，这对于没有编成基础的同学非常有帮助。 2.《流畅的Python》 这是一本经得起考验的Python书籍。 它和大多数书籍和在线教程蜻蜓点水式的讲解不同，它更加深入，深入而不冗余，在你看这本书的时候你会发现，它的每一段话都是有意义的，没有什么废话。 它分别从数据结构、字典集合、文本和字节序列、函数、设计、装饰器、闭包等讲起，然后对每一块知识进行展开，详细介绍里面最根本的原理，然后告诉你，该怎么用好它，高效的使用它。 举一个最简单的例子，在绝大多数教程都会讲到循环和条件语句，千篇一律的告诉你”if..else..”, “for…while”，这个有一点编程语言的同学都知道，但是在Python里面循环和条件语句有什么特殊的地方吗？该怎么用好它？ 《流畅的Python》这本书就教你怎么去使用它，告诉你列表推导该怎么用还有它的意义所在。 这就是这本书的优点：不仅告诉你怎么用Python，而是告诉你怎么用好Python。 3. 《Python CookBook》 学而不精的同学都会认为Python是一门很简单的编程语言，不错，Python相对于Java、C++要简单很多，没有严格的语法结构、没有变量类型，而且如果有一些编程基础去学Python的话可以一个周甚至一天即可学完。 但是我认为，Python入门简单，但是用好并不简单，当你接触到标准的商业项目时你就会意识到Python高级用法的重要性以及它的价值所在。 《Python CookBook》这本书就是这样的一本进阶教材，它不同于大多数教程，反复的介绍基本语法，它直接跳过基本语法开始讲解数据结构、算法、迭代器、生成器、类、对象、元编程等，我认为这些才是工作中真正有价值、拉开差距的地方，而那些基本语法是默认应该会的。 《Python CookBook》会在每个知识点开始提出一个应用场景，然后告诉你怎么去解决这种应用，同时会编程实现，这样对于提升Python是最为实际的，而且让你更加容易理解它这样用的价值所在。 这本书不仅有出版的书籍，也有免费的在线教程，需要可以看一下。 https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十讲：传统目标检测之卷积神经网络概述]]></title>
    <url>%2F2019%2F08%2F03%2Fcnn%2F</url>
    <content type="text"><![CDATA[前言 提起卷积神经网络(CNN)，应该很多人都有所耳闻。自从2012年AlexNet在ImageNet挑战赛一举夺魁，它再一次的回到的人们的视野。 为什么称之为”再一次”，因为CNN并不是近几年的产物，早在20世纪90年代Yann LeCun就提出了最基础的卷积神经网络模型(LeNet)，但是由于算力和数据的限制，它一直处于一种被冷遇的地位，传统目标识别方法，例如之前所讲到的SIFT、HOG、DPM占据着不可撼动的统治地位。 但是随着算力的提升和数据集的积累，这一切都变了，在AlexNet成功之后，CNN如同雨后春笋一样，每年各种各样的Net数不胜数，近其中知名的就有AlexNet、VGG、GoogleNet、UNet、R-CNN、FCN、SSD、YOLO等。 入门计算机视觉领域的绝大多数同学应该都学过或听说过斯坦福大学的公开课(CS231n: Convolutional Neural Networks for Visual Recognition)，主要就围绕CNN进行展开，甚至很多近几年入门计算机视觉的同学就斩钉截铁的认为，计算机视觉就是卷积神经网络，我认为这有一些”一叶障目，不见泰山的”感觉。 CNN只是计算机视觉的一个子集，而且是一个很小的子集，更确切的说，计算机视觉是一种应用性技术，CNN是一种工具。 但是，不可否认，CNN是目前阶段我们能力所达到的、在大多数CV方向应用最为成功的一项技术，尤其是R-CNN系列和YOLO系列，在商业中，例如交通监测、车站安检、人脸识别应用非常多，效果对比于传统目标识别算法也要好很多，所以，它是学习计算机视觉中非常重要的一环，本文就概述一下近年来比较成功的CNN模型。本文只是用简略的语言进行概述，后续会挑选一些比较经典的模型进行详解和编程实现。 卷积神经网络概述 按功能对卷积神经网络进行分类主要可以分为两类， 检测(detection) 分割(segmentation) 检测的目的是要判断一副图像中是否有特定的目标，以及它所在的位置，通过一些手段识别出它所在的包围合区域。 分割的目的要更加严格一些，它不仅要识别出目标的所在区域，还要分割出目标的边缘，尤其在CNN图像分割领域，和传统的图像分割不同，它不能简单的依靠梯度变化幅度把目标分割出来，还需要进行语义上的分割，识别到像素级的类别。 目前比较知名的用于识别的CNN模型有， AlexNet VGG R-CNN系列 Resnet MobileNet YOLO系列 在分割方面比较知名的CNN模型有， Mask R-CNN FCN U-Net SegNet CNN中主要用到的技术 系统学习以上上述所提到的知名CNN模型会发现，其中所使用到的技术手段大同小异，而那些知名度较小的CNN模型更是如此，创新点更是微乎其微，其中所使用到的技术主要有， 卷积 池化 基础块 Dropout 跳跃连接 锚点 优化算法 激活函数 批量正则化 回归 卷积和池化是非常基础的，在特征提取过程中至关重要。 基础块的思想最初出自于VGG，它在AlexNet的基础上进行了很大的改进，基础块思想的引入增加了网络的重用性，后续很多模型都死在这一举出上进行改进的，因此，在很多后续的网络模型都是以VGG为基础模型。 Dropout这个几乎成了CNN模型中必不可少的一个组件，它在应对过拟合问题中具有非常重要的价值。 跳跃连接最初出现在ResNet，在网络的不断改进中发现，其中的思想都是使网络越来越深，网络适当的加深的确能够带来识别精度的提到，但是真的越深越好吗？当然不是。随着网络的加深，很容易出现梯度消失和梯度爆炸现象，ResNet中提出的跳跃连接在后来的网络模型中扮演者非常重要的角色。 锚点这一概念最初是在2008年的DPM模型中看到，后来Faster R-CNN中主要的使用了这项技术，使得它名声大噪，后来的经典模型几乎都用到了锚点这个思想。 优化算法对于上述CNN模型的价值自然不言而喻，梯度下降、Adam、牛顿法等，可以说这是深度计算机视觉的核心所在，也是理论体系最完善、最能够用数学模型解释的一部分。 激活函数和Dropout一样，也是CNN模型中必不可少的一个组件，它的主要价值在于解决模型的线性不可分问题，把非线性的特性引入到网络模型中。 批量正则化也是CNN中常用的一个功能，它的主要作用是加速模型的收敛，避免深层神经网络的梯度消失和梯度爆炸。 回归中用到的较多的自然是softmax，它将经过各种网络层处理得到的特性向量进行回归，得到每一个类别对应的概率，在多分类问题中是一个必不可少的功能。 CNN模型架构 纵观上述所提及的经典CNN模型，它们的模型架构非常相似，主要包含如下几个部分： 输入层 特征提取层 全连接层 回归 输出层 输入层主要是用于读取图像，用于后面的网络层使用。 特征提取层主要通过卷积来获取图像局部的特征，得到图像的特征图。 全连接层用于对特征层进行后处理，然后用于回归层处理。 回归主要通过一些回归函数，例如softmax函数来对前面得到的特征向量进行处理，得到每个类别对应的概率。 输出层用于输出检测和分类的结果。 当然，在这个过程中某些环节会用到上述提到的激活函数、批量正则化、优化算法以及非极大值抑制。 搭建CNN目标识别系统有了上述强大的模型，在实际项目中该怎么搭建一个有价值的CNN目标识别系统呢？我认为主要分为如下几个步骤， 数据获取 数据预处理 模型搭建 数据后处理 在CNN，乃至整个深度学习领域都可以说数据获取是至关重要的一部分，甚至可以说占据了超过50%的地位。深度学习的发展主要就是得益于这么多年来数据的积累，很多项目和工程也是由于数据的限制和却是只能中途作废。因此，数据获取部分是搭建目标识别系统中最重要的一个环节，它直接决定着是否能够继续走下去。 目前有一些公开的数据集可以获取，例如MNIST、Pascal VOC、ImageNet、Kaggle等。如果自己所做的方向恰好巧合，这些公开数据集里有相应的数据，那么的确是幸运的，可以从这些数据中直接获取。 数据预处理对于CNN同样非常重要，各种视频、摄像头在数据采集的过程中很难保证数据是有价值的，或者干净的，这里就需要对数据进行去噪、去模糊、增强分辨率，如果数据集不充足，还需要对数据进行扩充。 模型搭建我认为是这几个环节中相对较为容易的一部分，首先目前这些经典的框架都有开源的项目，有的甚至不止一个版本，我们可以借鉴甚至直接拿来用这些模型。即便不愿意选择开源的项目，也可以使用tensorflow、pytorch进行搭建，其中需要的代码量是非常有限的。 输出检测的结果需要进行非极大值抑制、绘出包围合等后续工作，以及和一些系统进行对接，这样它才是一个可用的完整系统。 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐5款值得安装的Windows工具]]></title>
    <url>%2F2019%2F08%2F01%2Ffive-windows-tools%2F</url>
    <content type="text"><![CDATA[前言电脑，是我们日常学习和办公中主要依赖的工具之一。而windows作为最流行、使用最为广泛的桌面操作系统，与我们的生活有着密不可分的关系。尤其是近两年随着windows10的逐渐完善和丰富，使得windows操作系统更加受欢迎。我个人使用win10已经超过两年，不得不说，这的确是一个不错的操作系统，在此之前会想方设法安装Ubuntu、centos、redhat等操作系统来替代win7和win8，但是随着windows发布和晚上，加入了linux内核，丰富实用的小工具，让我认识到微软在操作系统和软件生态方面的强大。 在使用windows的过程中，仅仅依靠系统自带的功能是无法满足各种各样的工作、学习需求的，因此需要借助一些第三方工具，虽然有很多知名的软件，例如office、Matlab、Photoshop、CAD等，但是这些工具太过于臃肿，不仅占用很大硬盘空间，而且需要付出高额的费用。windows上其实有很多使用、免费，但不失强大的工具，因为是免费开源，所以没有那么过广告和宣传，所以知名度相对较低，本文就介绍5款值得安装的windows工具。 1. Click&amp;Clean 浏览器是我们使用最多的一款工具之一，甚至没有其中的之一。 每天我们花费大量的时间在浏览器上面，访问各种网址，也留下了很多访问的足迹，这就涉及一个问题，除了缓存垃圾之外就是隐私和信息安全。不知不觉中我们把自己的信息展露无疑。 我认为有着Click&amp;Clean这款超强的隐私保护工具就再也不用担心这个问题了。 当浏览器关闭时，这款应用程序删除你的浏览历史,防止他人跟踪你的网上活动，它支持以下诸多功能， 清空缓存 删除 Cookie 清除已保存的密码 浏览器关闭时运行外部应用程序 关闭所有窗口/标签前清理 Delete Web Local Storages Delete Extension Local Storages Delete Web SQL Databases Delete Extension SQL Databases Google Gears 认证数据删除 …… 2. 石墨文档 颠覆传统办公 就如同它的定位那样“颠覆传统办公”，我觉得它做到了。和以往臃肿的office、昂贵的xmind不同，它首先免费，其次它支持多平台同步，windows、mac、手机均支持。此外，它将常用的办公工具融合为一体。 虽然拿它和office做对比，但是它不仅仅是传统意义上的office工具，它还包含如下功能， 文档 表格 幻灯片 思维导图 协作空间 可以说，上述每一项功能都是目前一个完整的商业产品，需要付出高额的服用，而且非常臃肿，但是石墨文档把这些问题都给解决了，不仅使用简单，而且轻量化、见面简洁大方。 3. Squoosh 这是谷歌出品的一款强大的图片压缩工具，比之前较为知名的TinyPng还要强大一些。 我们都知道在我们传输图片，或者在一些平台上传图片时都会有图像大小的限制，例如微信公众号对上传图片就有限制，很多报名系统对上传图片也有限制。 但是我们又不想损失图片质量怎么办？可以尝试一下Squoosh，它采用谷歌强大的算法，在保障图像质量的前提下最大化压缩图片。 严格意义上说，它是一个网页工具，如果觉得用网页方便，可以直接保存网站https://squoosh.app/，到书签即可，如果不喜欢网页工具，没问题，它也支持安装， 打开网站后点击右上角会发现，菜单栏出现”安装Squoosh”的资源，点击安装即可，占用内存非常小。 前面铺垫了很多，效果到底真的那么强大吗？下面来看一下对比图， 把一副1.51MB的原图压缩到104KB，压缩率高达93%，但是视觉上并不是很明显，看上去依然很清晰。 此外，它还支持一些简单的在线编辑。 4. uTools 这可以称得上上“软件中的百宝箱”，内含丰富的插件，通过安装插件能够实现几十种功能，可以说，有了这款软件可以把很多软件卸载给你的系统节省一些空间了，此外，它还支持设置全局快捷键。 它的插件主要包括如下几类： 通用：例如二维码、翻译、待办事项、剪切板、颜色助手、本地搜索等实用的小工具。 图片：包括压缩图片、图床、图片转文字等功能。 开发：包括JSON、正则表达式、http抓包等功能。 这款工具可以适用于不同的人群，满足不同的需求，有了这款工具就不需要人群。 安装插件 插件安装非常简单，只需要点击对应软件-下载即可， 使用 安装之后打开对应的工具即可使用， 5. WGestures我之前其实并不看好鼠标手势，因为尝试过很多手势工具，大多数都是功能花哨，但是使用体验很差，我一度甚至认为鼠标手势就是一个鸡肋，直到我遇到WGestures这款工具，可以说是让人心里感觉豁然开朗，原来鼠标手势可以这么好用，下面简单举几个例子。 基本手势包括复制、剪切、粘贴、前进、后退、添加书签、刷新、Home、End、”摩擦”边缘、触发角等。 复制、粘贴 Web搜索 摩擦边缘 摩擦右边缘弹出任务管理器、摩擦左边缘弹出控制面板 自定义手势 如果WGestures自带的手势无法满足自己的需求，还可以根据自己的偏好添加手势，添加手势类型包括打开文件、窗口控制、命令行、音量控制、执行快捷键等，例如，我自定义一个打开文件的手势， 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>文件查找</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习资源 | 推荐2份Github热门校招面试汇总资料]]></title>
    <url>%2F2019%2F07%2F28%2Finterview%2F</url>
    <content type="text"><![CDATA[前言 秋季招聘一般集中在每年的9月-10月份，有早一些的城市或者公司会在8月就开始进入校园，开启招聘，也有比较晚的少数公司会在11月-12月进行招聘。 一年一度的秋季招聘马上就要开始了，秋季招聘是校园招聘中最为集中、岗位最多的一次集体招聘会，虽然也有春招，但是对于很多公司而言主要是“查漏补缺”，岗位数量相对于秋招有很大的差距。因此，很多即将毕业的学生会把秋季招聘看的非常重要，毕竟第一份工作对一个人来说是至关重要的，每个同学都希望找到称心如意的工作。 但是，“天下没有免费的午餐”，在秋季招聘中这个道理同样适用，好的工作岗位和面试的难度是成正比的，无论是银行、金融，还是互联网、IT。因此，要想找到一份称心如意的工作，必然需要做好充分的准备，毕竟，机会都是留给有准备的人。 面试是否有规可循？答案是肯定的，以阿里、百度、华为这些知名的互联网、IT公司为例，每年招聘的人数有限，但是应聘人数却是招聘人数的几十倍，甚至上千倍。招聘对于毕业生来说是一次煎熬的过程，对于企业同样是一个非常耗时耗力的事情，因此他们会通过一些“落入俗套”的方式进行人才的筛选，虽然这样会错失一部分真正的人才，但是能够筛选掉更多不符合要求的平庸人员，这样的损失是企业愿意接受的。有哪些“落入俗套”的方式呢？无非就是学历、学科、笔试、面试。其实经过一轮简历上学历、学科的筛选已经筛选下去一大批，而通过笔试又会筛选去一大批没有做好充分准备的同学，真正进入最后面试的已经是经过层层筛选留下来的。 学历、学科这些是人为无法改变的，但是笔试、面试却可以，因为成熟、经典的知识体系已经经过多年的洗礼逐渐完善了起来，笔试、面试的内容无非是变着花样的考书本上、教材上的知识，例如数据结构、算法设计等。因此，我认为通过准备，学习掌握目标公司历年来的出题类型和面试方式，会让应聘过程变的顺畅很多。 近期在github上发现两个不错的面试笔记，总结了各大知名互联网、IT公司，例如阿里、百度、腾讯、华为、美团等公司面试中常见的笔试、面试题型，并且给出了详细的解答。我认为每个人都有薄弱的地方，所以，如果心中有目标的公司，可以根据自身的不足之处学习一下对应公司近两年面试、笔试中常见的题型，好好准备，这样能够有效的帮助你在校招中找到称心如意的公司，废话不多说，下面介绍一下这两个开源学习项目。 0voice / interview_internal_reference https://github.com/0voice/interview_internal_reference 这是一个按公司和知识体系分类的的学习资源，目前已经13w+star。 如果心中有明确的目标公司，可以针对性的看一下对应公司的面试总结。它包含阿里、华为、百度、腾讯、美团、头条、滴滴、京东等。此外，还针对企业中比较常用的工程技术进行知识类型的总结，例如MySQL、Redis、MongoDB、Zookeeper、Nginx、算法、内存、磁盘、网络通信、安全、并发等，可以说是涵盖的非常全面。 和往常见到主要针对算法实现的笔试题目不同，这个项目更加偏向面试。我觉得对于有一些编程和数据结构知识的同学通过笔试都不是特别困难的事情，而真正能够在面试官心中留下深刻印象的往往在笔试中，而这些面试官往往是工作多年，深耕业务和产品的工作人员，因此，他们更多的关注的是现实中遇到的问题，所以，如果在这些问题上回答的不错更容易抓住面试官的心。 interview_internal_reference这个项目主要的针对这些问题，从系统稳定性到缓存机制，从并行计算内存优化，非常全面，知识体系也非常分散，我认为这远远要比反复的刷leetcode要有价值的多，尤其是对于腾讯、阿里这些偏工程的公司，在面试过程中会问很多非常分散的工程技术问题。 imhuay/Algorithm_Interview_Notes-Chinese https://github.com/imhuay/Algorithm_Interview_Notes-Chinese 如果说interview_internal_reference偏向于工程技术，那么Algorithm_Interview_Notes-Chinese更多的是围绕着算法进行展开，目前已经25w+star。 在面试过程中，偏开发和偏算法的面试差别非常大，偏开发，例如前端、后端、云服务等工程技术的会询问很多分散技术的问题，但是偏算法的往往会集中在算法和项目方面，例如，做过哪些相关的项目？使用了什么算法？这个算法具体细节是什么？等等。大多数是围绕着应聘岗位进行面试，如果发散一些，会问一些经典的数据结构和数学方面的知识。 这几年随着人工智能火热，计算机视觉、深度学习、自然语言处理方面的工作岗位也多了起来，应聘者也多了起来。这个学习资源主要围绕人工智能领域的技术进行总结，同时涵盖数学、编程、数据结构等方面的基础知识。 如图所示，在深度学习方面，它包含了深度学习的基础知识，例如过拟合、欠拟合、激活函数、反向传播、正则化、加速训练，同时还包含深度学习领域核心的优化算法，例如，梯度下降法、动量法、牛顿法等。 此外，在计算机视觉、自然语言处理方面也包含了诸如VGG、词向量等基础的知识。我认为，它不仅适用于即将应聘的同学，同时对于已经参加工作的同学也非常有用。 除了应用层面的算法之外，它还有数学基础，例如微积分、概率论方面的知识，从求导到极限、从微分到积分，泰勒级数等，样样都有。 123456789101112131415161718192021222324252627class Solution &#123;public: int widthOfBinaryTree(TreeNode* root) &#123; if (root == nullptr) return 0; queue&lt;TreeNode*&gt; Q; Q.push(root); int ans = 1; while(!Q.empty()) &#123; int cur_w = Q.size(); // 当前层的宽度 ans = max(ans, cur_w); for (int i=0; i&lt;cur_w; i++) &#123; auto p = Q.front(); Q.pop(); if (p-&gt;left) Q.push(p-&gt;left); if (p-&gt;right) Q.push(p-&gt;right); &#125; &#125; return ans; &#125;&#125;; 除了这些偏理论的知识，它还有面试中常出现的数据结构和算法方面的总结，同时给出了编程实现，例如，动态规划、双指针、排列组合、二叉树、链表、堆、栈等，此外，Algorithm_Interview_Notes-Chinese吸引人的地方是在这些算法方面不是单纯的给出编程实现，还是列出实现的步骤和思路，非常有助于理解。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第九讲：传统目标检测之DPM模型]]></title>
    <url>%2F2019%2F07%2F28%2Fcv-dpm%2F</url>
    <content type="text"><![CDATA[前言 DPM(Deformable Part Model)模型，又称为可变型部件模型，是Felzenszwalb于2008年提出的一个模型。这可以说是传统目标识别算法中最为经典的算法之一，我认为对计算机视觉有一些深入了解的同学应该对DPM模型都有所耳闻。 首先说一下DPM模型这篇文章有多牛。DPM模型的坐着Felzenszwalb凭借这个模型一举获得2010年voc挑战赛的终身成就奖，感觉还是不够牛？不知道Felzenszwalb是何许人也？Felzenszwalb正是Ross B. Girshick(也就是DPM模型的第二作者)硕士和博士期间的导师。我想，如果连Ross B. Girshick都不知道的话就真的称不上是一个计算机视觉领域的学习者了。它正是R-CNN系列、YOLO系列等现如今被封为经典的计算机视觉模型的提出者或共同提出者，可以说是这几年计算机视觉领域比较有作为的一位研究者。 说完DPM的作者很牛，那和DPM有什么关系？前面提到，它的作者是近几年计算机视觉领域非常知名的研究者，因此，自然而然，这几年比较成功的计算机视觉模型都会受到这个标杆性算法的影响。多尺度、锚点、可变型部件，都对后面深度学习计算机视觉带了巨大的影响。 介绍完DPM模型的背景，再回到这个算法本身。DPM模型和前文讲到的HOG整体流程非常类似，HOG采用HOG特征加linear SVM，而DPM采用多尺度特征加latent SVM，此外，DPM在特征提取方面也是在HOG特征的基础上进行稍加改进。虽然从文中看上去两者差别并不大，但是其实DPM无论是在特征提取层面还是在机器学习层面都做了巨大的改进。 首先是特征提取思想，HOG模型仅仅考虑根模型的特征，不考虑部件模型的特征，而DPM模型采用根模型加部件模型的思路，同时考虑外观和细节部分的特征。 其次是SVM方面，Latent SVM加入了潜在信息的训练。 下面就分别从特征提取到模型训练介绍一下这个模型。 特征提取 文章中讲的有点让新学者难以理解，这里我就对照着HOG特征讲解一下，更有助于理解。 两者相同的是第一步都要先计算梯度方向，然后对梯度方向进行统计。 不同之处是，HOG特征含有块(block)的概念，它首先把一副图像划分成若干个块，然后再把块划分成若干个单元，然后对单元内部的像素进行梯度统计，然后对同一个块内的特征向量进行归一化，HOG采用的是0~180度之间的梯度方向，20度一个区间，这样每个细胞单元就统计得到一个9维特征向量，一个块内就得到n * 9维特征向量。 由于HOG采用的梯度方向为0~180度方向不敏感特征，这样会丢失很多特征信息，DPM模型对HOG做了很大的改进。首先DPM模型没有快的概念，它是去一个细胞单元四角对应的领进单元的特征进行归一化，此外，更重要的是DPM不仅提取结合0~180度方向不敏感特征和0~360度方向敏感特征两种特征，它首先提取0~180度之间的特征，得到上图所示4*9维的特征，拼接起来得到13维特征向量，然后再提取0~360度之间的特征，得到18维特征向量，二者相加得到31维特征向量。 模型训练前面介绍了一下DPM模型特征提取的方法，虽然思想与HOG有很大不同之处，但是在最基本的梯度方向统计方面是相同的。 知道了如何从一副图像中提取我们想要的特征，要进一步深入理解一个算法，我认为从模型训练、模型预测方面是最简单明了的方法，无论是传统目标识别还是深度计算机视觉。知道它是如何训练、如何预测的就知道这个模型的运作情况，输入是什么？中间经历了什么过程？输出是什么？下面就来看一下DPM模型的训练过程。 本算法采用的训练说句来自于Pascal VOC，用过这个数据集的都知道，它只标记了图片中目标的包围合，并没有标记图像的部件，例如它只标记了一个人，并没有标记人的胳膊、腿、头部等，而DPM被称为可变型部件模型，那么部件体现在哪里？怎么知道它的部件在哪？下面来了解一下它的训练过程，能够帮助理解这个算法。 DPM的在训练之前先进性了初始化，主要包括3个阶段： 初始化根滤波器 为了训练一个有m个组件的混合模型，首先将正样本按照长宽比划分成m组，然后针对每一组训练一个根滤波器F1、F2、…、Fm，在训练根模型过程中使用的是标准的SVM， 不含有潜在信息，例如上图(a)、(b)就是初始化的两个根模型。 合并组件 把初始化的根滤波器合并到一个没有部件的混合模型中并且重新训练参数，在这个过程中，组件的标签和根的位置是潜在变量(组件和部件不是同一个概念)。 初始化部件滤波器 前面提到，数据集中并没有标记部件的位置，因此文中在初始化部件滤波器是用了一个简单的假设，将每个组件的部件数量固定在6个，并使用一个矩形部件形状的小池，文中贪婪地放置部件，以覆盖根过滤器得分较高的区域。 另外需要清楚的是，部件滤波器是在根据滤波器2倍分辨率的图像上进行初始化，因为分辨率越高，细节越清晰，越能提取部件的特征。 经过初始化之后就可以训练模型参数。 下面是详细的训练过程， 模型检测前面介绍了DPM模型的特征提取和训练过程，下面就来看一下模型检测过程。 上述就是就是DPM模型检测的详细过程： 对输入图像进行特征提取，得到特征图和2倍分辨率的特征图 分别在特征图和2倍分辨率上计算根滤波器和部件滤波器的得分 合并根位置的得分，得到总得分 用数学语言表示，图像的总得分为， \begin{array}{l}{\operatorname{score}\left(x_{0}, y_{0}, l_{0}\right)=} {\quad R_{0, l_{0}}\left(x_{0}, y_{0}\right)+\sum_{i=1}^{n} D_{i, l_{0}-\lambda}\left(2\left(x_{0}, y_{0}\right)+v_{i}\right)+b}\end{array}模型检测过程就是获取局部最大响应(得分)的过程，前面已经训练得到了模型参数，然后利用模型参数在图像特征图上滑动求点积，计算得分。DPM的得分包括两个方面：$R_{0, l_{0}}\left(x_{0}, y_{0}\right)$是根滤波器的得分， $\sum_{i=1}^{n} D_{i, l_{0}-\lambda}\left(2\left(x_{0}, y_{0}\right)+v_{i}\right)$是部件滤波器的得分，$b$是偏移量。 Latent SVM在经典的SVM中，认为训练样本的标记是严格符合类别标签的，标记的正样本就是正样本、标记负样本就是负样本，但是由于标记过程中有很多人为因素，因此，虽然能保证负样本一定是负的，但是却不能保证正样本一定属于正的。因此在训练过程中有很多潜在的未知信息，作者发现，将根位置作为一个潜在变量，可以有效地补偿正样本中存在噪声的边界框标签。 Latent SVM训练的目标函数为， L_{D}(\beta)=\frac{1}{2}\|\beta\|^{2}+C \sum_{i=1}^{n} \max \left(0,1-y_{i} f_{\beta}\left(x_{i}\right)\right)其中， f_{\beta}(x)=\max _{z \in Z(x)} \beta \cdot \Phi(x, z), $z$是潜在信息。 源码解析由于DPM模型工程量较大，而且作者已经开源代码并且经过多个版本的迭代，目前非常成熟，因此不在这里逐步实现，在这里主要讲解一下怎么使用源码去检测目标和训练模型。 目前源码版本为 voc-release5，可以直接访问官网下载， http://www.rossgirshick.info/latent/ 也可以关注公众号回复voc获取。 DPM的源码是由Matlab和C++进行混编而成，Matlab主要用于做一些简单的图像处理，由于在模型训练和特征提取过程中非常缓慢，因此，为了提高效率，作者用C++实现了特征提取和模型训练部分，另外，由于C++部分使用了一些多线程的库，所以在windows下无法直接运行，需要做一些修改，在linux和mac下可以直接运行。 目标检测 用训练好的模型检测目标，主要有如下几个步骤， 解压缩代码。 运行Matlab。 运行’compile’函数来编译helper函数。 加载模型和图像。 检测目标。 示例， 1234&gt;&gt; load VOC2007/car_final.mat; &gt;&gt; im = imread('000034.jpg'); &gt;&gt; bbox = process(im, model, -0.5); &gt;&gt; showboxes(im, bbox); 训练模型 可以自己按照voc的格式准备数据，训练自己的模型，去检测相应的目标，详细过程如下， 下载数据集和VOC devkit工具包。 根据自己的数据配置voc_config.m。 运行’compile’函数来编译helper函数。 利用pascal.m脚本训练模型 示例， 1&gt;&gt; pascal('bicycle', 3); 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 推荐10款浏览器插件]]></title>
    <url>%2F2019%2F07%2F24%2Fchrome-plugin%2F</url>
    <content type="text"><![CDATA[前言前一篇文章花大篇幅详细的介绍了一款强大的浏览器插件—tampermonkey，评论里很多同学对这款插件给予很高的评价。可以看出，浏览器插件在日常工作和生活中的确占据着很重要的地位，因此，本文整理推荐10款浏览器插件，每一款都是经过长时间使用并且最终保留下来的，插件主要是一些效率工具，希望能够对各位提供有效的帮助。 Infinity 这款插件自称是一款新标签页工具，但是我觉得，如果仅仅把它视为新标签页工具，那就把它想的太简单了。 它提供极简的标签页设计，而且含有丰富的标签页壁纸，轻轻点击即可切换新标签壁纸，但是它的功能远不止于此，它还包含如下丰富功能： 待办事项 实用笔记 精美天气 轻松切换搜索引擎 …… Forest 保持专注，用心生活，这是这款工具的宗旨。 这是一款习惯养成插件，就如同前面所说，浏览器是我们工作和学习中常用的一款工具，但是如果把过多的时间花费在浏览器上，那么就是一种巨大的时间浪费。Forest利用一种轻松有趣的方式让你远离网络成瘾。 在每次开始工作时，可以种下一颗树苗，它会慢慢长大。如果你频繁的打开浏览器，它会用各种方式提醒你应该专注工作。此外，你还可以把一些网站加入黑名单，如果在专注时间内访问这些网址则会被禁止，如果强制访问，那么辛辛苦苦种下的小树苗则会枯萎，利用这种游戏的方式让你更加专注于学习与工作。 LastPass 这是一款强大的密码管理工具。 互联网的时代，让我们困扰的就是频繁的注册、数不清的账号和密码，我们不断的在重复着设置密码、重置密码。怎么样才能解决这种困扰？可以尝试一下LastPass，它采用256位AES密匙的强大加密算法，首先保证了在本机上不获取得到您的信息，其次，每当访问对应的网站时，它能够快速提示你对应的密码并填充，这样就避免了重复记忆密码的困扰。 当然，它不仅仅包含密码管理，还包含安全笔记这个实用的功能。 OneTab 在浏览网站时我们会发现，不知不觉中打开了很多网页，这时候标签栏变的非常密集而混乱。 这时候该怎么办？逐个关闭不仅麻烦，而且如果后续用到的话又找不到了。如果不关闭吧，又影响了浏览器的使用体验。 有了OneTab这款工具，只需单击一下，就可以把所有标签页转化成一个列表，如果再次需要某个网页的时候，可以单个或者全部恢复标签页。此外，它还节省高达95%的内存占用。 Pocket 一款轻松捕获视频、文章等内容的快捷插件。 当我们看到一个视频或者文章时，由于种种原因无法当时去看，希望以后某个时段有了时间再去看，有了Pocket就使得这件事情变得简单起来，只需要点击一下Pocket图标或者鼠标右键保存即可轻松把内容保存到Pocket里面，而且Pocket还支持多平台、多终端，在浏览器上保存后再手机上也可以查看。 Grammar and Spell Checker 从名字就可以知道这款工具的功能—语法和拼写检查工具。 它能够在网站上任何位置对你输入的段落进行拼写和语法检查，它的强大之处主要有如下2点： 支持超过25种语言 适用于几乎所有的网站 FireShot 这是一款截图工具。 截图，是我们常用的一个功能，windows自带的截图功能自然不用多说，真的挺差的，借助QQ、微信等截图又比较麻烦，当然，可以借助Snipaste等强大的截图工具。支持截图的工具有很多，但是大多数只能截取可见部分，支持截取完整页面的却很少，FireShot就可以做到这一点，此外，它还支持： 截图保存到磁盘为PDF，PNG和JPEG 截图复制到剪贴板 打印截图 crxMouse 鼠标手势依赖者的福音。 它可以跨windows、linux、mac多平台支持自定义鼠标手势，同时支持超级拖拽、平滑滚动、摇杆手势。此外，还支持导入和导出配置文件。 它默认携带手势包括前进、后退、向上滚动、关闭标签页、到底部、刷新等，如果不满足于这些手势，还可以自定义添加。 Dark Reader 这是一款主题插件，现在有些手机和软件都实现了夜间模式，因为黑色的主题对于保护眼睛更有好处，能够减少明亮色彩带来的眼睛疲劳，Dark Reader就可以实现浏览器的夜间模式，同时它具有更强的定制性，能够调整亮度，对比度，应用棕褐色滤镜，黑暗模式，设置字体和忽略的网站列表。 SwitchyOmega 这是一款可以称得上“神器”的插件，轻松快捷的管理代理，同时支持自动切换多个代理模式。 在很多场景下，尤其是在有些公司内部对于信息安全的限制，需要设置代理才能访问不同类型的网站。如果通过IE代理设置，这样需要每次打开IE、连接设置…一系列流程，非常繁琐。而且针对不同的网络下只能使用同一种代理模式。SwitchyOmega能够让你轻松的切换不同的代理模式，而且支持自动代理切换，当访问不同网络时能够选取最快的代理方式，这样就不会出现有的网址可以访问，有的网址访问速度缓慢甚至打不开的现象。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>文件查找</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 也许，这是最强大的一款浏览器插件]]></title>
    <url>%2F2019%2F07%2F14%2Ftampermonkey%2F</url>
    <content type="text"><![CDATA[前言 浏览器是我们日常工作中接触最多的工具之一，甚至在很多人的排行榜里毫无争议的夺得第一的位置。目前市面上浏览器可谓是五花八门，谷歌浏览器、IE浏览器、火狐浏览器、QQ浏览器、搜狗浏览器、360浏览器等等，但是归根结底，使用的内核主要分为两类：Chromium内核和Trident内核(又称IE内核)。由于浏览器在工作中扮演者至关重要的作用，使用比重也非常之大，因此，简单的官网默认浏览器很难满足我们各种各样的需求，所以，浏览器插件也就应用而生。甚至，对于很多浏览器来说，它的最大特色和吸引人的地方就是丰富而实用的插件。 如果让选出几款不错的插件推荐给大家，不同的使用者应该会推荐不同的插件，毕竟每个人的使用偏好和工作内容不同。但是我相信，对于大多数推荐者都不会忽略一个插件，也就是本文的主角：Tampermonkey。可以毫不谦虚的说，Tampermonkey是目前最为流行的用户脚本管理器，它适用于 Chrome, Microsoft Edge, Safari, Opera Next, 和 Firefox。用户脚本是一个什么东西？简而言之，不同脚本可以实现不同的功能，Tampermonkey可以对这些功能进行管理，让你的浏览器如虎添翼。 由于我个人日常使用谷歌浏览器较多，因此在这里就以谷歌浏览器为例为大家推荐几款不错的插件，每一款都让人赞不绝口。 概述Tampermonkey有很多可选的脚本，但是如果让推荐的话，我认为以下5款是必不可少的： AC-baidu Yet Another Weibo Filter 百度网盘直链下载助手 豆瓣资源下载大师 破解VIP会员视频集合 下面就逐个详细介绍一下上述5款插件，耐心往后面看，一个比一个强大。 AC-baidu 提及百度搜索，应该很多人想到的就是广告、混乱，的确，经常使用谷歌搜索，每当回到百度搜索时都会克制不住的质疑：“为什么会存在百度搜索这样的东西？” 的确，广告、相关推荐、垃圾信息，样样都有，就是没有我们想要的东西。 有了AC-baidu这个脚本，上述困扰就迎刃而解了。 它能够让你的搜索重定向到原始网页，拦截百家号等无用推广，让搜索网页回到最原始、最本质的样子。同时，每个搜索条后面都会有一个block字样，如果觉得对某些网站或者搜索条不满意，可以点击一些block就可以在以后的搜索中屏蔽这些搜索条。 Yet Another Weibo Filter 微博，是我们日常接触到较多的社交工具，甚至很多人每天都会反复多次刷微博。如果你喜欢用电脑浏览器刷微博应该清楚，它的页面信息十分混乱，多而杂，热门视频、特别关注、微博电影榜等等。我们唯一想看的就是微博，但是在浏览过程中却不得不被这些混乱的信息所干扰。有了Yet Another Weibo Filter就不用为此烦恼了，让你看真正想看的微博。 安装Yet Another Weibo Filter脚本之后打开微博会发现，右上角会出现一个漏斗状的一个图标，点击图标会打开上述界面，我们可以对微博进行内容、账号、话题、来源等进行过滤和设置，而且可以对版面进行清理，功能进行改造，而且还可以通过外观样式来修改字体、字号等内容，看看下面这幅图，经过版面清理之后是不是很整洁？ 百度网盘直链下载助手百度网盘是资源共享使用较多的一个工具，因此很多同学会通过各种网盘搜索工具寻找百度网盘的资源。但是资源找到了，会发现一个令人头疼的问题，文件太大无法直接下载，必须保存到个人网盘、打开PC客户端才可以下载。而打开客户端下载又被百度限速，非常痛苦，百度网盘直链下载助手就能够轻松解决这个问题。 安装百度网盘直链下载助手这个脚本之后会发现，浏览器打开百度网盘时上端会出现一个下载助手的选项卡，点击后会弹出两个选项：API下载、外链下载。这样的话可以直接点击调用IDM等下载工具进行下载，也可以复制下载链接，粘贴到一些下载工具后下载。 豆瓣资源下载大师 喜欢影视、音乐、图书的同学对豆瓣应该都不陌生，有大量的影视评论、书评。很多人会想，豆瓣上书籍、影视、音乐倒是不少，但是只能看看评论、评分，又有什么意义呢？豆瓣资源下载大师就让这个网站变的有了意义，把一个单纯的论坛和资源紧密的联系了一起。 安装豆瓣资源下载大师脚本之后，打开要找的电影、电视剧、图书等，会在右端状态栏很多匹配的资源列表，当然也包括下载的链接，下面就通过一个动画来演示下载《流畅的Python》这本书籍。 点击匹配的对应资源即可找到下载链接。 破解VIP会员视频集合 看电影、追剧，是很多同学闲暇之余最大的乐趣之一。但是发现我们要看的影视分布在优酷、腾讯、爱奇艺等平台，如果要买吧，太耗钱，不买吧，又要忍受冗长的广告。狠下心买了之后发现，很多电影需要观影券，这时候都会愤恨的说一句”与其这样，我还买你们会员干什么？“ 既然买了会员还不行，那么只有通过暴力方法来解决这个问题。关于视频破解工具，网上可谓是层出不穷，但是经过我的试用发现，真的不敢恭维，绝大多数都是不稳定或者压根不能用，而剩余个别能用的在打开时又非常缓慢，卡顿，直到遇到破解VIP会员视频集合这个脚本，发现真的令人惊讶，怎么可以有这么强大的神器？ 安装脚本之后会发现，打开视频后会在左边缘出现一个黄色箭头，点击这个箭头之后会弹出多个资源选项，点击其中一个会对我们看的视频进行解析，能够跳过广告、破解会员、观影券限制，更重要的是，它还很快，下面就来演示一下。 重点！！！亲测优酷、爱奇艺、腾讯视频均可用！ 脚本安装方法安装方式有两种： Chrome网上应用店 离线安装 Chrome网上应用店 如果能够访问Chrome网上应用店，我建议通过应用商店安装，便捷、安全。只需打开应用商店，搜索Tampermonkey，添加至Chrome即可。 离线安装 如果无法访问应用商店，则只能通过离线下载crx格式插件，然后点击右上角—更多工具—扩展程序，把crx格式插件拖动到空白处即可， tampermonkey安装之后点击图标，选择获取新脚本， 然后点击GreasyFork， 然后搜索、点击对应的的脚本安装即可， 插件下载如果无法访问Chrome网上应用店，则只能通过离线下载安装的方式，网上有很多Tampermonkey的资源，但是大多数很混乱，为了避免寻找的麻烦，我把插件进行共享了，需要的可以关注公众号，回复关键字”tmk“获取。]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习资源 | 如何学习优化算法？]]></title>
    <url>%2F2019%2F07%2F13%2Flearn-optimization%2F</url>
    <content type="text"><![CDATA[前言 在学习机器学习的一段时间之后，如果善于总结都会发现，无论是传统机器学习中比较经典的支持向量机，还是深度学习，无论是目前应用较广的计算机视觉，还是让AlphaGo大发神威的强化学习，最终都会涉及一个优化问题，或者是微积分系列的凸优化算法，或者是粒子群、蚁群等群体智能优化算法，或者是近几年比较热门的黑盒优化算法。尤其是近两年在工业控制方面契合度较高的强化学习，仔细分析它的模型，其实就是一个数学优化模型。最优化问题在当今的军事、工程、管理等领域有着极其广泛的应用。因此，优化算法的重要性可见一斑。理解优化算法，能够有助于理解深度学习的运作过程，对于模型的优化和改进也大有益处。本文就概括一下常用的优化算法并介绍一些学习资源。 优化算法概括 我个人对优化算法划分主要为3类，分别是： 凸优化 智能优化 黑盒优化 其中目前用的比较广泛的，尤其是在机器学习领域就是凸优化，例如梯度优化算法系列的梯度下降法、随即梯度下降、小批量梯度下降法、动量法momentum、Adagrad、RMSProp、Adadelta、Adam等，它们都是以梯度下降法为基础，在梯度下降法的基础上进行改进和优化。除了常用的这些还有牛顿法系列，以及无约束优化算法中的模式搜索法、Rosenbrock方法、单纯形搜索法、Powell方法。 凸优化虽然很成熟，但是很多工程问题并非是严格的符合凸优化的要求，换句话说，它是一个非凸优化问题，这样直接利用前面提到的这些算法很容易陷入局部最小值。因此，为了满足工程需求，研究者会根据问题的需求提出一些新颖的优化算法，其中就包括目前在工程应用领域比较热门的群体智能优化算法系列，例如，粒子群优化、模拟退火法、遗传算法，它们以独特而适应性强的有点在工程应用领域倍受欢迎，尤其是在复杂数学模型求解问题中能够更快速的求解同时避免陷入局部最优。 黑盒优化算法我最初是在谷歌开放的内部调参系统Google Vizier介绍论文Google Vizier: A Service for Black-Box Optimization提到的。在前面的优化算法中，优化问题都是建立在一个完整的数学模型基础之上，但是现实世界中很多场景是很难用数学模型来描述，或者没有数学模型，例如我们经常接触到的交通系统。在这种问题求解过程中，上述严格依赖数学模型的优化算法就显得有些捉襟见肘。谷歌在2017年在开放内部调参系统的介绍论文中详细介绍了它们用于调参的几种优化算法，其中包括如下几种算法： 贝叶斯优化 进化策略 SMAC 随机搜索 并在文中详细的对比了几种黑盒优化算法的效果。 下面分别针对这3类优化算法介绍一些学习资料。 凸优化算法 凸优化算法在目前机器学习中用的较多，其中分别有： 梯度下降法 动量法 Adam RMSProp Adagrad … 感兴趣的可以看我的另一篇文章，里面对机器学习中常用的优化算法推导过程及不同算法之间的关系进行了详细的阐述：一文了解人工智能中常用的优化算法 由于凸优化发展时间较长，而且理论体系比较完善，因此在微积分、数值计算等课程中都会涉及一部分，但是分布比较零散，不同于目前机器学习系列的课程，针对性较强，而且内容专一。虽然课程方面没有针对纯粹优化算法的，但是书籍方面却有很多，在这里我推荐两本不错的凸优化算法的书籍， 《最优化理论与方法》—袁亚湘，孙文瑜 作者袁亚湘为中国科学院院士、数学家，在计算数学、运筹学、应用数学领域有较深入的研究。曾有幸听过袁亚湘院士到学校开的优化算法专题讲座，真可谓是”听君一席话，胜读十年书”，于是就购买了袁亚湘院士的这本书籍。语言生动而易懂，系统地介绍了无约束量优化，约束优化和非光滑量优化的理论和计算方法，内容全面而丰富。 《最优化理论与算法（第2版）》—陈宝林 本书是陈宝林教授在多年的授课基础之上编著而成，与袁亚湘院士的书籍目录划分结构不同，但是我认为这种内容分层更有助于初学者的学习，他分别把优化算法划分成单纯形方法、对偶理论、灵敏度分析、运输问题、内点算法、非线性规划KT条件、无约束优化方法、约束优化方法、整数规划和动态规划等内容。 智能优化算法 智能优化算法的发展历史相对而言要短一些，但是由于都是在工程应用领域遇到瓶颈是应运而生，因此它的实用价值和效果更加让它们受欢迎，目前比较经典的智能优化算法有， 遗传算法 禁忌搜索 模拟退火法 蚁群算法 粒子群优化算法 由于智能优化算法更多是应工程应用需求而生，因此在数学模型方面并没有太多改进，因此在通识教育的数学课程中也很少涉及，同时，相关的书籍较少，在这里我就推荐一本智能优化算法的书籍。 《智能优化方法》—汪定伟 之所以推荐这本书，更多的是因为它的全面，它几乎囊括了目前所有主流的智能优化算法，其中当然就有遗传算法、蚁群算法、粒子群算法等。书中讨论这些算法的产生和发展、算法的基本思想和理论、基本构成、计算步骤和主要的变形以及数值例子和实际应用，对于学习者非常友好。 黑盒优化算法 就如同前文所讲，黑盒优化算法我最初实在2017年谷歌开放内部调参系统的介绍论文中看到的，它详细的介绍了内部调参系统Google Vizier使用的几种主流黑盒优化算法。之所以称之为黑盒，就是因为在这类优化问题中我们没有数学模型，我们不清楚优化的目标函数到底什么样。这种场景和我们日常所接触的现实场景更加贴近，因此它的实用价值自然不言而喻。 在谷歌的这篇文章中，不仅介绍了系统内部使用的黑盒优化算法，还在不同维度求解问题下对比了以下几种优化算法的效果： 随机搜索 贝叶斯 SMAC 进化策略 概率搜索 虽然谷歌的文章发表于2017年，但是里面提及的算法并不算新颖，其中的算法都是经过几年甚至几十年的不断改进而形成现如今的样子，所以要想详细学习需要看一下Google Vizier: A Service for Black-Box Optimization这篇提及的参考文献，比较零散。虽然这些成熟算法的理论体系比较零散，但是它们共同用到的理论知识却是成体系的，它们都用到了概率论\随机过程相关的知识，尤其是其中表现较好的贝叶斯优化和进化策略，都是建立在高斯过程的基础之上，因此，本文就推荐1本随机过程方面的书籍，对这些概率论\随机过程的基础知识有所了解更加有助于对这些成形算法的理解。 《随机过程（原书第2版）》—Sheldon M.Ross 本书由世界著名的应用概率专家和统计学家Sheldon M. Ross编著，本书介绍了从概率论基础概念，到各种常见的分布模型。详细的介绍随机过程中经典的知识，包括Poisson过程、Markov链、鞅、Brown运动、随机序关系、Poisson逼近，并详细的介绍了这些理论的应用，更加有助于理解和学习。]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 推荐两款提高windows下工作效率的工具]]></title>
    <url>%2F2019%2F07%2F13%2Ftwo-windows-tools%2F</url>
    <content type="text"><![CDATA[前言好的工具能让做事效率事半功倍，学习和工作都是这样。不同专业方向都会有一些很知名、耳熟能详的工具，例如开发方面的visual studio、pycharm，办公方面的office、xmind。这些软件的确很强大，但是再强大的工具都很难做到面面俱到，把效率考虑的十分周全。而有一些高手就及时发现里面的不足之处并开发出一些强大高效的工具，能够让日常工作效率大大提升，本文要介绍的两款工具就是这样的：冷门而强大，它们分别是：DropletIt和Quicker，下面就来看一下这两款工具究竟强大在哪里。 DropIt 工作中，日积月累会积攒很多各种各样的文件，有word、Excel、powerpoint、pdf等文档，有png、jpg等图片，有zip、tgz、7z等压缩包，尤其是很多同学都有个习惯，为了方便会放在桌面上，当想要找自己需要的东西时如同大海捞针一样，不知道从何下手。我想这是困扰很多人的问题，删除—积累，不断的重复，但是始终没有找到一个高效的解决文件分类方法。我想说，有了DropIt，再也不用担心文件分类与管理了，真正的实现了文件一建整理，下面来介绍一下DropIt的使用。 下载安装 下载安装之后桌面会有这样一个图标， 添加协议 为了满足我们整理文件的偏好和需求，需要对DropIt设置一下协议，让它按照我们预先设定的协议整理，添加协议主要包括4个部分： 名称：添加协议的名称，按照自己的爱好随便命名即可。 规则：匹配文件的规则，按照我们需要整理的文件设置匹配规则，例如\.png*匹配以png结尾的文件，如果包含多个规则可以用;隔开。 操作：对我们规则匹配到的文件采用的操作，其中包括移动、删除、压缩等。 目标文件夹：对文件处理的目标文件夹，例如，移动规则匹配到的文件到目标文件夹。 例如，上述我个人设置的两个协议，分别对图片(bmp、gif、jpg)和压缩包(zip、7z)进行处理，将图片和压缩包分别移动到指定的文件夹内。 设置好协议之后只需要选中文件，拖动到DropIt图标上方即可，下面来看看效果， 这样，选中的文件会按照我们预先设定的协议分别移动到对应的文件夹内，就不用我们逐个选中文件然后剪切、粘贴到指定文件夹。 Quicker 之前介绍过两款高效的办公工具：Listary和Wox。如果说Listary和Wox是键盘增强工具，那么Quicker就是一款强大的鼠标增强工具，能够让对鼠标比较依赖的同学发现，原来鼠标可以做这么多事情。 我们都知道，大多数鼠标包含3个按键，分别是：左键、右键、中键。其中左键和右键日常工作中使用较为频繁，但是中键除了上下翻页之外很少使用。Quicker就合理的利用了这一点，为鼠标中键添加上一个强大的快捷面板。软件默认的快捷面板包含我们常用的记事本、计算器、截图、我的电脑、Excel等工具，能够避免再去开始菜单寻找这些小工具，能够大大提高效率。 如果觉得软件自带的动作不足以满足自己的使用需求，那么还可以添加其他的动作，添加方式包括两种： 添加他人分享的动作 自己设计动作 添加他人分享的动作 首先打开官网，https://getquicker.net/Share 会发现官网动作库包含很多别人分享的动作，其中不乏查询搜索、翻译、文本处理、编程相关，可以根据自己的需求搜索对应的动作，然后复制动作，中键打开快捷面板，点击鼠标右键，选择粘贴分享的动作，然后安装即可，下面来演示一下安装过程， 如果在分享的动作库找不到自己需要的，或者感觉别人分享的无法满足自己的需求，没问题，还可以选择自己设计动作，Quicker提供两种动作创建方式： 基础动作 组合动作 基础动作 基础动作主要包含打开网址、发送文本、模拟按键等，创建方法很简单：点击鼠标中键打开快捷面板，点击+号，创建基础动作，选择动作类型，录制即可，当然也可以选择快捷的方式录入打开网址等动作。 组合动作 如果觉得基础动作太单一，还不够便捷，没问题，我认为Quicker最强大的地方就是支持组合动作。创建组合动作相对而言也要复杂一些，我认为它像是一种高阶的编程语言，可以用条件语句、添加变量名来实现一连串的动作，这个相对复杂而且不够大众化，因此在这里不多阐述，如果喜欢折腾的可以查看官网教程，创建一些高级的动作来提高自己的效率。 https://www.yuque.com/quicker/help/xaction-editor]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第八讲：传统目标检测之HOG特征]]></title>
    <url>%2F2019%2F07%2F10%2Fcv-hog%2F</url>
    <content type="text"><![CDATA[前言 如果自称为计算机视觉工程师，没有听说过前文提到的尺度不变特征变换(SIFT)，可以理解，但是如果没有听说过方向梯度直方图(Histogram of oriented gradient，HOG)，就有一些令人诧异了。这项技术是有发过国家计算机技术和控制研究所(INRIA)的两位研究院Navneet Dalal和Bill Triggs在2005年CVPR上首先发表提出(那时的CVPR含金量还是很高的)。原文Histograms of oriented gradients for human detection截止2019年7月10日引用率已经达到26856。 HOG通过计算局部图像提取的方向信息统计值来统计图像的梯度特征，它跟EOH、SIFT及shape contexts有诸多相似之处，但是它有明显的不同之处：HOG特征描述子是在一个网格秘籍、大小统一的细胞单元上进行计算，而且为了提高性能，它还采用了局部对比度归一化思想。它的出现，使得目标检测技术在静态图像的人物检测、车辆检测等方向得到大量应用。 在传统目标检测中，HOG可以称得上是经典中的经典，它的HOG+SVM+归一化思想对后面的研究产生深远的影响，包括后面要讲到的神作DPM，可以说，HOG的出现，奠定了2005之后的传统目标检测的基调和方向，下面就来了解一下这个经典之作。 方向梯度直方图 HOG特征的算法可以用一下几个部分概括， 梯度计算 单元划分 区块选择 区间归一化 SVM分类器 下面分别来详细阐述一下。 梯度计算由于后面要进行归一化处理，因此在HOG中不需要像其他算法那样需要进行预处理，因此，第一步就成了梯度计算。为什么选择梯度特征？因为在目标边缘处灰度变化较大，因此，在边缘处灰度的梯度就较为明显，所以，梯度能够更好的表征目标的特征。 我们都知道在数学中计算梯度需要进行微分求导，但是数字图像是离散的，因此无法直接求导，可以利用一阶差分代替微分求离散图像的梯度大小和梯度方向，计算得到水平方向和垂直方向的梯度分别是， G_{h}(x, y)=f(x+1, y)-f(x-1, y),\forall x, yG_{v}(x, y)=f(x, y+1)-f(x, y-1) ,\forall x, y其中$f(x,y)$表示图像在$(x,y)$的像素值1。 可以得到梯度值(梯度强度)和梯度方向分别为, M(x, y)=\sqrt{G_{h}(x, y)^{2}+G_{v}(x, y)^{2}}\theta(x, y)=\arctan \left(G_{h}(x, y) / G_{v}(x, y)\right.单元划分 计算得到梯度的幅值和梯度方向之后，紧接着就是要建立分块直方图，得到图像的梯度大小和梯度方向后根据梯度方向对图像进行投影统计，首先将图像划分成若干个块(Block)，每个块又由若干个细胞单元(cell)组成，细胞单元由更小的单位像素(Pixel)组成，然后在每个细胞单元中对内部的所有像素的梯度方向进行统计。Dalal和Triggs通过测试验证得出，把方向分为9个通道效果最好，因此将180度划分成9个区间，每个区间为20度，如果像素落在某个区间，就将该像素的直方图累加在该区间对应的直方图上面，例如，如果像素的梯度方向在0~20度之间，则在0~20对应的直方图上累加该像素对应的梯度幅值。这样最终每个细胞单元就会得到一个9维的特征向量，特征向量每一维对应的值是累加的梯度幅值。 区块选择为了应对光照和形变，梯度需要在局部进行归一化。这个局部的区块该怎么选择？常用的有两种，分别是矩形区块(R-HOG)和圆形区块(C-HOG)，前面提供的例子就是矩形区块，一个矩形区块由三个参数表示：每个区块由多少放歌、每个方格有多少像素、每个像素有多少通道。前面已经提到，经过作者验证，每个像素选择9个通道效果最佳。同样，作者对每个方格采用的像素数也进行验证，经过验证每个方格采用3*3或者6*6个像素效果较好。 区间归一化每个方格内对像素梯度方向进行统计可以得出一个特征向量，一个区块内有多个方格，也就有多个特征向量，例如前面的示例区块Block内就有4个9维向量。这一步要做的就是对这4个向量进行归一化，Dalal和Triggs采用了四种不同的方式对区块进行归一化，分别是L2-norm、L2-hys、L1-norm、L1-sqrt，用$v$表示未被归一化的向量，以L2-norm为例，归一化后的特征向量为， v=\frac{v}{\sqrt{\|v\|_{2}^{2}+\varepsilon^{2}}}作者通过对比发现，L2-norm、L2-hys、L1-sqrt三种方式所取得的效果是一样的，L1-norm表现相对差一些。 SVM分类器最后一步，也是比较关键的一步，就是训练分类器，用SVM对前面提取的图像特征向量进行训练，寻找一个最优超平面作为决策函数，得到目标的训练模型。 编程实践完整代码请查看： https://github.com/Jackpopc/aiLearnNotes/blob/master/computer_vision/HOG.py HOG是一个优秀的特征提取算法，因此本文就仅介绍并实现特征提取算法部分，后面的训练分类器和目标检测偏重于机器学习内容，在这里就不多赘述。 HOG算法非常经典，因此，很多成熟的第三方库都已经集成了这个算法，例如比较知名的计算机视觉库OpenCV，对于HOG特征提取比较简单的方式就是直接调用OpenCV库，具体代码如下， 1234import cv2hog = cv2.HOGDescriptor()img = cv2.imread("../data/2007_000129.jpg", cv2.IMREAD_GRAYSCALE)des = hog.compute(img) 为了更好的理解HOG算法，本文就跟随文章的思路来重新实现一遍算法。 第一步：计算梯度方向和梯度幅值 这里用Sobel算子来计算水平和垂直方向的差分，然后用对梯度大小加权求和的方式来计算统计时使用的梯度幅值， 123456def compute_image_gradient(img): x_values = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5) y_values = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5) magnitude = cv2.addWeighted(x_values, 0.5, y_values, 0.5, 0) angle = cv2.phase(x_values, y_values, angleInDegrees=True) return magnitude, angle 第二步：统计细胞单元的梯度方向 指定细胞单元尺寸和角度单元，然后对用直方图统计一个细胞单元内的梯度方向，如果梯度角度落在一个区间内，则把该像素的幅值加权到和角度较近的一个角度区间内， 123456789101112def compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit): centers = [0] * bin_size # 遍历细胞单元，统计梯度方向 for i in range(cell_magnitude.shape[0]): for j in range(cell_magnitude.shape[1]): strength = cell_magnitude[i][j] gradient_angle = cell_angle[i][j] min_angle, max_angle, mod = choose_bins(gradient_angle, unit, bin_size) # 根据角度的相近程度分别对邻近的两个区间进行加权 centers[min_angle] += (strength * (1 - (mod / unit))) centers[max_angle] += (strength * (mod / unit)) return centers 第三步：块内归一化 根据HOG原文的思想可以知道，图像内分块，块内分细胞单元，然后对细胞单元进行统计。一个块由多个细胞单元组成，统计了每个细胞单元的梯度特征之后需要对这几个向量进行归一化， 1234567891011121314151617def normalized(cell_gradient_vector): hog_vector = [] for i in range(cell_gradient_vector.shape[0] - 1): for j in range(cell_gradient_vector.shape[1] - 1): block_vector = [] block_vector.extend(cell_gradient_vector[i][j]) block_vector.extend(cell_gradient_vector[i][j + 1]) block_vector.extend(cell_gradient_vector[i + 1][j]) block_vector.extend(cell_gradient_vector[i + 1][j + 1]) mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector)) magnitude = mag(block_vector) if magnitude != 0: # 归一化 normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector] block_vector = normalize(block_vector, magnitude) hog_vector.append(block_vector) return hog_vector 第四步：可视化 为了直观的看出特征提取的效果，对下图进行特征提取并且可视化， 可视化的方法是在每个像素上用线段画出梯度的方向和大小，用线段的长度来表示梯度大小， 12345678910111213141516171819def visual(cell_gradient, height, width, cell_size, unit): feature_image = np.zeros([height, width]) cell_width = cell_size / 2 max_mag = np.array(cell_gradient).max() for x in range(cell_gradient.shape[0]): for y in range(cell_gradient.shape[1]): cell_grad = cell_gradient[x][y] cell_grad /= max_mag angle = 0 angle_gap = unit for magnitude in cell_grad: angle_radian = math.radians(angle) x1 = int(x * cell_size + magnitude * cell_width * math.cos(angle_radian)) y1 = int(y * cell_size + magnitude * cell_width * math.sin(angle_radian)) x2 = int(x * cell_size - magnitude * cell_width * math.cos(angle_radian)) y2 = int(y * cell_size - magnitude * cell_width * math.sin(angle_radian)) cv2.line(feature_image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude))) angle += angle_gap return feature_image 提取的特征图为，图中白色的线段即为提取的特征， 完整代码如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import cv2import numpy as npimport mathimport matplotlib.pyplot as pltimg = cv2.imread("../data/2007_000129.jpg", cv2.IMREAD_GRAYSCALE)def compute_image_gradient(img): x_values = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5) y_values = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5) magnitude = abs(cv2.addWeighted(x_values, 0.5, y_values, 0.5, 0)) angle = cv2.phase(x_values, y_values, angleInDegrees=True) return magnitude, angledef choose_bins(gradient_angle, unit, bin_size): idx = int(gradient_angle / unit) mod = gradient_angle % unit return idx, (idx + 1) % bin_size, moddef compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit): centers = [0] * bin_size for i in range(cell_magnitude.shape[0]): for j in range(cell_magnitude.shape[1]): strength = cell_magnitude[i][j] gradient_angle = cell_angle[i][j] min_angle, max_angle, mod = choose_bins(gradient_angle, unit, bin_size) print(gradient_angle, unit, min_angle, max_angle) centers[min_angle] += (strength * (1 - (mod / unit))) centers[max_angle] += (strength * (mod / unit)) return centersdef normalized(cell_gradient_vector): hog_vector = [] for i in range(cell_gradient_vector.shape[0] - 1): for j in range(cell_gradient_vector.shape[1] - 1): block_vector = [] block_vector.extend(cell_gradient_vector[i][j]) block_vector.extend(cell_gradient_vector[i][j + 1]) block_vector.extend(cell_gradient_vector[i + 1][j]) block_vector.extend(cell_gradient_vector[i + 1][j + 1]) mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector)) magnitude = mag(block_vector) if magnitude != 0: normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector] block_vector = normalize(block_vector, magnitude) hog_vector.append(block_vector) return hog_vectordef visual(cell_gradient, height, width, cell_size, unit): feature_image = np.zeros([height, width]) cell_width = cell_size / 2 max_mag = np.array(cell_gradient).max() for x in range(cell_gradient.shape[0]): for y in range(cell_gradient.shape[1]): cell_grad = cell_gradient[x][y] cell_grad /= max_mag angle = 0 angle_gap = unit for magnitude in cell_grad: angle_radian = math.radians(angle) x1 = int(x * cell_size + magnitude * cell_width * math.cos(angle_radian)) y1 = int(y * cell_size + magnitude * cell_width * math.sin(angle_radian)) x2 = int(x * cell_size - magnitude * cell_width * math.cos(angle_radian)) y2 = int(y * cell_size - magnitude * cell_width * math.sin(angle_radian)) cv2.line(feature_image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude))) angle += angle_gap return feature_imagedef main(img): cell_size = 16 bin_size = 9 unit = 360 // bin_size height, width = img.shape magnitude, angle = compute_image_gradient(img) cell_gradient_vector = np.zeros((height // cell_size, width // cell_size, bin_size)) for i in range(cell_gradient_vector.shape[0]): for j in range(cell_gradient_vector.shape[1]): cell_magnitude = magnitude[i * cell_size:(i + 1) * cell_size, j * cell_size:(j + 1) * cell_size] cell_angle = angle[i * cell_size:(i + 1) * cell_size, j * cell_size:(j + 1) * cell_size] cell_gradient_vector[i][j] = compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit) hog_vector = normalized(cell_gradient_vector) hog_image = visual(cell_gradient_vector, height, width, cell_size, unit) plt.imshow(hog_image, cmap=plt.cm.gray) plt.show()if __name__ == '__main__': img = cv2.imread('../data/2007_002293.jpg', cv2.IMREAD_GRAYSCALE) cv2.imshow("origin", img) cv2.waitKey() main(img)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | Windows下一款强大的启动搜索工具]]></title>
    <url>%2F2019%2F07%2F02%2Ftools-wox%2F</url>
    <content type="text"><![CDATA[前言对于大多数人来说，日常生活和工作中接触较多的软件和工具就是浏览器、专业软件、翻译软件、笔记、办公等。其实有很多软件在名气上虽然不如这些商业软件，但是功能却丝毫不输这些知名软件。在工作中能够大大提高办公效率，而且内存占用小、免费开源。大家都知道windows自带的文件浏览器查找文件是一件非常令人痛苦的事情，不仅速度缓慢，而且准确度出奇的低，让人感觉很鸡肋。但是当我们要找一个文档时却忘记放在哪里，挨个硬盘去翻更加令人感到折磨。所以不得不去借助一些高效的搜索工具，其中用的较多、名气较大的就是everything。我个人也一直在用这款工具，的确非常强大，快速、支持正则表达式匹配。它作为一个文件搜索工具的确很称职，但是当我们想要更多扩展功能，例如用于程序启动工具时everything就显得有些不足了。之前我介绍过一款工具叫做Listary，能够完美的与everything结合，既能涵盖everything强大的搜索功能，还能融合Listary实用的启动功能。本文再给大家介绍一款与Listary类似的工具—Wox，有相同之处，也有很大的差异之处，各位可以根据自己的喜好进行选择。 Wox Wox是一款启动器。这就是它与Listary的最大的不同之处—定位不同。Listary本身兼顾搜索与启动，但是在搜索方面不如everything，如果想使用更加丰富的搜索功能需要在设置里配置一下everything，如果满足于Listary提供的快速搜索功能则无需配置。而Wox的定位就是一个简单、纯净的启动器。它可以快速的启动本机安装的各种程序、文件、网页等。当然，它也可以用于文件搜索，它指定的后端搜索工具是everything，所以在打开Wox之前需要先启动everything，这样才能够使用强大的搜索功能。它不仅可以用于搜索程序和文件，还可以配置各种丰富的插件满足更多场景的需求，例如计算器、天气、翻译、网页搜索等。 Wox与Listary前面提到了，Wox与Listary有很多类似之处： 搜索 启动器 配合everything使用 但是Wox也有很多特别之处是Listary无法比拟的，Wox的特别之处主要有如下几点： 支持丰富的插件 支持自己定义插件 支持多种主题切换 支持自定义快捷键 支持丰富插件Wox的插件主要分类两种： 系统插件 第三方插件 系统插件不需要关键字唤醒，直接用Alt + Space调出Wox的工具栏输入相应的命令即可，系统插件主要包含如下几类： 程序插件 颜色插件 控制面板插件 计算器插件 网址插件 Web搜索插件 命令行插件 文件夹插件 拿其中几个举个例子， 程序插件 Alt+空格键激活Wox，然后输入要启动的程序即可， 计算器插件 计算器对于很多人来说虽然不是主要的工作工具，但是偶尔会用到，当我们需要用计算器的时候就需要点击windows图标，搜索“计算器”，这样比较麻烦，Wox集成了计算器插件，激活Wox后输入要计算的公式即可， 网址插件 当我们要浏览某个网站时往往需要打开浏览器-&gt;在地址栏输入网址，Wox的浏览器插件大大简化这个过程，只需要激活Wox，输入相应网址即可， 其他还有很多实用的系统插件，可以查看网站进行了解， http://doc.wox.one/zh/basic/ 除了Wox自带的系统插件，Wox还提供了多大230款第三方插件，其中就包含有道翻译、天气查询、Steam、Putty、二维码、维基百科、书签搜索、待办事项、进制转换、哔哩哔哩、Skype、FileZilla、Stack Overflow、沪江日语等等。只需要下载安装一下即可，而且Wox提供了多选、简单的安装方式， 安装第三方插件 命令安装 这是最简单的一种安装方式，使用wpm进行插件的安装、卸载管理， 123456789# 安装插件wpm install &lt;插件名称&gt;# 卸载插件wpm uninstall &lt;插件名称&gt;# 列出已安装插件wpm list 手动安装 如果由于网络、代理等原因无法命令安装，可以打开插件主页[http://www.wox.one/plugin]下载到本地(以.wox结尾),拖动到Wox搜索框进行安装， 支持自己定义插件除了官网提供的系统插件和第三方插件之外，Wox还支持自定义插件，它支持以下3种方式来定义插件， plugin.json C# Python Wox与插件之间的通信原理： 支持多种主题切换 Wox安装后会发现自带BlurBlack、BlurWhite、Dark、Gray、Light、Metro Server、Pink七种主题，除了上述提到的7种主题之外，还可以在官网自定义主题，配置之后下载主题(.xaml文件)，放置到C:\Users\YourUserName\AppData\Local\Wox\app-1.3.524\Themes路径下，重启Wox即可。 支持自定义快捷键这一点也是Wox吸引人的一点，它支持自定义快捷键。如果觉得Alt+空格启动程序、文件夹还不够快捷，可以把常用的命令保存到快捷键，这样当使用快捷键时能够快速达到目的。 例如，我想百度搜索“哈尔滨工业大学”，使用Wox的方式是这样的， Alt + 空格激活Wox 输入”bd 哈尔滨工业大学” 这样比起”打开浏览器-&gt;打开百度-&gt;搜索”已经便捷了很多，但是还有更便捷的，就是Wox支持的快捷键。 可以把常用的命令添加到快捷键，例如把”bd 哈尔滨工业大学”添加为快捷键”Ctrl+Alt+H”,能够同时激活Wox并输入相应的命令，然后按Enter键即可搜索。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>文件查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一份热门机器学习资源]]></title>
    <url>%2F2019%2F07%2F01%2Fhomemade-machine-learning%2F</url>
    <content type="text"><![CDATA[前言最近几年人工智能异常火热，随之而来的就是各种针对入门者的学习资源，其中不乏很多经典的教程例如吴恩达的《机器学习》、《深度学习工程师》，但是也有很多千篇一律、照本宣科的学习资源。在学习进阶过程中很多人会到GitHub寻找一些可以动手实践的机器学习项目，会发现GitHub上会有和机器学习相关的各种awesome，恨不得把所有和机器学习、深度学习的资源都囊括进去。这样虽然全面，但是我认为它的价值并不高。我们之所以希望有经验者推荐学习资源，就是因为时间、精力有限，希望能够在鱼龙混杂的学习资源里筛选出真正有价值，或者与众不同的，能够让我们利用有限的精力和时间内真正学会一些东西。近期GitHub有一个关于机器学习的热门开源项目，homemade-machine-learning，目前已经11k+个star，近一周达到1.1k+，经过一段时间的学习发现这的确一个不错的学习项目，下面就详细介绍一下这个项目。 Homemade Machine Learning 开门见山，这个开源项目主要有以下几个优点： 少而精 不依赖python第三方库 详细解释它们背后的数学原理 交互式Jupyter notebook演示程序 丰富易懂的示例 这个项目用Python实现了目前热门、使用的一些机器学习算法，而不是像很多开源项目那样，从头至尾把每个机器学习算法都实现一遍。换句话说，这个开源项目追求“少而精”，它分别从监督学习、非监督学习、神经网络、异常检测、回归、分类这些类别中选择一种算法进行详细阐述算法背后的数学原理，然后使用jupyter notebook交互式的演示，随后会用多个示例进行实现，动手操作，不依赖集成的python第三方库，更容易理解机器学习算法的原理。 项目概括该项目主要包括如下几个方面的机器学习算法： 监督学习 无监督学习 异常检测 神经网络 其中监督学习又分为回归和分类，回归算法选取的是比较常用的线性回归，分类算法选取的是比较实用的逻辑回归。无监督学习中主要针对聚类进行讲解，项目中选取的是热门的k-means。异常检测是指通过大多数数据来检测出有显著差异的事件、观测结果，在数据处理、图像处理都有应用。神经网络中选择的是多层感知机。 安装首先要保证电脑上正确的安装了Python，然后安装一些项目依赖， 1pip install -r requirements.txt requirements: 1234567jupyter==1.0.0matplotlib==3.0.1numpy==1.15.3pandas==0.23.4plotly==3.4.1pylint==2.1.1scipy==1.1.0 如果要使用jupyter notebook，需要在命令行输入下面命令， 1jupyter notebook 然后会在浏览器中打开如下窗口， 详细介绍数学原理 我认为这是这个项目吸引人的地方，也是它与众不同的地方，它和很多项目不同，浮于表面，把很多环节都认为是既定的去阐述，有一些初学者会看的云里雾里，不明白“为什么是这样？”这个项目则不同，它详细、深入的阐述每个算法背后的数学原理，循序渐进，配合可视化很容易让人理解。 详细编码过程 该项目不过多依赖tensorflow、pytorch、keras这些高度集成的机器学习平台，它从梯度下降到损失函数、从训练到预测都是一步一步实现，尽量减少对高度集成第三方库的依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104@staticmethoddef gradient_descent(data, labels, initial_theta, lambda_param, max_iteration): """Gradient descent function. Iteratively optimizes theta model parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param initial_theta: initial model parameters. :param lambda_param: regularization parameter. :param max_iteration: maximum number of gradient descent steps. """ # Initialize cost history list. cost_history = [] # Calculate the number of features. num_features = data.shape[1] # Launch gradient descent. minification_result = minimize( # Function that we're going to minimize. lambda current_theta: LogisticRegression.cost_function( data, labels, current_theta.reshape((num_features, 1)), lambda_param ), # Initial values of model parameter. initial_theta, # We will use conjugate gradient algorithm. method='CG', # Function that will help to calculate gradient direction on each step. jac=lambda current_theta: LogisticRegression.gradient_step( data, labels, current_theta.reshape((num_features, 1)), lambda_param ), # Record gradient descent progress for debugging. callback=lambda current_theta: cost_history.append(LogisticRegression.cost_function( data, labels, current_theta.reshape((num_features, 1)), lambda_param )), options=&#123;'maxiter': max_iteration&#125; ) # Throw an error in case if gradient descent ended up with error. if not minification_result.success: raise ArithmeticError('Can not minimize cost function: ' + minification_result.message) # Reshape the final version of model parameters. optimized_theta = minification_result.x.reshape((num_features, 1)) return optimized_theta, cost_history@staticmethoddef gradient_step(data, labels, theta, lambda_param): """GRADIENT STEP function. It performs one step of gradient descent for theta parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param theta: model parameters. :param lambda_param: regularization parameter. """ # Initialize number of training examples. num_examples = labels.shape[0] # Calculate hypothesis predictions and difference with labels. predictions = LogisticRegression.hypothesis(data, theta) label_diff = predictions - labels # Calculate regularization parameter. regularization_param = (lambda_param / num_examples) * theta # Calculate gradient steps. gradients = (1 / num_examples) * (data.T @ label_diff) regularized_gradients = gradients + regularization_param # We should NOT regularize the parameter theta_zero. regularized_gradients[0] = (1 / num_examples) * (data[:, [0]].T @ label_diff) return regularized_gradients.T.flatten()@staticmethoddef cost_function(data, labels, theta, lambda_param): """Cost function. It shows how accurate our model is based on current model parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param theta: model parameters. :param lambda_param: regularization parameter. """ # Calculate the number of training examples and features. num_examples = data.shape[0] # Calculate hypothesis. predictions = LogisticRegression.hypothesis(data, theta) # Calculate regularization parameter # Remember that we should not regularize the parameter theta_zero. theta_cut = theta[1:, [0]] reg_param = (lambda_param / (2 * num_examples)) * (theta_cut.T @ theta_cut) # Calculate current predictions cost. y_is_set_cost = labels[labels == 1].T @ np.log(predictions[labels == 1]) y_is_not_set_cost = (1 - labels[labels == 0]).T @ np.log(1 - predictions[labels == 0]) cost = (-1 / num_examples) * (y_is_set_cost + y_is_not_set_cost) + reg_param # Let's extract cost value from the one and only cost numpy matrix cell. return cost[0][0] 丰富示例 理解了算法背后的数学原理，跟着作者一步一步实现了算法，要想更加深入的理解就需要把算法应用到不同方面，本项目提供了丰富的示例，其中不乏MNIST这类经典的演示样例。 其中每个项目后面都包含至少一个示例，可以获取对应的数据进行实现，这样对算法的理解和应用会有更加清晰而深入的认识。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文熟练掌握Docker使用]]></title>
    <url>%2F2019%2F06%2F30%2Flearning-docker%2F</url>
    <content type="text"><![CDATA[Docker是由dotCloud公司发起并与2013年开源的一个项目，一径开源就备受欢迎，其主要项目至今在github已经54k个star。它是使用Go语言开发实现，基于Linux内核cgroup、namespace以及AUFS类等技术对进程进行封装隔离，属于一种操作系统层面的虚拟化技术。此后，进一步开发开始使用runC和containerd，进一步封装，从文件系统到网路互联，再到进行都进行隔离，极大的简化了容器的创建和维护，使得Docker比虚拟机更为轻便、快捷。 为什么要用docker？Docker与传统虚拟机一样，同属于虚拟化技术，但是它拥有众多虚拟机无法比拟的优势： 持续交付和部署 更快的迁移 更高效的利用系统资源 更快的启动时间 一致的运行环境 更轻松的维护和扩展 容器与虚拟机对比详情： 对于大多数开发人员感受最为就是前两点：持续交付和部署、更快的迁移。 我想这对于很多开发人员都是一个很头疼的问题，在开发过程中会遇到这种抱怨：“在我电脑上可以运行啊？为什么换一台电脑就不行了？” 虽然诸如maven、nodejs的package.json、Python的requirement.txt的出现使得迁移变得简单，但是它们更多的是使得在第三方工具包的迁移方面变得简单方面，但是在系统和开发环境方面却没有什么作用。docker确保了直行环境的一致性，可以在多平台上运行，使得应用迁移更加容易。此外，docker使用分层存储以及镜像技术，使得应用重复部分的复用更加容易，可以基于基础镜像做更多的扩展，使得系统的维护变得更加简单。 基本概念使用docker接触最多的就是以下3个概念， 镜像：image 容器：container 仓库：repository 了解这三个概念，对容器的整个生命周期便有了认识。在这里，我用简单的语言对上述3个概念进行描述 镜像：进行就相当于一个精简化的文件系统，例如官方提供的Ubuntu镜像，就只包含了最小化的root文件系统。 容器：容器是一个拥有自己root文件系统、自己网络配置、自己命名空间的进程。镜像和容器就像是编程中的类和实例，镜像时静态的定义，而镜像运行时的实体是容器。什么是类和实例？举一个编程的例子阐述一下， 1234567891011121314# 类class HelloWorld: def __init__(self, x, y): self.x = x self.y = y def add(self): return self.x + self.y# 实例hello_world = HelloWorld(2, 3)print(hello_world.add())&gt;&gt;&gt; 5 其中HelloWorld是类，hello_world是实例，类比一下，就能够理解容器和镜像之间的关系。 仓库：docker镜像仓库就如同github代码仓库一样，当一个人构建一个项目，想在其他其他电脑上运行这个项目，那么就去从代码仓库把这个项目克隆下来。docker镜像仓库也是这样，当构建一个镜像之后，想在其他服务器上使用这个镜像，就需要一个集中的存储、分发服务，仓库就是这样的服务。官方的镜像仓库是DockerHub，它存储了丰富的镜像，但是国内拉取镜像速度缓慢，因此可以使用国内镜像仓库进行替代，例如阿里云镜像仓库、网易云镜像仓库、DaoCloud镜像市场等。 安装docker目前支持Linux、Windows 10、macOS，下面就一个Linux安装为例， APT方式安装 首先安装HTTPS软件包和CV证书， 123456$ sudo apt-get update$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 添加软件源GPG密钥， 1$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add 添加docker软件源， 1234$ sudo add-apt-repository \ "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable" 安装docker ce, 12$ sudo apt-get update$ sudo apt-get install docker-ce 添加用户组 docker命令会使用Unix socket与docker引擎通讯，因此每次使用时会需要root权限，也就是需要在命令前加sudo比较麻烦，为了避免这个麻烦可以把建立docker组并把当前用户加入docker用户组， 12$ sudo groupadd docker$ sudo usermod -aG docker $USER 启动、退出、重启docker 123$ systemctl start docker$ systemctl stop docker$ systemctl restart docker 也可以使用， 123$ service docker start$ service docker stop$ service docker restart Dockerfile理解docker中一些基本概念，并完成docker安装下一步就是学习docker的使用。对于大多数开发人员来说，docker使用过程中最为核心的部分就是Dockerfile。 Dockerfile是一个文本文件，它包含了一些指令，docker镜像的构建就是通过Dockerfile中的这一条一条的指令完成的。也就是说，要构建一个镜像，就需要一个Dockerfile，然后根据自己的需求配置一些指令集合，下面就看一下Dockerfile中使用的一些指令。 FROM：指定基础镜像 定制我们的镜像，是需要以一个镜像为基础的，就是基础镜像，例如Ubuntu、 nginx、postgres、mysql等，例如，FROM Ubuntu:16.04，如果本地有Ubuntu基础镜像则使用本地基础镜像，如果没有则会到官方镜像仓库拉取，16.04是镜像版本号，如果不指定则会拉取lastest。 RUN：执行命令 RUN指定我们在构建镜像时需要执行的命令，比如apt-get install安装某个软件，pip install安装Python依赖包，配置软件源，配置时区等， 例如，RUN apt-get install python3。 ADD和COPY：文件操作 ADD和COPY是两个功能类似的指令，一般优先使用COPY，它比ADD更透明，它的功能是将本地文件拷贝到容器中，例如，COPY ./ /home/jackpop/test。 WORKDIR：指定工作路径 指定镜像的运行时的工作路径，例如，WORKDIR /home/jackpop/test 。 ENTRYPOINT：设置镜像主命令 指定镜像运行是运行的命令，例如, ENTRYPOINT [“python”, “-m”, “main”]。 LABEL：添加标签 可以为镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。 CMD：执行目标镜像中包含的软件 如果创建镜像的目的是为了部署某个服务，可能会执行某种形式的命令，可以包含参数。 EXPOSE：指定监听端口 给外部访问指定访问端口。 ENV：环境变量 为了方面程序运行，有时需要更新环境变量。 VOLUME：暴露数据库存储文件 USER：指定当前用户 其中常用的命令就是FROM、COPY、WORKDIR、RUN、ENTRYPOINT。 常用命令了解了Dockerfile的常用指令，我们该怎么对镜像和容器进行操作呢？下面就来学习一下docker常用的一些命令， 备注：由于我已经把当前用户加入到docker用户组，所以下面命令没有加sudo，如果没有加用户组需要使用sudo docker。 查看本地镜像 123$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 ****** 10 days ago 119MB 查看容器 123$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES*** *** *** *** *** *** *** *** 启动、停止、重启容器 123$ docker start $container_id$ docker stop $container_id$ docker restart $container_id 退出和进入镜像 12$ exit$ docker exec $container_id /bin/bash 启动镜像 1$ docker run $image_id 可以用—p和—dns指定端口和dns来配置网络。 container_id是容器ID，image_id是镜像ID。 拉取镜像 1$ docker image pull ubuntu 从Dockerfile创建镜像 1$ docker build 从一个修改的容器创建镜像 1$ docker commit 容器与本地之间复制文件 1$ docker cp 推送镜像 1$ docker push 为镜像打标签 1$ docker tag 重命名容器 1$ docker rename 删除容器 1$ docker rm 删除镜像 1$ docker rmi 搜索镜像 1$ docker search docker常用命令概括： 实践创建项目 123Test/├── Dockerfile└── main.py 写一个简单的测试程序 1234567891011121314151617181920# main.pyimport loggingfrom time import sleepimport numpy as nplogging.basicConfig(level=logging.DEBUG, format="'%(asctime)s - " "%(filename)s[line:%(lineno)d] - " "%(levelname)s: %(message)s")def main(): for i in range(10): logging.debug(np.random.randint(0, 5)) sleep(0.1)if __name__ == '__main__': main() Dockerfile 这是构建镜像中的重点部分， 1234567891011FROM ubuntu:16.04COPY ./ /home/Test_dockerWORKDIR /home/Test_dockerRUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip \&amp;&amp; ln -s pip3 /usr/bin/pip \&amp;&amp; ln -sf /usr/bin/python3 /usr/bin/python \&amp;&amp; rm -rf ls /var/cache/apt/* \ENTRYPOINT ["python3", "-m", "main"] 进入项目根目录 1$ cd Test 开始创建 1$ docker build test:v1.0 . test是指定构建镜像的名称，v1.0指定镜像标签，如果不指定，镜像名称和标签会显示为。 运行镜像 1234567891011$ docker run $image_id'2019-06-29 12:26:38,298 - main.py[line:13] - DEBUG: 0'2019-06-29 12:26:38,399 - main.py[line:13] - DEBUG: 2'2019-06-29 12:26:38,499 - main.py[line:13] - DEBUG: 1'2019-06-29 12:26:38,599 - main.py[line:13] - DEBUG: 3'2019-06-29 12:26:38,699 - main.py[line:13] - DEBUG: 0'2019-06-29 12:26:38,799 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:38,900 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,000 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,100 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,200 - main.py[line:13] - DEBUG: 2 当然也可以在基础镜像的基础上进行修改来创建我们的镜像，例如，我们拉取一个Ubuntu基础镜像，可以启动镜像后安装我们需要的软件和环境，然后利用docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]来创建一个新镜像。 延伸阅读除了基础的docker之外，还有一些高级的docker开源工具，比较知名的有如下3项， docker compose docker machine docker swarm 其中docker compose是官方编排项目之一，用于快速在集群中部署分布式应用。docker machine同样是官方编排项目之一，负责在多种平台上快速安装docker环境。docker swarm提供docker容器集群服务，是docker官方对容器云生态进行支持的核心方案。 除此之外，还有一些比较知名的集群管理系统，例如， Mesos Kubernetes 其中Mesos是来自UC Berkeley的集群资源管理开源项目，它可以让用户很容易实现分布式应用的自动化调度。Kubernetes是由Google团队发起并维护的给予docker的开源容器集群管理系统，应用比较广泛，它不仅支持场景的云平台，而且支持内部数据中心。 学习资源上述所讲的常用命令、指令含义等对于日常开发使用已经够用了，如果对Docker更深入的内容，例如，数据管理、安全、底层实现、容器与云计算等感兴趣可以选取其他的学习资料。在这里我推荐一份我认为不错的学习资料。就是yeasy大神在github开源的一份详细的docker教程—docker_practice，目前docker_practice项目在github已经13.7k个star，想深入学习的可以查看github项目， 也可以查看gitbooks， 或者关注公众号【平凡而诗意】回复关键字”dk”获取pdf和epub版教程， 更多内容请关注公众号【平凡而诗意】]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019最佳论文解读]]></title>
    <url>%2F2019%2F06%2F28%2FCVPR2019%E6%9C%80%E4%BD%B3%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[前言 CVPR，全称IEEE Conference on Computer Vision and Pattern Recognition，与ECCV(Europeon Conference on Computer Vision)，ICCV( IEEE International Conference on Computer Vision)并称为计算机视觉领域三大会议，均为计算机视觉领域的顶级会议。由于近几年计算机视觉的异常火热，CVPR也就成为很多计算机视觉领域研究者趋之若鹜的盛宴，它的受关注程度更是今非昔比。CVPR2019于2019年6月16日在美国召开，此次会议共收到来自全球14104位研究者提交的5160篇文章，同比2018年增长56%，一举打破记录，受欢迎程度可见一斑。 CVPR2019最终共接收1294篇文章，尽管CVPR被计算机视觉领域视为顶尖，我个人认为，其中不乏质量平平的水文，真正令人印象深刻，几年之后依然被人所熟知且实用，并在算法思想方面取得跨越的却寥寥无几。闲话说完，回到本文的重点CVPR2019最佳论文，该荣誉最终由卡耐基梅隆大学、多伦多大学、伦敦大学学院的多位研究者斩获，论文名称为A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction，接下来，我详细解读一下这篇文章。 数学符号含义$s$ 光源上的点$v$ 可见场景内的点$x$ 不可见场景内的点$d$ 检测器上的点$\tau_{\mathcal{F}}$ 费马路径长度$I(\tau ; \boldsymbol{v})$ 瞬态 概念解释瞬态(transients):一种测量值，用于重建隐藏形状信息的大多数方法采用快速调制光源已经传感器来记录光子强度和旅行时间的测量值。费马路径(Fermat paths):首先返回的光子路径的超集合。费马路径长度(Fermat pathlengths):顾名思义，就是通过光速等计算出来的离散路径长度。 算法详解 目前大多数计算机视觉领域的研究都是围绕着视觉可见范围内的研究，但是理解视野范围之外的场景在很多领域却有着非常重要的应用，因此，这使得这项研究更加具有价值。被动方式通过分析隐藏场景所投射的阴影来粗略估计物体的运动和结构，或者利用光的相干性来定位隐藏对象。这些方法没有足够的信息来精确计算位置隐藏场景的三维形状。主动方式提取隐藏场景的附加信息时可能的。大多数重建隐藏形状信息的方法都是用调制光源和时间分辨传感器、超快光电二极管等，这些传感器不仅记录入射光子的强度，还在时间分辨率范围内记录它们到达的时间，这种测量称为瞬态。 大多数主动技术都是通过测量一个已知可见场景的不同位置的瞬态，然后根据已经获取的辐射成像逆向来进行三维重建，例如椭圆反投影、正则化线性系统、光锥变换等。这些方法主要有两个缺点： 它们依赖于辐射测量信息 为了简化反演问题，所有现有的重建技术都依赖于非视线场景的Lambertian 反射假设 在这篇文章中，作者提出一种使用视线以外场景的瞬态测量得到的几何信息的方法，克服了上述的限制。简言之，它主要使用视线内和视线外场景之间的一种称之为费马路径的几何路径星系，通过观察发现这些路径遵循镜面或者物体边界特点的反射定理。作者证明，费马路径对应于瞬态测量中的不连续性，不连续点的时间位置仅是视线外场景对象的形状而不是其反射率。利用上述理论，推导出一种精确重建视线外物体形状的算法，称之为费马流(Fermat Flow)。作者证明，费马路径长度的空间导数提供了一个简单的约束，它唯一地决定了隐藏场景点的深度和法线。这个导数是通过将光滑的路径函数拟合到一组稀疏的测量值上而得到，然后结合深度和法线信息来计算平滑网网格。概括一下，本文在隐藏物体重建方面主要包含以下3个步骤： 瞬态测量 求解费马流方程 表面拟合 测量瞬态 假设已经校准了从光源到可见点，和从可见点到检测器的距离 \mathcal{\tau\mathcal{V}}(\boldsymbol{v}) \triangleq\|s-v\|+\|d-v\|，那么可以通过光速等计算在非可见场景的路径长度， I(\tau ; \boldsymbol{v})=\int_{\mathcal{X}} f(\boldsymbol{x} ; \boldsymbol{v}) \delta(\tau-\tau(\boldsymbol{x} ; \boldsymbol{v})) \mathrm{d} A(p, q)其中 $\tau(\boldsymbol{x} ; \boldsymbol{v}) \triangleq 2 \cdot|\boldsymbol{x}-\boldsymbol{v}|,(p, q) \in[0,1]^{2} $是非可见物体表面的参数化表示。 费马流方程 给定测量的瞬态$I(\tau ; \boldsymbol{v})$ ，可以把它的离散性定义为对费马路径长度的贡献，每个路径长度约束了球面上的点的法线和曲率。这是本文的核心所在，给定一组费马路径长度，就可以得到隐藏物体表面点集的位置和法线。首先定义费马路径函数， \tau_{\mathcal{F}}(\boldsymbol{v})=\{\tau : I(\tau ; \boldsymbol{v}) \text { is discontinuous }\}在每个瞬态可以有多个不连续的路径长度，因此费马路径函数是一个多值函数 \boldsymbol{x}_{\mathcal{F}}=\boldsymbol{v}-\left(\tau_{\mathcal{F}}(\boldsymbol{v}) / 4\right) \nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})其中 $\boldsymbol{x}_{\mathcal{F}} $是隐藏物体球面上的点，因此，物体可以唯一的被可见点\boldsymbol{v}、路径长度、梯度 $\nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})$ 重建，得到隐藏物体表面的点，然后通过一些简单的集合操作即可。但是就算路径长度的导数是一件非常难的事情，它和选取的可视面的形状、位置有密切的关系，为了简化，文中采用选取平面作为可视区域，得到的导数为， \begin{array}{l}{\nabla_{\boldsymbol{v}^{\tau} \mathcal{F}}(\boldsymbol{v})=} \\ {\left.\left(\frac{\partial \tau_{\mathcal{F}}}{\partial x}, \frac{\partial \tau_{\mathcal{F}}}{\partial y}, \sqrt{4-\left(\frac{\partial \tau_{\mathcal{F}}}{\partial x}\right)^{2}-\left(\frac{\partial \tau_{\mathcal{F}}}{\partial y}\right)^{2}}\right)\right|_{\boldsymbol{v}}}\end{array}表面拟合上述的步骤生成了一系列的有向点云，它的密度相当于在可视区域 \mathcal{V} 上的测量密度，然后，可以使用算法，利用正常信息，以更高的精度将曲面表示(如三角形网格)匹配到点云，给定这样一个初始的表面重建，在补充中，我们描述了一个基于高光路径扰动理论的优化过程，该过程对拟合表面进行了细化，以考虑由于梯度 $\nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})$ 估计不准确而可能产生的误差。 实验结果 如图中所示，分别从两个视图中重建了一个有方向的点云，点按它们的法线着色。最后，我们将一个表面与点云相匹配，显示在右边的两个视图下。 扫描的对象跨越各种形状(凸，凹)和反射(半透明，光泽，镜面)。对于每一个物体，我们展示了环境光下的照片，以及它表面重建的两个视图。 更多我的作品Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 Jackpop：【动手学计算机视觉】第三讲：图像预处理之图像分割 Jackpop：【动手学计算机视觉】第四讲：图像预处理之图像增广 Jackpop：【动手学计算机视觉】第五讲：传统目标检测之特征工程 Jackpop：【动手学计算机视觉】第六讲：传统目标检测之Harris角点检测 Jackpop：【动手学计算机视觉】第七讲：传统目标检测之SIFT特征]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第七讲：传统目标检测之SIFT特征]]></title>
    <url>%2F2019%2F05%2F07%2F7-SIFT%2F</url>
    <content type="text"><![CDATA[前言提到传统目标识别，就不得不提SIFT算法，Scale-invariant feature transform，中文含义就是尺度不变特征变换。此方法由David Lowe于1999年发表于ICCV(International Conference on Computer Vision)，并经过5年的整理和晚上，在2004年发表于IJCV(International journal of computer vision)。由于在此之前的目标检测算法对图片的大小、旋转非常敏感，而SIFT算法是一种基于局部兴趣点的算法，因此不仅对图片大小和旋转不敏感，而且对光照、噪声等影响的抗击能力也非常优秀，因此，该算法在性能和适用范围方面较于之前的算法有着质的改变。这使得该算法对比于之前的算法有着明显的优势，所以，一直以来它都在目标检测和特征提取方向占据着重要的地位，截止2019年6月19日，这篇文章的引用量已经达到51330次(谷歌学术)，受欢迎程度可见一斑，本文就详细介绍一下这篇文章的原理，并一步一步编程实现本算法，让各位对这个算法有更清晰的认识和理解。 SIFT前面提到，SIFT是一个非常经典而且受欢迎的特征描述算法，因此关于这篇文章的学习资料、文章介绍自然非常多。但是很多文章都相当于把原文翻译一遍，花大量篇幅在讲高斯模糊、尺度空间理论、高斯金字塔等内容，容易让人云里雾里，不知道这种算法到底在讲什么？重点又在哪里？ 图1 SIFT算法步骤 其实下载这篇文章之后打开看一下会发现，SIFT的思想并没有想的那么复杂，它主要包含4个步骤： 尺度空间极值检测：通过使用高斯差分函数来计算并搜索所有尺度上的图像位置，用于识别对尺度和方向不变的潜在兴趣点。 关键点定位：通过一个拟合精细的模型在每个候选位置上确定位置和尺度，关键点的选择依赖于它们的稳定程度。 方向匹配：基于局部图像的梯度方向，为每个关键点位置分配一个或多个方向，后续所有对图像数据的操作都是相对于关键点的方向、尺度和位置进行变换，从而而这些变换提供了不变形。 关键点描述：这个和HOG算法有点类似之处，在每个关键点周围的区域内以选定的比例计算局部图像梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。 由于它将图像数据转换为相对于局部特征的尺度不变坐标，因此这种方法被称为尺度不变特征变换。 如果对这个算法思路进行简化，它就包括2个部分： 特征提取 特征描述 特征提取特征点检测主要分为如下两个部分， 候选关键点 关键点定位 候选关键点 Koenderink（1984）和Lindeberg（1994）已经证明，在各种合理的假设下，高斯函数是唯一可能的尺度空间核。因此，图像的尺度空间被定义为函数，它是由一个可变尺度的高斯核和输入图像生成， 其中高斯核为， 为了有效检测尺度空间中稳定的极点，Lowe于1999年提出在高斯差分函数(DOG)中使用尺度空间极值与图像做卷积，这可以通过由常数乘法因子分隔的两个相邻尺度的差来计算。用公式表示就是， 由于平滑区域临近像素之间变化不大，但是在边、角、点这些特征较丰富的地方变化较大，因此通过DOG比较临近像素可以检测出候选关键点。 关键点定位 检测出候选关键点之后，下一步就是通过拟合惊喜的模型来确定位置和尺度。 2002年Brown提出了一种用3D二次函数来你和局部样本点，来确定最大值的插值位置，实验表明，这使得匹配和稳定性得到了实质的改进。 他的具体方法是对函数进行泰勒展开， 上述的展开式，就是所要的拟合函数。 极值点的偏移量为， 如果偏移量在任何一个维度上大于0.5时，则认为插值中心已经偏移到它的邻近点上，所以需要改变当前关键点的位置，同时在新的位置上重复采用插值直到收敛为止。如果超出预先设定的迭代次数或者超出图像的边界，则删除这个点。 特征描述前面讲了一些有关特征点检测的内容，但是SIFT实质的内容和价值并不在于特征点的检测，而是特征描述思想，这是它的核心所在，特征点描述主要包括如下两点： 方向分配 局部特征描述 方向分配 根据图像的图像，可以为每个关键定指定一个基准方向，可以相对于这个指定方向表示关键点的描述符，从而实现了图像的旋转不变性。 关键点的尺度用于选择尺度最接近的高斯平滑图像，使得计算是以尺度不变的方式执行，对每个图像，分别计算它的梯度幅值和梯度方向， 然后，使用方向直方图统计关键点邻域内的梯度幅值和梯度方向。将0~360度划分成36个区间，每个区间为10度，统计得出的直方图峰值代表关键点的主方向。 局部特征描述 通过前面的一系列操作，已经获得每个关键点的位置、尺度、方向，接下来要做的就是用已知特征向量把它描述出来，这是图像特征提取的核心部分。为了避免对光照、视角等因素的敏感性，需要特征描述子不仅仅包含关键点，还要包含它的邻域信息。 SIFT使用的特征描述子和后面要讲的HOG有很多相似之处。它一检测得到的关键点为中心，选择一个1616的邻域，然后再把这个邻域再划分为44的子区域，然后对梯度方向进行划分成8个区间，这样在每个子区域内疚会得到一个448=128维的特征向量，向量元素大小为每个梯度方向区间权值。提出得到特征向量后要对邻域的特征向量进行归一化，归一化的方向是计算邻域关键点的主方向，并将邻域旋转至根据主方向旋转至特定方向，这样就使得特征具有旋转不变性。然后再根据邻域内各像素的大小把邻域缩放到指定尺度，进一步使得特征描述子具有尺度不变性。 以上就是SIFT算法的核心部分。 编程实践本文代码已经放在github，感兴趣的可以自行查看， https://github.com/jakpopc/aiLearnNotes/blob/master/computer_vision/SIFT.pygithub.com 本文实现SIFT特征检测主要基于以下工具包： OpenCV numpy 其中OpenCV是一个非常知名且受欢迎的跨平台计算机视觉库，它不仅包含常用的图像读取、显示、颜色变换，还包含一些为人熟知的经典特征检测算法，其中就包括SIFT，所以本文使用OpenCV进行读取和SIFT特征检测。 numpy是一个非常优秀的数值计算库，也常用于图像的处理，这里使用numpy主要用于图像的拼接和显示。 导入工具包 12import numpy as npimport cv2 图像准备 首先写一下读取图像的函数， 123456def load_image(path, gray=True): if gray: img = cv2.imread(path) return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) else: return cv2.imread(path) 然后，生成一副对原图进行变换的图像，用于后面特征匹配，本文选择对图像进行垂直镜像变换， 1234567def transform(origin): h, w = origin.shape generate_img = np.zeros(origin.shape) for i in range(h): for j in range(w): generate_img[i, w - 1 - j] = origin[i, j] return generate_img.astype(np.uint8) 显示一下图像变换的结果， 12345img1 = load_image('2007_002545.jpg')img2 = transform(img1)combine = np.hstack((img1, img2))cv2.imshow("gray", combine)cv2.waitKey(0) 先用 xfeatures2d 模块实例化一个sift算子，然后使用 detectAndCompute 计算关键点和描述子，随后再用 drawKeypoints 绘出关键点， 12345678910111213# 实例化sift = cv2.xfeatures2d.SIFT_create()# 计算关键点和描述子# 其中kp为关键点keypoints# des为描述子descriptorskp1, des1 = sift.detectAndCompute(img1, None)kp2, des2 = sift.detectAndCompute(img2, None)# 绘出关键点# 其中参数分别是源图像、关键点、输出图像、显示颜色img3 = cv2.drawKeypoints(img1, kp1, img1, color=(0, 255, 255))img4 = cv2.drawKeypoints(img2, kp2, img2, color=(0, 255, 255)) 显示出检测的关键点为， 关键点已经检测出来，最后一步要做的就是绘出匹配效果，本文用到的是利用 FlannBasedMatcher 来显示匹配效果， 首先要对 FlannBasedMatcher 进行参数设计和实例化，然后用 *knn 对前面计算的出的特征描述子进行匹配，最后利用 drawMatchesKnn 显示匹配效果， 123456789101112131415161718# 参数设计和实例化index_params = dict(algorithm=1, trees=6)search_params = dict(checks=50)flann = cv2.FlannBasedMatcher(index_params, search_params)# 利用knn计算两个描述子的匹配matche = flann.knnMatch(des1, des2, k=2)matchesMask = [[0, 0] for i in range(len(matche))]# 绘出匹配效果result = []for m, n in matche: if m.distance &lt; 0.6 * n.distance: result.append([m])img5 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matche, None, flags=2)cv2.imshow("MatchResult", img5)cv2.waitKey(0) 检测结果， 完整代码如下， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npimport cv2def load_image(path, gray=False): if gray: img = cv2.imread(path) return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) else: return cv2.imread(path)def transform(origin): h, w, _ = origin.shape generate_img = np.zeros(origin.shape) for i in range(h): for j in range(w): generate_img[i, w - 1 - j] = origin[i, j] return generate_img.astype(np.uint8)def main(): img1 = load_image('2007_002545.jpg') img2 = transform(img1) # 实例化 sift = cv2.xfeatures2d.SIFT_create() # 计算关键点和描述子 # 其中kp为关键点keypoints # des为描述子descriptors kp1, des1 = sift.detectAndCompute(img1, None) kp2, des2 = sift.detectAndCompute(img2, None) # 绘出关键点 # 其中参数分别是源图像、关键点、输出图像、显示颜色 img3 = cv2.drawKeypoints(img1, kp1, img1, color=(0, 255, 255)) img4 = cv2.drawKeypoints(img2, kp2, img2, color=(0, 255, 255)) # 参数设计和实例化 index_params = dict(algorithm=1, trees=6) search_params = dict(checks=50) flann = cv2.FlannBasedMatcher(index_params, search_params) # 利用knn计算两个描述子的匹配 matche = flann.knnMatch(des1, des2, k=2) matchesMask = [[0, 0] for i in range(len(matche))] # 绘出匹配效果 result = [] for m, n in matche: if m.distance &lt; 0.6 * n.distance: result.append([m]) img5 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matche, None, flags=2) cv2.imshow("MatchResult", img5) cv2.waitKey(0)if __name__ == '__main__': main() 以上就是SIFT的完整内容。 往期回顾Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 Jackpop：【动手学计算机视觉】第三讲：图像预处理之图像分割 Jackpop：【动手学计算机视觉】第四讲：图像预处理之图像增广 Jackpop：【动手学计算机视觉】第五讲：传统目标检测之特征工程 Jackpop：【动手学计算机视觉】第六讲：传统目标检测之Harris角点检测]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第六讲：传统目标检测之Harris角点检测]]></title>
    <url>%2F2019%2F05%2F06%2F6-harris%2F</url>
    <content type="text"><![CDATA[前言在传统目标识别中，特征提取是最终目标识别效果好坏的一个重要决定因素，因此，在这项工作里，有很多研究者把主要精力都放在特征提取方向。在传统目标识别中，主要使用的特征主要有如下几类： 边缘特征 纹理特征 区域特征 角点特征 本文要讲述的Harris角点检测就是焦点特征的一种。 目前角点检测算法主要可归纳为3类： 基于灰度图像的角点检测 基于二值图像的角点检测 基于轮廓的角点检测 因为角点在现实生活场景中非常常见，因此，角点检测算法也是一种非常受欢迎的检测算法，尤其本文要讲的Harris角点检测，可以说传统检测算法中的经典之作。 Harris角点检测什么是角点？ 要想弄明白角点检测，首先要明确一个问题，什么是角点？ 这个在现实中非常常见，例如图中标记出的飞机的角点，除此之外例如桌角、房角等。这样很容易理解，但是该怎么用书面的语言阐述角点？ 角点就是轮廓之间的交点。 如果从数字图像处理的角度来描述就是：像素点附近区域像素无论是在梯度方向、还是在梯度幅值上都发生较大的变化。 这句话是焦点检测的关键，也是精髓，角点检测算法的思想就是由此而延伸出来的。 角点检测的算法思想是：选取一个固定的窗口在图像上以任意方向的滑动，如果灰度都有较大的变化，那么久认为这个窗口内部存在角点。 要想实现角点检测，需要用数学语言对其进行描述，下面就着重用数学语言描述一下角点检测算法的流程和原理。 用 $w(x,y)$ 表示窗口函数， $[u,v]$为窗口平移量，像素在窗口内的变化量为， E(u, v)=\sum_{x, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}其中 $I(x, y) $为平移前的像素灰度值， $I(x+u, y+v)$ 为平移后的像素灰度值， 通过对灰度变化部分进行泰勒展开得到， \begin{array}{c}{\sum[I(x+u, y+v)-I(x, y)]^{2}} \\ {\approx \sum\left[I(x, y)+u I_{x}+v I_{y}-I(x, y)\right]^{2} \\ =\sum u^{2} I_{x}^{2}+2 u v I_{x} I_{y}+v^{2} I_{y}^{2} \\ =\sum \left[ \begin{array}{ll}{u} & {v}\end{array}\right] \left[ \begin{array}{l}{I_{x}^{2}} & {I_{x} I_{y}} \\ {I_{x} I_{y}} & {I_{y}^{2}}\end{array}\right] \left[ \begin{array}{l}{u} \\ {v}\end{array}\right] \\ = \left[ \begin{array}{ll}{u} & {v}\end{array}\right] (\sum\left[ \begin{array}{l}{I_{x}^{2}} & {I_{x} I_{y}} \\ {I_{x} I_{y}} & {I_{y}^{2}}\end{array}\right]) \left[ \begin{array}{l}{u} \\ {v}\end{array}\right]} \end{array}因此得到， E(u, v) \cong[u, v] M \left[ \begin{array}{l}{u} \\ {v}\end{array}\right]矩阵 $M$ 中 $I_x$ 、 $I_y$ 分别是像素在 $x$ 、 $y$ 方向的梯度，从上述化简公式可以看出，灰度变化的大小主要取决于矩阵， M=\sum_{x, y} W(x, y) \left[ \begin{array}{cc}{I_{x}(x, y)^{2}} & {I_{x}(x, y) I_{y}(x, y)} \\ {I_{x}(x, y) I_{y}(x, y)} & {I_{y}(x, y)^{2}}\end{array}\right]现在在回过头来看一下角点与其他类型区域的不同之处： 平坦区域：梯度方向各异，但是梯度幅值变化不大 线性边缘：梯度幅值改变较大，梯度方向改变不大 角点：梯度方向和梯度幅值变化都较大 明白上述3点之后看一下怎么利用其矩阵 $M$ 进行角点检测。 根据主成分分析(PCA)的原理可知，如果对矩阵 $M$ 对角化，那么，特征值就是主分量上的方差，矩阵是二维的方阵，有两个主分量，如果在窗口区域内是角点，那么梯度变化会较大，像素点的梯度分布比较离散，这体现在特征值上就是特征值比较大。 换句话说， 如果矩阵对应的两个特征值都较大，那么窗口内含有角点 如果特征值一个大一个小，那么窗口内含有线性边缘 如果两个特征值都很小，那么窗口内为平坦区域 读到这里就应该明白了，角点的检测转化为数学模型，就是求解窗口内矩阵的特征值并且判断特征值的大小。 如果要评价角点的强度，可以用下方公式， R=\operatorname{det} M-k(\operatorname{trace} M)^{2} \tag{1}其中， \operatorname{det} M=\lambda_{1} \lambda_{2}\operatorname{trace} M=\lambda_{1}+\lambda_{2}编程实践因为Harris角点检测算法非常经典，因此，一些成熟的图像处理或视觉库都会直接提供Harris角点检测的算法，以OpenCV为例， 1234567891011121314151617import cv2import numpy as npfilename = '2007_000480.jpg'img = cv2.imread(filename)gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)gray = np.float32(gray)dst = cv2.cornerHarris(gray,blockSize,ksize,k)"""其中，gray为灰度图像，blockSize为邻域窗口的大小，ksize是用于Soble算子的参数，k是一个常量，取值为0.04~0.06""" 因为本文要讲一步一步实现Harris角点检测算法，因此，对OpenCV提供的函数不多阐述，下面开始一步一步实现Harris角点检测算法。 检点检测算法的流程如下： 利用公式(1)求出输入图像每个位置的角点强度响应 给定阈值，当一个位置的强度大于阈值则认为是角点 画出角点 首先是第一步，根据上述提到的公式求矩阵的特征值和矩阵的迹，然后计算图像的角点强度，这里选取常数k=0.04， 12345678910111213141516def calculate_corner_strength(img, scale=3, k=0.06): # 计算图像在x、y方向的梯度 # 用滤波器采用差分求梯度的方式 gradient_imx, gradient_imy = zeros(img.shape), zeros(img.shape) filters.gaussian_filter(img, (scale, scale), (0, 1), gradient_imx) filters.gaussian_filter(img, (scale, scale), (1, 0), gradient_imy) # 计算矩阵M的每个分量 I_xx = filters.gaussian_filter(gradient_imx*gradient_imx, scale) I_xy = filters.gaussian_filter(gradient_imx*gradient_imy, scale) I_yy = filters.gaussian_filter(gradient_imy*gradient_imy, scale) # 计算矩阵的迹、特征值和响应强度 det_M = I_xx * I_yy - I_xy ** 2 trace_M = I_xx + I_yy return det_M + k * trace_M ** 2 接下来完成第2步，根据给定阈值，获取角点， 12345678910111213141516171819def corner_detect(img, min=15, threshold=0.04): # 首先对图像进行阈值处理 _threshold = img.max() * threshold threshold_img = (img &gt; _threshold) * 1 coords = array(threshold_img.nonzero()).T candidate_values = [img[c[0], c[1]] for c in coords] index = argsort(candidate_values) # 选取领域空间，如果邻域空间距离小于min的则只选取一个角点 # 防止角点过于密集 neighbor = zeros(img.shape) neighbor[min:-min, min:-min] = 1 filtered_coords = [] for i in index: if neighbor[coords[i, 0], coords[i, 1]] == 1: filtered_coords.append(coords[i]) neighbor[(coords[i, 0] - min):(coords[i, 0] + min), (coords[i, 1] - min):(coords[i, 1] + min)] = 0 return filtered_coords 然后是画出角点， 1234567def corner_plot(image, filtered_coords): figure() gray() imshow(image) plot([p[1] for p in filtered_coords], [p[0] for p in filtered_coords], 'ro') axis('off') show() 检测结果， 完整代码如下， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from scipy.ndimage import filtersimport cv2from matplotlib.pylab import *class Harris(object): def __init__(self, img_path): self.img = cv2.imread(img_path, 0) def calculate_corner_strength(self): # 计算图像在x、y方向的梯度 # 用滤波器采用差分求梯度的方式 scale = self.scale k = self.k img = self.img gradient_imx, gradient_imy = zeros(img.shape), zeros(img.shape) filters.gaussian_filter(img, (scale, scale), (0, 1), gradient_imx) filters.gaussian_filter(img, (scale, scale), (1, 0), gradient_imy) # 计算矩阵M的每个分量 I_xx = filters.gaussian_filter(gradient_imx*gradient_imx, scale) I_xy = filters.gaussian_filter(gradient_imx*gradient_imy, scale) I_yy = filters.gaussian_filter(gradient_imy*gradient_imy, scale) # 计算矩阵的迹、特征值和响应强度 det_M = I_xx * I_yy - I_xy ** 2 trace_M = I_xx + I_yy return det_M + k * trace_M ** 2 def corner_detect(self, img): # 首先对图像进行阈值处理 _threshold = img.max() * self.threshold threshold_img = (img &gt; _threshold) * 1 coords = array(threshold_img.nonzero()).T candidate_values = [img[c[0], c[1]] for c in coords] index = argsort(candidate_values) # 选取领域空间，如果邻域空间距离小于min的则只选取一个角点 # 防止角点过于密集 neighbor = zeros(img.shape) neighbor[self.min:-self.min, self.min:-self.min] = 1 filtered_coords = [] for i in index: if neighbor[coords[i, 0], coords[i, 1]] == 1: filtered_coords.append(coords[i]) neighbor[(coords[i, 0] - self.min):(coords[i, 0] + self.min), (coords[i, 1] - self.min):(coords[i, 1] + self.min)] = 0 return filtered_coords def corner_plot(self, img, corner_img): figure() gray() imshow(img) plot([p[1] for p in corner_img], [p[0] for p in corner_img], 'ro') axis('off') show() def __call__(self, k=0.04, scale=3, min=15, threshold=0.03): self.k = k self.scale = scale self.min = min self.threshold = threshold strength_img = self.calculate_corner_strength() corner_img = self.corner_detect(strength_img) self.corner_plot(self.img, corner_img)if __name__ == '__main__': harris = Harris("2007_002619.jpg") harris() 我整理了一些计算机视觉、Python、强化学习、优化算法等方面的电子书籍、学习资料，同时还打包了一些我认为比较实用的工具，如果需要请关注公众号【平凡而诗意】，回复相应的关键字即可获取~ 更多我的作品Jackpop：学习资源：图像处理从入门到精通 Jackpop：Python调试神器之PySnooper Jackpop：【动手学计算机视觉】第五讲：传统目标检测之特征工程 Jackpop：是时候给大家推荐这款强大的神器了]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第五讲：传统目标检测之特征工程]]></title>
    <url>%2F2019%2F05%2F05%2F5-feature-engineering%2F</url>
    <content type="text"><![CDATA[前言随着2012年AlexNet在ImageNet挑战赛一举夺魁，让深度卷积网络迅速霸占了目标识别和计算机视觉的头把交椅。随后的VGG、R-CNN、SSD、YOLO让深度卷积网络在计算机视觉领域的地位更加稳固。 由于深度卷积网络在目标识别方面表现得太过于抢眼，所以，很多入门计算机视觉的同学会选择从深度学习切入，目前惯用的学习套路莫过于如下几条： 吴恩达《机器学习》《深度学习工程师》 李飞飞《cs231n》 clone开源代码 微调代码跑模型 …… 整个过程中很少涉及图像底层的内容，甚至知名的cs231课程对传统目标识别也未丝毫提及。 对于这个问题不难理解，深度学习与传统目标识别有着最根本的区别。 传统目标识别有两个非常重要的步骤：特征提取和机器学习。尤其是特征提取需要人为选取特征和特征后处理，特征的好坏对于识别的精确度有着至关重要的作用。而深度学习只需要对数据进行预处理输入到卷积神经网络中，特征提取由卷积神经网络自行完成，不需要人为干预特征的选取。 避免了人为特征提取的确给目标识别带来了质的飞跃，但是我认为计算机视觉依然脱离不了图像的范畴，它依然是一门以图像为根本的技术。目前深度计算机视觉模型的可迁移性差也体现出这一点，不能忽略不同类型图像之间的差异性。 例如， 自然图像 遥感图像 医学图像 三者之间有着巨大的差异性，在进行模型的学习过程中需要充分考虑不同类型图像的特点，这样对于模型的学习也有着非常大的益处。 特征工程 特征工程在传统目标识别中占据着举足轻重的地位，甚至可以说，特征工程做的好坏能够直接影响最终识别的精度。 特征工程主要包括三个部分： 数据预处理 特征提取 特征后处理 数据预处理主要用一些手段和技术对数据做一下处理： 无量纲化 缺失值 归一化 标准化 …… 这项工作基础而且重要，数据的准确性是识别效果的前提条件。 特征后处理主要包括如下几项： 主成分分析 奇异值分解 线性判别分析 目前常用的特征后处理手段就是对特征进行降维，由特征降维主要有如下几项优点： 降低计算开销 获取有价值的信息 去除噪声 使得数据更加易用 …… 关于图像预处理的内容前面已经用几讲进行阐述，这里就不再阐述。关于特征降维的知识后续会详细介绍，本阶段主要围绕特征提取进行讲解。 特征提取特征提取主要的目的是在图像中提取出一些有价值的信息，在传统目标识别中所占地位丝毫不亚于支持向量机、Adaboost这类机器学习算法。在特征选择的过程中需要充分考虑目标的相关性，这样才能提取更加能够描述目标类别的特征，进而影响到目标检测的精度。 目前的特征种类有非常多，颜色特征、纹理特征、区域特征、边缘特征等，本文不过多介绍这类概念性的内容，主要概括一些常用的特征描述子，后续文章会逐个对这些经典的特征提取算法进行展开和详解。 传统目标识别中常用的特征描述子有： Harris SIFT SURF LBP HOG DPM Harris是一种角点特征描述子，角点对应于物体的拐角，道路的十字路口、丁字路口等，在现实中非常常见，因此，Harris一直以来都是一个非常热门的特征检测算法。 Harris角点特征 SIFT，即尺度不变特征变换（Scale-invariant feature transform，SIFT），该方法于1999年由David Lowe发表在ICCV。由于该算法对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性，使其备受关注， SIFT提取特征点 SIFT有着非常多的优点，但是也有一点致命的缺陷—实时性不足。SURF（Speeded Up Robust Features）改进了特征的提取和描述方式，用一种更为高效的方式完成特征的提取和描述。 SURF提取特征点 方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子，这个名气就更大了。它是一种基于统计的特征提取算法，通过统计不同梯度方向的像素而获取图像的特征向量。 HOG特征 LBP（Local Binary Pattern，局部二值模式），它是首先由T. Ojala, M.Pietikäinen,和 D. Harwood 在1994年提出，是一种纹理特征描述算子，旋转不变性和灰度不变性等显著的优点。 LBP特征 DPM(Discriminatively Trained Part Based Models)是Felzenszwalb力作，作者在这个算法中提出了很多对后续目标识别甚至深度学习影响深远的思想，作者也因此一举获得VOC挑战赛的终身成就奖。 DPM算法 由于时间问题，本文仅仅是概括性的介绍一下传统目标检测中常用的特征描述子，后续会单独对每个算法详细展开并一步一步编程实践。 往期回归Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 Jackpop：【动手学计算机视觉】第三讲：图像预处理之图像分割 Jackpop：【动手学计算机视觉】第四讲：图像预处理之图像增广 感兴趣的可以关注一下，也可以关注公众号”平凡而诗意”，我在公众号共享了一些资源和学习资料，关注后回复相应关键字可以获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第四讲：图像预处理之图像增广]]></title>
    <url>%2F2019%2F05%2F04%2F4-image-augmented%2F</url>
    <content type="text"><![CDATA[前言近几年深度学习的大规模成功应用主要的就是得益于数据的累积和算例的提升，虽然近几年很多研究者竭力的攻克半监督和无监督学习，减少对大量数据的依赖，但是目前数据在计算机视觉、自然语言处理等人工智能技术领域依然占据着非常重要的地位。甚至可以说，大规模的数据是计算机视觉成功应用的前提条件。但是由于种种原因导致数据的采集变的十分困难，因此图像增广技术就在数据的准备过程中占据着举足轻重的角色，本文就概括一下常用的图像增广技术并编程实现相应手段。 介绍图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。 目前常用的图像增广技术有如下几种： 镜像变换 旋转 缩放 裁剪 平移 亮度修改 添加噪声 剪切 变换颜色 在图像增广过程中可以使用其中一种手段进行扩充，也可以使用其中的几种方法进行组合使用，由于概念比较简单，容易理解，所以接下来就边实现，边详细阐述理论知识。 几何变换首先以水平镜像为例，假设在原图中像素的坐标为，在镜像变换之后的图像中的坐标为，原图像坐标和镜像变换后的坐标之间的关系式： \left\{ \begin{aligned} x_1 &=& w-1-x_0 \\ y_1 &=& y_0 \end{aligned} \right.其中 w 为图像的宽度。 那么两张图像的关系就是： \left[ \begin{array}{c}{x_{1}} \\ {y_{1}} \\ {1}\end{array}\right]=\left[ \begin{array}{ccc}{-1} & {0} & {w-1} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{l}{x_{0}} \\ {y_{0}} \\ {1}\end{array}\right]它的逆变换就是 \left[ \begin{array}{c}{x_{0}} \\ {y_{0}} \\ {1}\end{array}\right]=\left[ \begin{array}{ccc}{-1} & {0} & {w-1} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{l}{x_{1}} \\ {y_{1}} \\ {1}\end{array}\right]从原图到水平镜像的变换矩阵就是： \left[ \begin{array}{ccc}{-1} & {0} & {w-1} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right]同理，可知，垂直镜像变换的关系式为： \left[ \begin{array}{ccc}{-1} & {0} & {w-1} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right]其中为图像高度。 通过上述可以知道，平移变换的数学矩阵为： H=\left[ \begin{array}{lll}{1} & {0} & {d_{x}} \\ {0} & {1} & {d_{y}} \\ {0} & {0} & {1}\end{array}\right] 其中和分别是像素在水平和垂直方向移动的距离。 同理可以推广到旋转变换上，加上原像素的坐标为 )(x_0,y_0) ，该像素点相对于原点的角度为，假设有一个半径为的圆，那么原像素的坐标可以表示为： \left\{\begin{array}{l}{x_{0}=r \cos \alpha} \\ {y_{0}=r \cos \alpha}\end{array}\right.加上旋转后的像素坐标为 )(x_1,y_1) ，旋转角度为 \theta ，那么可以表示为： \left\{\begin{array}{l}{x_1=r \cos (\alpha+\theta)} \\ {y_1=r \sin (\alpha+\theta)}\end{array}\right.通过展开、化简可得， \left\{\begin{array}{l}{x_1=r \cos (\alpha+\theta)=r \cos \alpha \cos \theta-r \sin \alpha \sin \theta=x_{0} \cos \theta-y_{0} \sin \theta} \\ {y_1=r \sin (\alpha+\theta)=r \sin \alpha \cos \theta+r \cos \alpha \sin \theta=x_{0} \sin \theta+y_{0} \cos \theta}\end{array}\right.把上述公式写成数学矩阵形式为： \left[ \begin{array}{l}{x_1} \\ {y_1} \\ {1}\end{array}\right]=\left[ \begin{array}{ccc}{\cos \theta} & {-\sin \theta} & {0} \\ {\sin \theta} & {\cos \theta} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{l}{x_{0}} \\ {y_{0}} \\ {1}\end{array}\right]因此旋转变换的矩阵为： H=\left[ \begin{array}{ccc}{\cos \theta} & {-\sin \theta} & {0} \\ {\sin \theta} & {\cos \theta} & {0} \\ {0} & {0} & {1}\end{array}\right]其他的几何变换方式和上述提到的核心思想大同小异，因此，就不再详细展开，感兴趣的可以在网上搜集一下，或者看一下数字图像处理相关的书籍，关注这些内容的讲解有很多。 编程实践 编程实践过程中主要用到opencv、numpy和skimage。 读取图像： 1234# 1. 读取图像img = cv2.imread("./data/000023.jpg")cv2.imshow("Origin", img)cv2.waitKey() 初始化一个矩阵，用于存储转化后的图像， 1generate_img = np.zeros(img.shape) 1.水平镜像 遍历图像的像素，用前文提到的数学关系式进行像素的转化， 123456for i in range(h): for j in range(w): generate_img[i, w - 1 - j] = img[i, j]cv2.imshow("Ver", generate_img.astype(np.uint8))cv2.waitKey() 备注：初始化的图像数据类型是numpy.float64，用opencv显示时无法正常显示，因此在显示时需要用astype(np.uint8)把图像转化成numpy.uint8数据格式。 2.垂直镜像 垂直镜像变换代码， 123for i in range(h): for j in range(w): generate_img[h-1-i, j] = img[i, j] 镜像变换也可以直接调用opencv的flip进行使用。 3.图像缩放 这个比较简单，直接调用opencv的resize函数即可， 1output = cv2.resize(img, (100, 300)) 4.旋转变换 这个相对复杂一些，需要首先用getRotationMatrix2D函数获取一个旋转矩阵，然后调用opencv的warpAffine仿射函数安装旋转矩阵对图像进行旋转变换， 12center = cv2.getRotationMatrix2D((w/2, h/2), 45, 1)rotated_img = cv2.warpAffine(img, center, (w, h)) 5. 平移变换 首先用numpy生成一个平移矩阵，然后用仿射变换函数对图像进行平移变换， 12move = np.float32([[1, 0, 100], [0, 1, 100]])move_img = cv2.warpAffine(img, move, (w, h)) 6.亮度变换 亮度变换的方法有很多种，本文介绍一种叠加图像的方式，通过给原图像叠加一副同样大小，不同透明度的全零像素图像来修改图像的亮度， 12alpha = 1.5light = cv2.addWeighted(img, alpha, np.zeros(img.shape).astype(np.uint8), 1-alpha, 3) 其中alpha是原图像的透明度， 7.添加噪声 首先写一下噪声添加的函数，原理就是给图像添加一些符合正态分布的随机数， 123456789def add_noise(img): img = np.multiply(img, 1. / 255, dtype=np.float64) mean, var = 0, 0.01 noise = np.random.normal(mean, var ** 0.5, img.shape) img = convert(img, np.floating) out = img + noise return out 8.组合变换 除了以上方法单独使用之外，还可以叠加其中多种方法进行组合使用，比如可以结合选择、镜像进行使用， 完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import cv2import numpy as npfrom skimage.util.dtype import convertclass ImageAugmented(object): def __init__(self, path="./data/000023.jpg"): self.img = cv2.imread(path) self.h, self.w = self.img.shape[0], self.img.shape[1] # 1. 镜像变换 def flip(self, flag="h"): generate_img = np.zeros(self.img.shape) if flag == "h": for i in range(self.h): for j in range(self.w): generate_img[i, self.h - 1 - j] = self.img[i, j] else: for i in range(self.h): for j in range(self.w): generate_img[self.h-1-i, j] = self.img[i, j] return generate_img # 2. 缩放 def _resize_img(self, shape=(100, 300)): return cv2.resize(self.img, shape) # 3. 旋转 def rotated(self): center = cv2.getRotationMatrix2D((self.w / 2, self.h / 2), 45,1) return cv2.warpAffine(self.img, center, (self.w, self.h)) # 4. 平移 def translation(self, x_scale=100, y_scale=100): move = np.float32([[1, 0, x_scale], [0, 1, y_scale]]) return cv2.warpAffine(self.img, move, (self.w, self.h)) # 5. 改变亮度 def change_light(self, alpha=1.5, scale=3): return cv2.addWeighted(self.img, alpha, np.zeros(self.img.shape).astype(np.uint8), 1-alpha, scale) # 6. 添加噪声 def add_noise(self, mean=0, var=0.01): img = np.multiply(self.img, 1. / 255, dtype=np.float64) noise = np.random.normal(mean, var ** 0.5, img.shape) img = convert(img, np.floating) out = img + noise return out 往期回顾Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 Jackpop：【动手学计算机视觉】第三讲：图像预处理之图像分割 感兴趣的可以关注一下，也可以关注公众号”平凡而诗意”，我在公众号共享了一些资源和学习资料，关注后回复相应关键字可以获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第三讲：图像预处理之图像分割]]></title>
    <url>%2F2019%2F05%2F03%2F3-image-segmentation%2F</url>
    <content type="text"><![CDATA[前言图像分割是一种把图像分成若干个独立子区域的技术和过程。在图像的研究和应用中，很多时候我们关注的仅是图像中的目标或前景(其他部分称为背景)，它们对应图像中特定的、具有独特性质的区域。为了分割目标，需要将这些区域分离提取出来，在此基础上才有可能进一步利用，如进行特征提取、目标识别。因此，图像分割是由图像处理进到图像分析的关键步骤，在图像领域占据着至关重要的地位。 介绍提到图像分割，主要包含两个方面： 非语义分割 语义分割 首先，介绍一下非语义分割。 非语义分割在图像分割中所占比重更高，目前算法也非常多，研究时间较长，而且算法也比较成熟，此类图像分割目前的算法主要有以下几种： 阈值分割 阈值分割是图像分割中应用最多的一类，该算法思想比较简单，给定输入图像一个特定阈值，如果这个阈值可以是灰度值，也可以是梯度值，如果大于这个阈值，则设定为前景像素值，如果小于这个阈值则设定为背景像素值。 阈值设置为100对图像进行分割： 区域分割 区域分割算法中比较有代表性的算法有两种：区域生长和区域分裂合并。 区域生长算法的核心思想是给定子区域一个种子像素，作为生长的起点，然后将种子像素周围邻域中与种子像素有相同或相似性质的像素(可以根据预先设定的规则，比如基于灰度差)合并到种子所在的区域中。 区域分裂合并基本上就是区域生长的逆过程，从整个图像出发，不断分裂得到各个子区域，然后再把前景区域合并，实现目标提取。 聚类 聚类是一个应用非常广泛的无监督学习算法，该算法在图像分割领域也有较多的应用。聚类的核心思想就是利用样本的相似性，把相似的像素点聚合成同一个子区域。 边缘分割 这是图像分割中较为成熟，而且较为常用的一类算法。边缘分割主要利用图像在边缘处灰度级会发生突变来对图像进行分割。常用的方法是利用差分求图像梯度，而在物体边缘处，梯度幅值会较大，所以可以利用梯度阈值进行分割，得到物体的边缘。对于阶跃状边缘，其位置对应一阶导数的极值点，对应二阶导数的过零点(零交叉点)。因此常用微分算子进行边缘检测。常用的一阶微分算子有Roberts算子、Prewitt算子和Sobel算子，二阶微分算子有Laplace算子和Kirsh算子等。由于边缘和噪声都是灰度不连续点，在频域均为高频分量，直接采用微分运算难以克服噪声的影响。因此用微分算子检测边缘前要对图像进行平滑滤波。LoG算子和Canny算子是具有平滑功能的二阶和一阶微分算子，边缘检测效果较好，因此Canny算子也是应用较多的一种边缘分割算法。 直方图 与前面提到的算法不同，直方图图像分割算法利用统计信息对图像进行分割。通过统计图像中的像素，得到图像的灰度直方图，然后在直方图的波峰和波谷是用于定位图像中的簇。 水平集 水平集方法最初由Osher和Sethian提出，目的是用于界面追踪。在90年代末期被广泛应用在各种图像领域。这一方法能够在隐式有效的应对曲线/曲面演化问题。基本思想是用一个符号函数表示演化中的轮廓（曲线或曲面），其中符号函数的零水平面对应于实际的轮廓。这样对应于轮廓运动方程，可以容易的导出隐式曲线/曲面的相似曲线流，当应用在零水平面上将会反映轮廓自身的演化。水平集方法具有许多优点：它是隐式的，参数自由的，提供了一种估计演化中的几何性质的直接方法，能够改变拓扑结构并且是本质的。 语义分割和非语义分割的共同之处都是要分割出图像中物体的边缘，但是二者也有本质的区别，用通俗的话介绍就是非语义分割只想提取物体的边缘，但是不关注目标的类别。而语义分割不仅要提取到边缘像素级别，还要知道这个目标是什么。因此，非语义分割是一种图像基础处理技术，而语义分割是一种机器视觉技术，难度也更大一些，目前比较成熟且应用广泛的语义分割算法有以下几种： Grab cut Mask R-CNN U-Net FCN SegNet 由于篇幅有限，所以在这里就展开介绍语义分割，后期有时间会单独对某几个算法进行详细解析，本文主要介绍非语义分割算法，本文就以2015年UCLA提出的一种新型、高效的图像分割算法—相位拉伸变换为例，详细介绍一下，并从头到尾实现一遍。 相位拉伸变换相位拉伸变换(Phase Stretch Transform, PST)，是UCLA JalaliLab于2015年提出的一种新型图像分割算法[Edge Detection in Digital Images Using Dispersive Phase Stretch Transform]，该算法主要有两个显著优点： 速度快 精度高 思想简单 实现容易 PST算法中，首先使用定位核对原始图像进行平滑，然后通过非线性频率相关（离散）相位操作，称为相位拉伸变换(PST)。 PST将2D相位函数应用于频域中的图像。施加到图像的相位量取决于频率;也就是说，较高的相位量被应用于图像的较高频率特征。由于图像边缘包含更高频率的特征，因此PST通过将更多相位应用于更高频率的特征来强调图像中的边缘信息。可以通过对PST输出相位图像进行阈值处理来提取图像边缘。在阈值处理之后，通过形态学操作进一步处理二值图像以找到图像边缘。思想主要包含三个步骤： 非线性相位离散化 阈值化处理 形态学运算 下面来详细介绍一下。 相位拉伸变换，核心就是一个公式， A[n, m]=\angle(IFFT2(\tilde{K}[p, q]\cdot\tilde{L}[p, q]\cdot FFT2(B[n, m]))) \tag{1}其中 $B[n, m]$ 为输入图像， $m,n$ 为图像维数， $A[n, m]$ 为输出图像， $\angle$ 为角运算， $FFT2$ 为快速傅里叶变换， $IFFT2$ 为逆快速傅里叶变换，$p$和 $q$ 是二维频率变量， $\tilde{L}[p, q]$ 为局部频率响应核，通俗的讲，就是一个用于图像平滑、去噪的滤波核，论文中没有给出，可以使用一些用于图像平滑的滤波核代替， $\tilde{K}[p, q]$ 为相位核，其中， \tilde{K}[p, q]=e^{j\cdot\varphi[p,q]}\varphi[p,q]=\varphi_{polar}[r,\theta] \\=\varphi_{polar}[r]\\=S\cdot\frac{W\cdot r \cdot tan^{-1}(W \cdot r)-(1/2)\cdot ln(1+(W \cdot r)^2)}{W \cdot r_{max} \cdot tan^{-1}(W \cdot r_{max})-(1/2)ln(1+W \cdot r_{max})^2} \tag{2}$r=\sqrt{p^2+q^2}$， $\theta=tan^{-1}(q/p)$， $S$ 和 $W$ 是施加到图像相位的强度和扭曲，是影响图像分割效果的两个重要参数。 编程实践PST算法中最核心的就是公式(1)，编程实现可以一步一步来实现公式中的每个模块。 首先导入需要的模块， 12345import os import numpy as npimport mahotas as mhimport matplotlib.pylab as pltimport cv2 定义全局变量， 123456L = 0.5 S = 0.48 W= 12.14Threshold_min = -1Threshold_max = 0.0019FLAG = 1 计算公式中的核心参数， r，\theta , 1234def cart2pol(x, y): theta = np.arctan2(y, x) rho = np.hypot(x, y) return theta, rho 生成变量 $p$ 和 $q$ , 12345x = np.linspace(-L, L, img.shape[0])y = np.linspace(-L, L, img.shape[1])X, Y = np.meshgrid(x, y)p, q = X.T, y.Ttheta, rho = cart2pol(p, q) 接下来对公式(1)从右至左依次实现， 对输入图像进行快速傅里叶变换, 1orig = np.fft.fft2(img) 实现 $\tilde{L}[p, q]$， 1expo = np.fft.fftshift(np.exp(-np.power((np.divide(rho, math.sqrt((LPF ** 2) / np.log(2)))), 2))) 对图像进行平滑处理， 1orig_filtered = np.real(np.fft.ifft2((np.multiply(orig, expo)))) 实现相位核， 12PST_Kernel_1 = np.multiply(np.dot(rho, W), np.arctan(np.dot(rho, W))) - 0.5 * np.log(1 + np.power(np.dot(rho, W), 2))PST_Kernel = PST_Kernel_1 / np.max(PST_Kernel_1) * S 将前面实现的部分与相位核做乘积， 1temp = np.multiply(np.fft.fftshift(np.exp(-1j * PST_Kernel)), np.fft.fft2(orig_filtered)) 对图像进行逆快速傅里叶变换， 12temp = np.multiply(np.fft.fftshift(np.exp(-1j * PST_Kernel)), np.fft.fft2(Image_orig_filtered))orig_filtered_PST = np.fft.ifft2(temp) 进行角运算，得到变换图像的相位， 1PHI_features = np.angle(Image_orig_filtered_PST) 对图像进行阈值化处理， 1234features = np.zeros((PHI_features.shape[0], PHI_features.shape[1]))features[PHI_features &gt; Threshold_max] = 1 features[PHI_features &lt; Threshold_min] = 1 features[I &lt; (np.amax(I) / 20)] = 0 应用二进制形态学操作来清除转换后的图像, 12345out = featuresout = mh.thin(out, 1)out = mh.bwperim(out, 4)out = mh.thin(out, 1)out = mh.erode(out, np.ones((1, 1))) 到这里就完成了相位拉伸变换的核心部分， 12345678910111213141516171819202122232425262728293031def phase_stretch_transform(img, LPF, S, W, threshold_min, threshold_max, flag): L = 0.5 x = np.linspace(-L, L, img.shape[0]) y = np.linspace(-L, L, img.shape[1]) [X1, Y1] = (np.meshgrid(x, y)) X = X1.T Y = Y1.T theta, rho = cart2pol(X, Y) orig = ((np.fft.fft2(img))) expo = np.fft.fftshift(np.exp(-np.power((np.divide(rho, math.sqrt((LPF ** 2) / np.log(2)))), 2))) orig_filtered = np.real(np.fft.ifft2((np.multiply(orig, expo)))) PST_Kernel_1 = np.multiply(np.dot(rho, W), np.arctan(np.dot(rho, W))) - 0.5 * np.log( 1 + np.power(np.dot(rho, W), 2)) PST_Kernel = PST_Kernel_1 / np.max(PST_Kernel_1) * S temp = np.multiply(np.fft.fftshift(np.exp(-1j * PST_Kernel)), np.fft.fft2(orig_filtered)) orig_filtered_PST = np.fft.ifft2(temp) PHI_features = np.angle(orig_filtered_PST) if flag == 0: out = PHI_features else: features = np.zeros((PHI_features.shape[0], PHI_features.shape[1])) features[PHI_features &gt; threshold_max] = 1 features[PHI_features &lt; threshold_min] = 1 features[img &lt; (np.amax(img) / 20)] = 0 out = features out = mh.thin(out, 1) out = mh.bwperim(out, 4) out = mh.thin(out, 1) out = mh.erode(out, np.ones((1, 1))) return out, PST_Kernel 下面完成调用部分的功能， 首先读取函数并把图像转化为灰度图， 12345Image_orig = mh.imread("./cameraman.tif")if Image_orig.ndim == 3: Image_orig_grey = mh.colors.rgb2grey(Image_orig) else: Image_orig_grey = Image_orig 调用前面的函数，对图像进行相位拉伸变换， 1edge, kernel = phase_stretch_transform(Image_orig_grey, LPF, Phase_strength, Warp_strength, Threshold_min, Threshold_max, Morph_flag) 显示图像， 1234Overlay = mh.overlay(Image_orig_grey, edge)edge = edge.astype(np.uint8)*255plt.imshow(Edge)plt.show() 主函数的完整内容为， 123456789101112def main(): Image_orig = mh.imread("./cameraman.tif") if Image_orig.ndim == 3: Image_orig_grey = mh.colors.rgb2grey(Image_orig) else: Image_orig_grey = Image_orig edge, kernel = phase_stretch_transform(Image_orig_grey, LPF, S, W, Threshold_min, Threshold_max, FLAG) Overlay = mh.overlay(Image_orig_grey, Edge) Edge = Edge.astype(np.uint8)*255 plt.imshow(Edge) plt.show() 往期回顾Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 !img](https://pic2.zhimg.com/v2-4da90f547b115c1a398b3cc516eada3d_b.png) 感兴趣的可以关注一下，也可以关注公众号”平凡而诗意”，我在公众号共享了一些资源和学习资料，关注后回复相应关键字可以获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第二讲：图像预处理之图像增强]]></title>
    <url>%2F2019%2F05%2F02%2F2-image-enhancement%2F</url>
    <content type="text"><![CDATA[前言图像增强是图像处理中一种常用的技术，它的目的是增强图像中全局或局部有用的信息。合理利用图像增强技术能够针对性的增强图像中感兴趣的特征，抑制图像中不感兴趣的特征，这样能够有效的改善图像的质量，增强图像的特征。 介绍计算机视觉主要有两部分组成： 特征提取 模型训练 其中第一条特征提取在计算机视觉中占据着至关重要的位置，尤其是在传统的计算机视觉算法中，更为明显，例如比较著名的HOG、DPM等目标识别模型，主要的研究经历都是在图像特征提取方面。图像增强能够有效的增强图像中有价值的信息，改善图像质量，能够满足一些特征分析的需求，因此，可以用于计算机视觉数据预处理中，能够有效的改善图像的质量，进而提升目标识别的精度。 图像增强可以分为两类： 频域法 空间域法 首先，介绍一下频域法，顾名思义，频域法就是把图像从空域利用傅立叶、小波变换等算法把图像从空间域转化成频域，也就是把图像矩阵转化成二维信号，进而使用高通滤波或低通滤波器对信号进行过滤。采用低通滤波器（即只让低频信号通过）法，可去掉图中的噪声；采用高通滤波法，则可增强边缘等高频信号，使模糊的图片变得清晰。 其次，介绍一下空域方法，空域方法用的比较多，空域方法主要包括以下几种常用的算法： 直方图均衡化 滤波 直方图均衡化直方图均衡化的作用是图像增强，这种方法对于背景和前景都太亮或者太暗的图像非常有用。直方图是一种统计方法，根据对图像中每个像素值的概率进行统计，按照概率分布函数对图像的像素进行重新分配来达到图像拉伸的作用，将图像像素值均匀分布在最小和最大像素级之间。 具体原理和示例如下： 原图像向新图像的映射为： s_k = \sum_{j=0}^{k}{\frac{n_j}{n}}, k=0, 1, ...,L-1其中 L 为灰度级。 用直白的语言来描述：把像素按从小到大排序，统计每个像素的概率和累计概率，然后用灰度级乘以这个累计概率就是映射后新像素的像素值。 假设一幅图像像素分布如图，像素级为255，利用直方图对像素分布进行统计，示例如下： 滤波基于滤波的算法主要包括以下几种： 均值滤波 中值滤波 高斯滤波 这些方法主要用于图像平滑和去噪，在前一讲中已经阐述，感兴趣的可以看一下【动手学计算机视觉】第一讲：图像预处理之图像去噪。 编程实践123完整代码地址： https://github.com/jakpopc/aiLearnNotes/blob/master/computer_vision/image_enhancement.py requirement:matplotlib/opencv 本文主要介绍直方图均衡化图像增强算法，前一讲已经实现了滤波法，需要的可以看一下。 首先利用opencv读取图像并转化为灰度图，图像来自于voc2007: 12img = cv2.imread("../data/2007_000793.jpg")gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) 可以显示图像的灰度直方图： 123456789def histogram(gray): hist = cv2.calcHist([gray], [0], None, [256], [0.0, 255.0]) plt.plot(range(len(hist)), hist)# opencv calcHist函数传入5个参数：# images：图像# channels：通道# mask：图像掩码，可以填写None# hisSize：灰度数目# ranges：回复分布区间 直方图均衡化，这里使用opencv提供的函数： 1dst = cv2.equalizeHist(gray) 均衡化后的图像为： 可以从上图看得出，图像的对比度明显比原图像更加清晰。 往期回顾Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 感兴趣的可以关注一下，也可以关注公众号”平凡而诗意”，我在公众号共享了一些资源和学习资料，关注后回复相应关键字可以获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第一讲：图像预处理之图像去噪]]></title>
    <url>%2F2019%2F05%2F01%2F1-image-denoising%2F</url>
    <content type="text"><![CDATA[前言很多人想入门AI，可是AI包含很多方向，我建议首先应该明确的选择一个方向，然后有目标、有针对的去学习。 计算机视觉作为目前AI领域研究较多、商业应用较为成功的一个方向，这几年也是非常火热，无论是学术界还是企业界，学术界有CVPR、ICCV、ECCV等顶刊，企业界对计算机视觉领域的人口需求也非常的大，因此，我从计算机视觉这个方向开始着手AI教程。 介绍最近几年计算机视觉非常火，也出现了很多成熟的卷积神经网络模型，比如R-CNN系列、SSD、YOLO系列，而且，这些模型在github上也有很多不错的开源代码，所以，很多入门计算机视觉的人会早早的克隆下开源代码、利用tensorflow或pytorch搭建计算机视觉平台进行调试。 我个人不推崇这种方式，我更推崇对图像底层的技术有一些了解，比如图像去噪、图像分割等技术，这有几点好处： 对图像内部的结构有更清晰的认识 这些技术可以用于计算机视觉预处理或后处理，能够有助于提高计算机视觉模型精度 第一讲，我从图像去噪开始说起，图像去噪是指减少图像中造成的过程。现实中的图像会受到各种因素的影响而含有一定的噪声，噪声主要有以下几类： 椒盐噪声 加性噪声 乘性噪声 高斯噪声 图像去噪的方法有很多种，其中均值滤波、中值滤波等比较基础且成熟，还有一些基于数学中偏微分方程的去噪方法，此外，还有基于频域的小波去噪方法。均值滤波、中值滤波这些基础的去噪算法以其快速、稳定等特性，在项目中非常受欢迎，在很多成熟的软件或者工具包中也集成了这些算法，下面，我们就来一步一步实现以下。 编程实践123完整代码地址：https://github.com/jakpopc/aiLearnNotes/blob/master/computer_vision/image_denoising.pyrequirement:scikit-image/opencv/numpy 首先读取图像，图像来自于voc2007: 123img = cv2.imread("../data/2007_001458.jpg")cv2.imshow("origin_img", img)cv2.waitKey() 生成噪声图像，就是在原来图像上加上一些分布不规律的像素值，可以自己用随机数去制造噪声，在这里，就用Python第三方库scikit-image的random_noise添加噪声： 方法1： 1noise_img = skimage.util.random_noise(img, mode="gaussian") mode是可选参数：分别有’gaussian’、’localvar’、’salt’、’pepper’、’s&amp;p’、’speckle’，可以选择添加不同的噪声类型。 方法2： 也可以自己生成噪声，与原图像进行加和得到噪声图像： 123456789def add_noise(img): img = np.multiply(img, 1. / 255, dtype=np.float64) mean, var = 0, 0.01 noise = np.random.normal(mean, var ** 0.5, img.shape) img = convert(img, np.floating) out = img + noise return out 最后是图像去噪，图像去噪的算法有很多，有基于偏微分热传导方程的，也有基于滤波的，其中基于滤波的以其速度快、算法成熟，在很多工具包中都有实现，所以使用也就较多，常用的滤波去噪算法有以下几种： 中值滤波 均值滤波 高斯滤波 滤波的思想和这两年在计算机视觉中用的较多的卷积思想类似，都涉及窗口运算，只是卷积是用一个卷积核和图像中对应位置做卷积运算，而滤波是在窗口内做相应的操作， 以均值滤波为例， 对图像中每个像素的像素值进行重新计算，假设窗口大小ksize=3，图像中棕色的”5”对应的像素实在33的邻域窗口内进行计算，对于均值滤波就是求33窗口内所有像素点的平均值，也就是 \frac{1+2+3+4+6+7+8+9}{9}=4.4同理，对于中值滤波就是把窗口内像素按像素值大小排序求中间值，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到， 下面开始编写去噪部分的代码： 方法1： 可以使用opencv这一类工具进行去噪： 123456# 中值滤波denoise = cv2.medianBlur(img, ksize=3)# 均值滤波denoise = cv2.fastNlMeansDenoising(img, ksize=3)# 高斯滤波denoise = cv2.GaussianBlur(img, ksize=3) 方法2： 编程一步一步实现图像去噪，首先是计算窗口邻域内的值，这里以计算中值为例： 123456def compute_pixel_value(img, i, j, ksize, channel): h_begin = max(0, i - ksize // 2) h_end = min(img.shape[0], i + ksize // 2) w_begin = max(0, j - ksize // 2) w_end = min(img.shape[1], j + ksize // 2) return np.median(img[h_begin:h_end, w_begin:w_end, channel]) 然后是去噪部分，对每个像素使用compute_pixel_value函数计算新像素的值： 12345678def denoise(img, ksize): output = np.zeros(img.shape) for i in range(img.shape[0]): for j in range(img.shape[1]): output[i, j, 0] = compute_pixel_value(img, i, j, ksize, 0) output[i, j, 1] = compute_pixel_value(img, i, j, ksize, 1) output[i, j, 2] = compute_pixel_value(img, i, j, ksize, 2) return output 感兴趣的可以关注公众号”平凡而诗意”，共享了一些机器学习、计算机视觉等方面的教材电子版、文章、资源等。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
</search>
