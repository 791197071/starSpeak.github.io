<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[开发工具 | 你真的会用jupyter吗？]]></title>
    <url>%2F2019%2F09%2F14%2Fjupyter%2F</url>
    <content type="text"><![CDATA[前言 提起jupyter notebook，应该很多学习过Python的同学都不模型。虽然用jupyter notebook的同学相对较少，但是提及这款开发工具，很多人都会赞不绝口，“jupyter很强大，交互式、富文本”，很多人都知道jupyter notebook的这几个优点，但是，试问一下，你真的会用jupyter吗？ 以Python开发为例，我们只需要在windows命令行或者linux/Mac终端输入“jupyter notebook”或者“ipython notebook”即可使用默认浏览器打开一个在线IDE， ![nffQun.png](https://s2.ax1x.com/2019/09/16/nffQun.png) 首先说一下交互式， jupyter notebook中一个非常重要的概念就是cell，每个cell能够单独进行运算，这样适合于代码调试。我们开发一个完整的脚本时变量会随着代码执行的结束而从内存中释放，如果我们想看中间的变量或者结构，我们只能通过断点或者输出日志信息的方式进行调试，这样无疑是非常繁琐的，如果一个程序运行很多这种方式还可行，如果运行时间长达几个小时，这样我们调试一圈耗费的时间就太长了。 而在jupyter notebook中我们可以把代码分隔到不同的cell里逐个进行调试，这样它会持续化变量的值，我们可以交互式的在不同cell里获取到我们想要测试的变量值和类型。 ![nffGNT.png](https://s2.ax1x.com/2019/09/16/nffGNT.png) 然后说一下富文本， 开发代码不仅是给机器去“阅读”，也需要让其他的同事、同学能够很容易的阅读，因此，注释就在开发过程中变的非常重要，一个完善的注释能够让周围人更加容易理解，协作效率也更高，避免重复性劳动。在大多数IDE中都可以进行注释，但是几乎都是相同的，只支持一些简单的文本格式注释，这显然是不够的，jupyter notebook支持Markdown编辑，它的cell不仅可以用于编码，还可以用于书写文本，Markdown可以轻松完成标题、数学公式等格式的编辑，更加有助于解释代码，适用于教学等场景。 最后在说一下轻量、触手可及， 开发过程中我经常需要测试一个小的代码块或者函数，这时候有两个选择：在IDE中新建一个测试脚本；打开命令行下的Python。我觉的这两个都不是好的选择，如果在项目下新建一个脚本后续还需要记住把它清理掉，如果写一个完善的测试脚本用于Alpha、beta测试这显然是低效不现实的。而选择在命令行下，界面不友好，操作不灵活，体验更差。 这时候就显现出jupyter notebook的优势，只需要输入jupyter notebook就会在流量器中打开一个网页，能轻量、快捷的进行开发验证，效率很好。此外，我们还可以通过搭建jupyter notebook服务使得它一直在服务器下运行来避免每次需要时都要在命令行下重复打开，我们只需要在浏览器打开对应的网页即可，这一点下文会详细介绍。 其实，除了这些我们耳熟能详的优点之外，jupyter还有很多令人惊叹的亮点： - 丰富的插件 - 主题修改 - 多语言支持 下面就针对这3点分别介绍一下，介绍下面3个功能的前提条件是已经通过下方命令成功安装jupyter notebook， 1$ pip install jupyter notebook 丰富的插件安装插件管理器 如果没有安装插件管理器，打开jupyter notebook后菜单栏只有如下3项， Files Running Clusters 我们需要安装插件管理器来管理我们需要的插件， 第一步：用pip安装插件管理包， 12$ pip install jupyter_contrib_nbextensions$ pip install jupyter_nbextensions_configurator 第二步：安装一些插件并启用插件管理器， 12$ jupyter nbextensions_configurator enable --user$ jupyter nbextensions_configurator enable --user 然后再次打开jupyter notebook会发现菜单栏多了一个选项Nbextensions, 记得勾选disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development)，否则下方插件是不可选状态。 我们可以通过命令来管理开启或关闭某个插件，但是我觉得还是通过直接勾选我们需要的插件效率更高。 选择插件 我们从上面可以看出，jupyter notebook有很多插件，我们该用哪一个呢？我推荐5款个人认为不错的插件。 Table of Contents Execute Time Nofity Codefolding Hinterland 下面分别介绍一下它们的功能， Table of Contents是一款自动生成目录的工具，它能够通过我们我们富文本中定义的标题自动生成目录，这样我们能够通过点击左侧目录快速定位到我们想要的到达的代码片段。 Execute Time顾名思义，执行时间，我觉得这是一款非常实用的插件，在企业项目开发中，效率是永远无法越过的一个门槛，和学术上理论效果足够优秀即可不同，在企业项目中对效率要求也很高，因此，我们需要统计代码的运行时间，其中最初级的用法就是在每个函数开始和结尾处写一个计时语句，这样比较繁琐。然后再高阶一些的用法就是通过装饰器写一些计时器，在每个函数上调用这个装饰器。其实，如果用jupyter notebook完全没必要这么麻烦。我们只需要打开Execute Time，它就能统计每个cell的运行耗费时间，结束时间等，非常详细，一目了然。 Nofity同样是一款非常实用的插件，当我们运行一个耗时较长的代码时，我们不可能一直盯着屏幕等待，但是我们又希望及时知道它运行结束了，Notify这款插件就可以实现这个功能，它能够在代码运行结束时发出通知，及时告知你代码运行结束了。 Codefolding是一款代码折叠工具，有时候我们写的一个函数非常长，但是我们又不关注 ，这样在阅读过程中会使得效率很低，代码折叠就是一个不错的选择，折叠我们不关注的代码块，Codefolding能够像其他IDE那样让你轻松自如的折叠代码块。 Hinterland是一款自动补全插件，称一个IDE“优秀”，如果没有自动补全显然是说不过去的。jupyter notebook自带补全功能，但是每次都需要点击tab键来补全，这样效率比较低，我们可以通过勾选Hinterland让jupyter notebook具备自动补全功能，当我们输入几个包含字母后它能够快速补全我们想要的函数，补全速度堪比pycharm。 主题修改很多同学使用jupyter notebook都会觉得，这款开发工具界面太单调了，只有纯白色的主题，其实并不是这样，jupyter notebook也支持主题修改，而且非常方便。 首先在命令行下输入下面命令安装主题， 1$ pip install jupyterthemes jupyter notebook的主题管理工具叫做jt，我们可以通过下面命令查看可用主题， 1234567891011$ jt -lAvailable Themes: chesterish grade3 gruvboxd gruvboxl monokai oceans16 onedork solarizedd solarizedl 然后通过下面命令设置主题， 1$ jt -t &lt;theme_name&gt; 其中theme_name为主题名称。 如果觉得不满意，想退回默认主题，可以通过下方命令实现， 1$ jt -r 多语言支持很多同学是因为Python而解除到jupyter notebook的，因此会认为这就是一款Python专属的开发工具，如果这样的话，那么也不足以我专门用一篇文章来介绍这款开发工具。 它更像是eclipse、IDEA、vscode，是一款综合的开发工具，它不仅支持Python，还支持C++、julia、R、ruby、Go、Scala、C#、Perl、PHP、Octave、Matlab、Lua、Tcl、等多种编程语言，功能十分强大，支持语言详情，请查看下方链接， https://github.com/jupyter/jupyter/wiki/Jupyter-kernels 不同语言的配置方式各不相同，这里不再一一介绍，可以根据自己需要的语言自行在网上搜索相关配置资料进行配置。 jupyter notebook服务如果非要找出使用jupyter notebook的缺点，我认为就是每次启动的时候相对繁琐，我们启动本地安装的IDE，一个命令或者点击一下图标即可，但是如果启动jupyter notebook就需要进入命令行或终端，输入“jupyter notebook”进行打开，如果使用的是虚拟环境，首先还要激活虚拟环境，这无疑是非常繁琐的，而且启动后它会占用一个终端或命令行窗口，如果意外关闭则会终止jupyter notebook服务。其实，这也是有解决方法的，我们搭建一个持续化的jupyter notebook服务，让它一直在服务器后台运行，这样既不占用窗口，也不需要繁琐的打开步骤，我们只需要把对应的URL收藏，每次需要时能够秒级速度打开，下面就来介绍一下jupyter notebook的搭建步骤。 第一步：获取sha1密码 在命令行下输入ipython， 12345In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password:Verify password:Out[2]: 'sha1:746cf729d33f:0af9cda409de9791f237a6c46c3c76a3237962fc' 导入passwd函数，调用后会让你输入密码，你可以设置一个明文密码，例如123，然后它会生成一个sha1密码串，这个很重要，后面会用到。 修改jupyter配置文件，linux系统配置文件路径为~/.jupyter/jupyter_notebook_config.py，windows系统配置文件路径为C:\\Users\\User\.jupyter\\jupyter_notebook_config.py，如果没有这个文件，可以使用下面命令生成， 1$ jupyter notebook --generate-config 这个配置文件很长，以linux为例，主要关注的是如下几项， 12345c.NotebookApp.ip = '*' c.NotebookApp.password = u'sha1:xxx:xxx' c.NotebookApp.open_browser = False c.NotebookApp.port = 8888c.NotebookApp.enable_mathjax = True c.NotebookApp.ip、c.NotebookApp.port，ip要和服务器保持一致，端口可以自行设定，不和其他端口冲突即可，后续访问时在浏览器输入ip:port即可。 c.NotebookApp.password就是前面生成的sha1密码串，复制过来即可。 c.NotebookApp.open_browser = False 的含义为是每次启动命令时是否打开浏览器，由于我们用的时候直接输入URL即可，所以这里不需要打开浏览器。 c.NotebookApp.enable_mathjax的含义为是否用mathjax，它是一种用于数学公式显示的工具，这里选True。 配置好这几项之后保存退出，输入下面命令即可启动， 1$ nohup jupyter notebook &gt; /dev/null 2&gt;&amp;1 &amp; nohup的含义是后台运行，这样就不用占用一个窗口来了。 配置好之后只要服务器不关机，jupyter notebook的服务会一直处于运行状态，我们随时可以使用，只需要打开ip:port即可。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十五讲：卷积神经网络之LeNet]]></title>
    <url>%2F2019%2F09%2F13%2Flenet%2F</url>
    <content type="text"><![CDATA[前言 提起卷积神经网络，也许可以避开VGG、GoogleNet，甚至可以忽略AleNet，但是很难不提及LeNet。 LeNet是由2019年图灵奖获得者、深度学习三位顶级大牛之二的Yann LeCun、Yoshua Bengio于1998年提出(Gradient-based learning applied to document recognition)，它也被认为被认为是最早的卷积神经网络模型。但是，由于算力和数据集的限制，卷积神经网络提出之后一直都被传统目标识别算法(特征提取+分类器)所压制。终于在沉寂了14年之后的2012年，AlexNet在ImageNet挑战赛上一骑绝尘，使得卷积神经网络又一次成为了研究的热点。 近几年入门计算机视觉的同学大多数都是从AlexNet甚至更新的网络模型入手，了解比较多的就是R-CNN系列和YOLO系列，在很多知名的课程中对LeNet的介绍也是非常浅显或者没有介绍。虽然近几年卷积神经网络模型在LeNet的基础上加入了很多新的单元，在效果方面要明显优于LeNet，但是作为卷积神经网络的基础和源头，它的很多思想对后来的卷积神经网络模型具有很深的影响，因此，我认为了解一下LeNet还是非常有必要的。 本文首先介绍一下LeNet的网络模型，然后使用tensorflow来一步一步实现LeNet。 LeNet 上图就是LeNet的网络结构，LeNet又被称为LeNet-5，其之所以称为这个名称是由于原始的LeNet是一个5层的卷积神经网络，它主要包括两部分： 卷积层 全连接层 其中卷积层数为2，全连接层数为3。 这里需要注意一下，之前在介绍卷积、池化时特意提到，在网络层计数中池化和卷积往往是被算作一层的，虽然池化层也被称为”层”，但是它不是一个独立的运算，往往都是紧跟着卷积层使用，因此它不单独计数。在LeNet中也是这样，卷积层块其实是包括两个单元：卷积层与池化层。 在网络模型的搭建过程中，我们关注的除了网络层的结构，还需要关注一些超参数的设定，例如，卷积层中使用卷积核的大小、池化层的步幅等，下面就来介绍一下LeNet详细的网络结构和参数。 第一层：卷积层 卷积核大小为5*5，输入通道数根据图像而定，例如灰度图像为单通道，那么通道数为1，彩色图像为三通道，那么通道数为3。虽然输入通道数是一个变量，但是输出通道数是固定的为6。 池化层中窗口大小为2*2，步幅为2。 第二层：卷积层 卷积核大小为5*5，输入通道数即为上一层的输出通道数6，输出通道数为16。 池化层和第一层相同，窗口大小为2*2，步幅为2。 第三层：全连接层 全连接层顾名思义，就是把卷积层的输出进行展开，变为一个二维的矩阵(第一维是批量样本数，第二位是前一层输出的特征展开后的向量)，输入大小为上一层的输出16，输出大小为120。 第四层：全连接层 输入大小为120，输出大小为84。 第五层：全连接层 输入大小为84，输出大小为类别个数，这个根据不同任务而定，假如是二分类问题，那么输出就是2，对于手写字识别是一个10分类问题，那么输出就是10。 激活函数 前面文章中详细的介绍了激活函数的作用和使用方法，本文就不再赘述。激活函数有很多，例如Sigmoid、relu、双曲正切等，在LeNet中选取的激活函数为Sigmoid。 模型构建 如果已经了解一个卷积神经网络模型的结构，知道它有哪些层、每一层长什么样，那样借助目前成熟的机器学习平台是非常容易的，例如tensorflow、pytorch、mxnet、caffe这些都是高度集成的深度学习框架，虽然在强化学习、图神经网络中表现一般，但是在卷积神经网络方面还是很不错的。 我绘制了模型构建的过程，详细的可以看一下上图，很多刚入门的同学会把tensorflow使用、网络搭建看成已经非常困难的事情，其实理清楚之后发现并没有那么复杂，它主要包括如下几个部分： 数据输入 网络模型 训练预测 其中，重点之处就在于网络模型的搭建，需要逐层的去搭建一个卷积神经网络，复杂程度因不同的模型而异。训练测试过程相对简单一些，可以通过交叉熵、均方差等构建损失函数，然后使用深度学习框架自带的优化函数进行优化即可，代码量非常少。 LeNet、AlexNet、VGG、ResNet等，各种卷积神经网络模型主要的区别之处就在于网络模型，但是网络搭建的过程是相同的，均是通过上述流程进行搭建，因此，本文单独用一块内容介绍模型搭建的过程，后续内容不再介绍网络模型的搭建，会直接使用tensorflow进行编程实践。 编程实践完整代码请查看github项目： aiLearnNotes 首先需要说明一下，后续的内容中涉及网络模型搭建的均会选择tensorflow进行编写。虽然近几年pytorch的势头非常迅猛，关于tensorflow的批评之声不绝于耳，但是我一向认为，灵活性和易用性总是成反比的，tensorflow虽然相对复杂，但是它的灵活性非常强，而且支持强大的可视化tensorboard，虽然pytorch也可以借助tensorboard实现可视化，但是这样让我觉得有一些”不伦不类”的感觉，我更加倾向于一体化的框架。此外，有很多同学认为Gluon、keras非常好用，的确，这些在tensorflow、mxnet之上进一步封装的高级深度学习框架非常易用，很多参数甚至不需要开发者去定义，但是正是因为这样，它们已经强行的预先定义在框架里了，可想而知，它的灵活性是非常差的。因此，综合灵活性、一体化、丰富性等方面的考虑，本系列会采用tensorflow进行编程实践。 其次，需要说明的是本系列重点关注的是网络模型，因此，关于数据方面会采用MNIST进行实践。MNIST是一个成熟的手写字数据集，它提供了易用的接口，方便读取和处理。 在使用tensorflow接口读取MNIST时，如果本地有数据，它会从本地加载，否则它会从官网下载数据，如果由于代理或者网速限制的原因自动下载数据失败，可以手动从官网下载数据放在MNIST目录下，数据包括4个文件，分别是： train-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz 它们分别是训练数据集和标签，测试数据集和标签。 可能会有人有疑问，手写体识别不是图像吗？为什么是gz的压缩包？因为作者对手写体进行了序列化处理，方便读取，数据原本是衣服单通道28*28的灰度图像，处理后是784的向量，我们可以通过一段代码对它可视化一下， 12345678910from matplotlib import pyplot as pltfrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST", one_hot=True)for i in range(12): plt.subplot(3, 4, i+1) img = mnist.train.images[i + 1] img = img.reshape(28, 28) plt.imshow(img)plt.show() 通过读取训练集中的12副图像，然后把它修改成28*28的图像，显示之后会发现和我们常见的图像一样， 下面开始一步一步进行搭建网络LeNet，由前面介绍的模型构建过程可以知道，其中最为核心的就是搭建模型的网络架构，所以，首先先搭建网络模型， y=wx+b卷积的运算是符合上述公式的，因此，首先构造第一层网络，输入为批量784维的向量，需要首先把它转化为28*28的图像，然后初始化卷积核，进行卷积、激活、池化运算， 123456X = tf.reshape(X, [-1, 28, 28, 1])w_1 = tf.get_variable("weights", shape=[5, 5, 1, 6])b_1 = tf.get_variable("bias", shape=[6])conv_1 = tf.nn.conv2d(X, w_1, strides=[1, 1, 1, 1], padding="SAME")act_1 = tf.sigmoid(tf.nn.bias_add(conv_1, b_1))max_pool_1 = tf.nn.max_pool(act_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") 然后构建第二层网络， 12345w_2 = tf.get_variable("weights", shape=[5, 5, 6, 16])b_2 = tf.get_variable("bias", shape=[16])conv_2 = tf.nn.conv2d(max_pool_1, w_2, strides=[1, 1, 1, 1], padding="SAME")act_2 = tf.nn.sigmoid(tf.nn.bias_add(conv_2, b_2))max_pool_2 = tf.nn.max_pool(act_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") 到这里，卷积层就搭建完了，下面就开始搭建全连接层。 首先需要把卷积层的输出进行展开成向量， 1flatten = tf.reshape(max_pool_2, shape=[-1, 2 * 2 * 16]) 然后紧接着是3个全连接层， 12345678910111213141516171819# 全连接层1with tf.variable_scope("fc_1") as scope: w_fc_1 = tf.get_variable("weight", shape=[2 * 2 * 16, 120]) b_fc_1 = tf.get_variable("bias", shape=[120], trainable=True)fc_1 = tf.nn.xw_plus_b(flatten, w_fc_1, b_fc_1)act_fc_1 = tf.nn.sigmoid(fc_1)# 全连接层2with tf.variable_scope("fc_2") as scope: w_fc_2 = tf.get_variable("weight", shape=[120, 84]) b_fc_2 = tf.get_variable("bias", shape=[84], trainable=True)fc_2 = tf.nn.xw_plus_b(act_fc_1, w_fc_2, b_fc_2)act_fc_2 = tf.nn.sigmoid(fc_2)# 全连接层3with tf.variable_scope("fc_3") as scope: w_fc_3 = tf.get_variable("weight", shape=[84, 10]) b_fc_3 = tf.get_variable("bias", shape=[10], trainable=True)fc_3 = tf.nn.xw_plus_b(act_fc_2, w_fc_3, b_fc_3) 这样就把整个网络模型搭完成了，输入是批量图像X，输出是预测的图像，输出是一个10维向量，每一维的含义是当前数字的概率，选择概率最大的位置，就是图像对应的数字。 完成了网络模型的搭建，它能够将输入图像转化成预测标签进行输出，接下来要做的就是训练和测试部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def train(): # 1. 输入数据的占位符 x = tf.placeholder(tf.float32, [None, 784]) y = tf.placeholder(tf.float32, [BATCH_SIZE, 10]) # 2. 初始化LeNet模型，构造输出标签y_ le = LeNet() y_ = le.create(x) # 3. 损失函数，使用交叉熵作为损失函数 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y)) # 4. 优化函数，首先声明I个优化函数，然后调用minimize去最小化损失函数 optimizer = tf.train.AdamOptimizer() train_op = optimizer.minimize(loss) # 5. summary用于数据保存，用于tensorboard可视化 tf.summary.scalar("loss", loss) merged = tf.summary.merge_all() writer = tf.summary.FileWriter("logs") # 6. 构造验证函数，如果对应位置相同则返回true，否则返回false correct_pred = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)) # 7. 通过tf.cast把true、false布尔型的值转化为数值型，分别转化为1和0，然后相加就是判断正确的数量 accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # 8. 初始化一个saver，用于后面保存训练好的模型 saver = tf.train.Saver() with tf.Session() as sess: # 9. 初始化变量 sess.run((tf.global_variables_initializer())) writer.add_graph(sess.graph) i = 0 for epoch in range(5): for step in range(1000): # 10. feed_dict把数据传递给前面定义的占位符x、y batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE) summary, loss_value, _ = sess.run(([merged, loss, train_op]), feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) print("epoch : &#123;&#125;----loss : &#123;&#125;".format(epoch, loss_value)) # 11. 记录数据点 writer.add_summary(summary, i) i += 1 # 验证准确率 test_acc = 0 test_count = 0 for _ in range(10): batch_xs, batch_ys = mnist.test.next_batch(BATCH_SIZE) acc = sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) test_acc += acc test_count += 1 print("accuracy : &#123;&#125;".format(test_acc / test_count)) saver.save(sess, os.path.join("temp", "mode.ckpt")) 上述就是训练部分的完整代码，在代码中已经详细的注释了每个部分的功能，分别包含数据记录、损失函数、优化函数、验证函数、训练过程等，然后运行代码可以看到效果， 12345678...epoch : 4----loss : 0.07602085173130035epoch : 4----loss : 0.05565792694687843epoch : 4----loss : 0.08458487689495087epoch : 4----loss : 0.012194767594337463epoch : 4----loss : 0.026294417679309845epoch : 4----loss : 0.04952147603034973accuracy : 0.9953125 准确率为99.5%，可以看得出，在效果方面，LeNet在某些任务方面并不比深度卷积神经网络差。 打开tensorboard可以直观的看到网络的结构、训练的过程以及训练中数据的变换， 1$ tensorboard --logdir=logs 通过损失函数的变化过程可以看出，训练过程在2000步左右基本达到了最优解， 更多精彩内容请关注公众号【平凡而诗意】，或者加入我的知识星球【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 神器AutoHotkey的使用教程]]></title>
    <url>%2F2019%2F09%2F06%2Fautohotkey%2F</url>
    <content type="text"><![CDATA[简单示例AutoHotkey是一款强大、开源的热键脚本工具。在以往的文章里，我介绍了很多强大的效率提升工具，其中包括Wox、Listary、QuickLook等。如果说这些软件在某些领域独树一帜，那么AutoHotkey则是在则是“无所不能”的强大工具。 易用性和功能的灵活性往往是成反比的，举一个极端的例子，开发语言这类工具在使用方面非常不友好，但是经过开发语言的各种组合可以实现各种手机、PC端软件数不胜数的功能。当然，由于它的使用偏于专业，所以更多人选择定制化较强、界面友好的工具，但是却要牺牲一些灵活性。 Autohotkey是一款介于纯编程语言和分发软件之间的一款工具，你可以使用它写一些简单的脚本语言，用内置的一些函数或者自定义的函数去单独或组合使用，以达到我们期望的功能，下面先来看一个示例。 先看一个简单的AutoHotkey脚本， 12345678^j::Send, This is a Hotkey!return::hs::This is a HotString!^#s::Run, D:\Sublime Text 3\sublime_text.exe 然后鼠标右键点击编译，或者双击脚本运行，然后点击对应的快捷键， 这个演示包括三个动作： 快捷键启动sublime 快捷键输入“This is a Hotkey!” 快捷字符输入“This is a HotString!” 回到前面给出的AutoHotkey脚本，来一步一步的解释怎么实现的。 第一个动作：快捷键启动sublime 12^#s::Run, D:\Sublime Text 3\sublime_text.exe 这句命令实现的是快捷键启动sublime。 第一行中^和#是两个代表按键的符号，分别代表ctrl和windows，s就是键盘上的s键，::可以认为是命令的结束符号。 第二行中Run是一个内置函数，用于运行一个工具或者打开一个网页，后面跟的是要打开的网页或者软件路径(如果已经加入到环境变量，就不需要完整的安装路径)。 因此，上述两行脚本的功能就是ctrl+win+s就可以打开sublime这款软件。 第二个动作：快捷键输入“This is a Hotkey!” 123^j::Send, This is a Hotkey!return 看完第一个动作的介绍，应该很容易理解这个动作， 第一行的含义是ctrl+j快捷键。 第二行的含义是发送一段字符串。 第三个动作：快捷字符输入“This is a HotString!” 1::hs::This is a HotString! 这个和前两个动作不同，介绍这个动作之前需要先简单的了解一下AutoHotkey，它主要包括两个概念： hotkey hotstring 其中hotkey并不陌生，就是热键、快捷键，前两个动作实现的就是快捷键。 这里需要说一下hotstring，顾名思义，就是通过一段字符串实现一个快捷功能，第三个动作实现的就是一个hotstring功能。 现在回过头来解释一下第三个动作的脚本的含义， 两个::之间定义的是快键字符串，后面跟随的是要输入的完整字符串，这样的话在文本框输入hs然后点击Tab键即可在编辑器中输入This is a HotString!这个完整的字符串。 很多人用惯了快捷键会疑问，hostring有什么价值？我认为它对于开发者或者文本编辑相关的工作者是非常有意义的。我们可以在脚本中预先用一些hostring定义好我们常用的代码块或者文字内容，这样，当输入对应的hostring时就可以快速补全我们想要的内容，速度和资源消耗要远远小于常用的代码补全工具。 热键符号通过前面的简单示例，想必应该对AutoHotkey有了简单的了解，它就是通过一些内置的符号、函数、自定义函数来任意组合，定制化的实现我们想要的功能。 通过示例中第一个动作^#s代表快捷键ctrl+win+s可以看出，热键符号具有至关重要的作用，我们怎么就知道^代表ctrl、#代表win呢？因为官网给出了不同符号的对应关系，下面是各个符号对应的热键， 符号 描述 # win ！ Alt ^ Ctrl + Shift &amp; 可以在两个组合键之间使用 &lt; 指定快捷键的位置在左边 &gt; 指定快捷键的位置在右边 注：我们都知道，alt、ctrl、shift这些键都有两个，左右两边均有，&lt;和&gt;两个符号就指定使用左边的符号还是右边的符号，例如&lt;!指定使用左边的Alt键。 标签在示例中，我们也看到使用了Run、Send，这两个称为AutoHotkey的标签，虽然我是以大写字母开头，但是标签名称其实是部分不区分大小写的，可以由空格、制表符、逗号、转义符以外的任何字符组成，但是由于样式的约定，通常最好使用字母、数字、下划线，AutoHotkey常用的内置标签有如下几个， 标签 描述 Send 向编辑器发送一段字符 SendInput 同上 MsgBox 弹出对话框 Run 运行一个工具或打开网页 WinActivate 窗口激活 WinWaitActive 窗口等待激活 下面看一个例子， 1234^j::Run, https://www.baidu.comMsgBox, 已经打开网页！return 有了前面的基础，应该很容易理解这个脚本，它是一个组合功能，分别是打开网页和弹出对话框，下面看看效果， 函数想要更加灵活，仅仅使用上述这些符号、标签显然是不够的。和大多数编程语言一样，AutoHotkey也支持自定义函数，这才是它的强大之处。 AutoHotkey内置了一些常用的函数，如下， 函数 描述 FileExist 检查文件或文件夹是否存在，并返回其属性 GetKeyState 获取按键状态，向下返回true，向上返回false InStr 从左或右搜索字符串的给定出现项 RegExMatch 确定字符串是否包含正则表达式匹配模式 RegExReplace 替换字符串中出现的模式(正则表达式) StrLen 获取字符串长度 StrReplace 替换字符串 StrSplit 用指定的分隔符分割字符串 SubStr 按指定位置返回子字符串 当然，AutoHotkey内置的函数远不止这些，它还包括以下类型的函数： 文件读取 数学计算 条件判断 异常处理 状态获取 鼠标键盘 屏幕状态 声音 进程管理 窗口状态 …… 没错，AutoHotkey在功能和丰富性方面丝毫不亚于一些老的脚本语言，但是它的优点是更加实用。我们可以即写即用，能够辅助我们日常生活中多种场景的工作。 使用场景快捷启动 这方面它可以替代Wox、Listary这些快速启动工具，我们可以把常用的网站、软件用脚本的方式定义不同的hotkey或者hotstring，当我们需要打开一个网页或者软件时就不需要再繁琐、多步骤的去寻找、打开。 例如用下面这个脚本，能够快速打开优酷、B站、直播吧， 1234567891011^y::Run, https://www.youku.com/return^b::Run, https://www.bilibili.com/return^z::Run, https://www.zhibo8.cc/return 自动补全 我们在日常开发或者文本编辑时，会有大量重复的工作，以编程为例，我们会有很多重复的代码块，因此，才有各种各样的补全工具，但是目前大多数补全工具可以说是差强人意，速度方面甚至不及自己手动敲代码的速度，但是通过AutoHotkey把我们常用的代码块、文本用hotkey、hotstring代替，这样能够快速的补全我们想要的内容， 例如，用下面几行脚本补全我们常用的代码片段， 123::np::import numpy as np::plt::from matplotlib import pyplt as plt::tf::import tensorflow as tf 效率提升 关于效率提升，这就因人而异，不同的人工作内容不同，因此常用的操作和功能也截然不同，这方面就需要发散思维，总结一下平时自己常用的操作，例如管理进程、取色、文件读取、编程、数学运算等，可以根据自己的需求，使用内置的或者自定义的函数来组合成自己想要的功能，当然，AutoHotkey内置了很多标签、函数，这足以满足日常大多数场景的需求。 软件分发 我们经常在能够在开源平台寻找到很多别人写的软件，其实自己也可以利用AutoHotkey实现一些比较有趣、高效的工具，它不像C++、Java那么难以入门，而且在代码规范方面要求没那么严格，因此门槛相对较低。此外，AutoHotkey脚本的编译非常迅速，资源消耗低，能够一键编译成我们常见的exe软件，这样的话可以把它分享给周围的同学、同事，或者更多的人，这样在提高周围人效率的同时能够锻炼自己的产品思维和开发规范。 安装与基本操作下载安装包 AutoHotkey是一款开源免费的工具，能够直接从官网下载AutoHotkey的安装包， https://www.autohotkey.com/ 如果觉得麻烦，也可以在公众号后台回复关键字hot获取安装包。 安装 双击下载的安装包，会弹出安装界面，直接一步步往下点击即可，如果需要更改安装路径，可以在location页面进行更改。 新建脚本 成功安装后，在桌面或者其他空白处点击鼠标右键-新建-AutoHotkey Script，来创建一个AutoHotkey脚本。 编辑脚本 创建脚本后，鼠标右键点击脚本，可以看到有三个选项：Run Script、Compile Script、Edit Script。 可以先点击Edit Script，它会用记事本打开，当然也可以用sublime、UE、notepad++等文本编辑器打开，然后进行编辑。 编辑之后可以双击脚本直接运行，也可以点击Run Script运行脚本，这样就可以使用我们脚本中定义的快捷键或者快捷字符串，此外，还可以点击Compile Script把脚本编译成exe文件，这样的话脚本会被加密，可以用于分发，其他使用者就无法看到工具源码。 更多精彩内容请关注公众号【平凡而诗意】，或者加入我的知识星球【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>文件查找</tag>
        <tag>工具</tag>
        <tag>实用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十四讲：正则化之Dropout]]></title>
    <url>%2F2019%2F09%2F01%2Fcnn-dropout%2F</url>
    <content type="text"><![CDATA[本文完整代码请查看：aiLearnNotes 前言在前几讲里已经介绍了卷积神经网络中常用的一些单元，例如， 卷积层 池化层 填充 激活函数 批量归一化 本文会介绍最后一个卷积神经网络中常用的单元Dropout，可以称之为“丢弃法”，或者“随机失活”。它在2012年由Alex Krizhevsky、Geoffrey Hinton提出的那个大名鼎鼎的卷积神经网络模型AlexNet中首次提出并使用，Dropout的使用也是AlexNet与20世纪90年代提出的LeNet的最大不同之处。随后，Krizhevsky和Hinton在文章《Dropout: A Simple Way to Prevent Neural Networks from Over tting》又详细的介绍了介绍了Dropout的原理。发展至今，Dropout已经成为深度学习领域一个举足轻重的技术，它的价值主要体现在解决模型的过拟合问题，虽然它不是唯一的解决过拟合的手段，但它却是兼备轻量化和高效两点做的最好的一个手段。 “丢弃法”，从字面意思很好理解，就是丢弃网络中的一些东西。丢弃的是什么？神经元，有放回的随机丢弃一些神经元。 很多刚接触或者使用过Dropout的同学都会觉得“这有什么好讲的？这是一个非常简单的东西啊。”，如果仅仅从使用角度来讲，这的确非常简单。以目前主流的机器学习平台而言，tensorflow、mxnet、pytorch，均是传入一个概率值即可，一行代码即可完成。但是，我认为学习深度学习如果仅仅是为了会使用，那么真的没什么可以学习的，抽空把tensorflow教程看一下既可以称得上入门深度学习。如果剖开表象看一下Dropout的原理，会发现，它的理论基础是非常深的，从作者先后用《Improving neural networks by preventing co-adaptation of feature detectors》《Dropout: A Simple Way to Prevent Neural Networks from Over tting》等多篇文章来阐述这个算法就可以看出它的不可小觑的价值。 和往常先讲理论再讲用法不同，本文先介绍一下它在tensorflow中的用法，然后做一个了解后带着问题去介绍它的理论知识，本文主要包括如下几块内容， - tensorflow中Dropout的使用 - 优化与机器学习的区别 - 过拟合 - Dropout理论知识 # tensorflow中Dropout的使用 在tensorflow中Dropout的函数为， 1tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) 函数中的参数分别是： x：Dropout的输入，即为上一层网络的输出 keep_prob：和x尺寸相同的张量(多维向量)，它用于定义每个神经元的被保留的概率，假如keep_prob=0.8，那么它被保留的概率为0.8，换个角度说，它有20%的概率被丢弃。 noise_shape：表示随机生成的保存/删除标志的形状，默认值为None。 seed：一个用于创建随机种子的整数，默认值为None。 name：运算或操作的名称，可自行定义，默认值为None。 上述5个参数中x和keep_prob为必须参数，其他很少用到，所以不多介绍。x不难理解，就是上一层网络的输出。这里主要提示一下keep_prob，它是Dropout使用中最为重要的参数。 注意：keep_prob是网络中每个神经元被保留的概率，并非是神经网络中神经元被保留个数的概率。举个例子，加入有个3层神经网络，共100个神经元，定义keep_prob=0.6，那么并不是说要保留60个神经元而丢弃40个。而是每个神经元将会有60%的概率被保留，40%的概率被丢弃，所以最终剩余的神经元并不是确切的60个，可能多于60也可能少于60。 策略：深度学习是一门经验主义非常重的方向，Dropout的使用同样需要很多经验。一般情况下建议靠近输入层的把keep_prob设置大一些，就是靠近输入层经历多保留神经元。 优化与机器学习的区别讲完Dropout的使用，话说回来，为什么要用Dropout？ 提到这个问题，就不得不先做一下铺垫，先谈一下优化与机器学习的区别。 机器学习主要包括如下几块内容： 数据集 模型 损失函数 优化算法 其中优化算法直接决定着最终模型效果的好坏，因此，很多人都肆意的扩大优化算法的价值，认为“机器学习就是优化算法”。 我认为这是不严谨的说法机器学习与优化算法有这本质的区别。优化算法主要用于已知或未知数学模型的优化问题，它主要关注的是在既定模型上的误差，而不关注它的泛化误差。而机器学习则不同，它是在训练集上训练书模型，训练过程中与优化算法类似，考虑在训练集上的误差，但是它对比于优化算法还要多一步，要考虑在测试集上的泛化误差。 过拟合 (图片截取自吴恩达《深度学习工程师》) 在训练集和测试集这两个数据集上的精确度就引出几种情况： 在训练集上效果不好，在测试集上效果也不好：欠拟合(上图中第一种情况) 在训练集上效果很好，在测试集上效果不好：过拟合(上图中第三种情况) 在实际项目中，这两种情况是非常常见的，显然，这并不是我们想要的，我们追求的是第三种情况： 在训练集和测试集上的效果都相对较好(上图中第二种情况) 但是，事与愿违，欠拟合和过拟合是机器学习中非常常见的现象，尤其是过拟合。 过拟合的形成原因主要包括如下几点： 训练数据集太少 参数过多 特征维度过高 模型过于复杂 噪声多 很多研究者把目光和精力都聚焦在解决过拟合这个问题上，目前用于解决过拟合问题的算法有很多，例如， 权重衰减 Early stopping 批量归一化(没错，就是前一讲讲的BN，它也带有一些正则化的功能) 在解决过拟合问题的算法中最为知名的词汇莫过于正则化。 提到正则化，很多同学会想到L1、L2正则化，其实它是一类方法的统称，并非仅限于L1、L2正则化，目前用于结果过拟合的正则化方法主要包括如下几种： 数据扩充 L1、L2正则化 Dropout 没错，Dropout也是正则化方法中的一种！铺垫这么多，终于引出本文的主角了。 数据扩充解决过拟合，这一点不难理解，因为数据缺少是引起过拟合的主要原因之一，由于数据的却是导致模型学习过程中不能学到全局特征，只能通过少量数据学习到一个类别的部分特征，通过数据的扩充能够让模型学习到全局特征，减少过拟合现象。 L1、L2正则化主要为损失函数添加一个L1或L2的正则化惩罚项，防止学习过程中过于偏向于某一个方向引起过拟合。 最后就轮到Dropout，下面来详细讲一下Dropout的原理。 Dropout如何使用Dropout？ (图片来自于《Dropout: A Simple Way to Prevent Neural Networks from Over tting》) 上图中左图为一个标准的神经网络，右图是采用Dropout之后的神经网络，其中的区别一目了然，就是丢弃了一些神经元。 前面已经讲解了在tensorflow中如何使用Dropout，已经清楚，对于Dropout最为核心的就是保留概率或者丢弃概率，简述它的原理就是：遍历神经网络的每一层中每一个神经元，以一定概率丢弃或保留某个神经元。用数学语言描述如下， 假设某一个神经元的输入有4个，那么神经元的计算表达式为， h_{i}=\phi\left(x_{1} w_{1 i}+x_{2} w_{2 i}+x_{3} w_{3 i}+x_{4} w_{4 i}+b_{i}\right)其中$x_i$是输入，$w$是权重，$b$是偏差。 假设保留概率为$p$，那么丢弃改为就为$1-p$，那么神经元$h_i$就有$1-p$的概率被丢弃，那么经过Dropout运算后的神经元为， h_{i}^{\prime}=\frac{\xi_{i}}{1-p} h_{i}其中$\xi_{i}$为0或者1，它为0或者1的概率分别为$1-p$和$p$，如果为0,则这个神经元被清零，临时被丢弃，一定要注意，是临时的丢弃，Dropout是有放回的采样。在一轮训练中前向或反向传播都会用丢弃后的神经网络，下一轮训练又会随机丢弃，用一个新的网络去训练。 编程实现Dropout其实只需要几行代码，下面结合代码来解释，会更容易理解， 123456def dropout(X, keep_prob): assert 0 &lt; keep_prob &lt; 1 if keep_prob == 0: return np.zeros(X.shape) mask = np.random.uniform(0, 1, X.shape) &lt; keep_prob return mask * X / keep_prob 输入参数为上一层的激活值和保留概率， 第3行：如果保留概率为0，也就是不保留的话，则全部元素都丢弃。 第5行：生成一个随机的掩码，掩码和输入X形状相同，每个位置非0即1，然后用这个掩码与X相乘，如果对应位置乘以0，则这个神经元被丢弃，反之保留。 Dropout为什么起作用？ (图片来自《深度学习》) 这里不得不提一下Bagging集成集成学习方法，在Bagging集成学习方法中，预先会定义k的模型，然后采用k个训练集，然后分别训练k个模型，然后以各种方式去评价、选取最终学习的模型。 Dropout的训练过程中与Bagging集成学习方法类似，以上图为例，有一个三层神经网络(两个输入神经元、两个隐藏神经元、一个输出神经元)，从这个基础的网络中随机删除非输出神经元来构建一些子集。这样每一次训练就如同在这些子集中随机选择一个不同的网络模型进行训练，最后通过”投票”或者平均等策略而选择出一个最好的模型。 其实这个思想并不陌生，在传统机器学习中Adaboost、随机森林都是采用集成学习的思想，效果非常好。采用Dropout后的深度神经网络和这类思想也是类似的，这样能够结合不同网络模型的训练结果对最终学习的模型进行评价，能够综合多数，筛选掉少数，即便是某个网络模型出现了过拟合，最终经过综合也会过滤掉，效果自然会好很多。 需要说明一点的是，虽然采用Dropout和其他集成学习方法思想有异曲同工之处，但是也有一些细节的差异。在Bagging中所有模型都是独立的，但是在Dropout中所有模型是共享参数的，每个子模型会继承父网络的网络参数，这样可以有效的节省内存的占用。 需要注意的点到这里，Dropout的内容就讲解完了，总结一些本文，需要有几点需要注意， keep_prob是每个神经元被保留的概率 Dropout和L1、L2、数据扩充都属于正则化方法 Dropout的丢弃是有放回的 参考文献 Dropout: A Simple Way to Prevent Neural Networks from Over tting Dropout as data augmentation Improving Neural Networks with Dropout Improving neural networks by preventing co-adaptation of feature detectors 《深度学习》 资源获取我把参考文献中列出的4片文章和《深度学习》这本书籍的电子版进行整理共享了，感兴趣的可以关注公众号，回复关键字“dl”获取。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第四讲：类的特殊方法(下篇)]]></title>
    <url>%2F2019%2F08%2F28%2Fpython-4%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言特殊方法是为我们定义的类添加上某些特殊功能的方法，上一讲分组讲解了Python的几对特殊方法(或者成为魔术方法)，分别是， __new__与__init__ __enter__与__exit__ __str__与__repr__ __setattr__、__getattr__、__getattribute__与__delattr__ 这些都是相对较为常用的。Python中类的特殊方法远不止这些，其中还有一些不太常用，或者在某些特定场景下用到的特殊方法。 本讲会按照功能对剩余的特殊方法进行分类，不再详细的把每个特殊方法的使用都展开阐述，会着重的从每种功能中挑选出具有代表性的特殊方法进行实现、详细讲解。 # 函数调用 假如我们定义一个用于算数运算的类， 12345678910111213class Operation(object): def __init__(self, x, y): self.x = x self.y = y def add(self): print("The result is &#123;&#125;".format(self.x + self.y)) opt = Operation(2, 2)opt.add()# 输出The result is 4 从这段代码可以看出 ，这是我们一贯使用类及类方法的方式，实例化—调用，其实Python提供有特殊方法__call__能够让类的调用像调用函数的方式一样。 这句话听着似乎很绕口，具体什么含义呢？用一段代码来说明， 123456789101112131415161718class Operation(object): def __init__(self): self.x = None self.y = None def add(self): print("The result is &#123;&#125;".format(self.x + self.y)) def __call__(self, x, y): self.x = x self.y = y self.add() opt = Operation()opt(3, 3)# 输出The result is 6 当我们给类添加特殊方法__call__后，我们可以直接使用实例名(opt)来调用类的方法，就不用在用instance.method的方法去调用。换句话说就是，当我们定义__call__后，我们使用实例名进行调用时，它会首先进入__call__方法，执行__call__中的程序。 容器与序列容器和序列分别涉及2个特殊方法：__contains__、__len__。 从__len__名称就可以看出它的功能，给类添加一个获取序列长度的功能，所以这里着重讲解一下容器，顺带讲解一下__len__。 我们在条件语句中经常会用到这样的语句if … in、if … not in，其中__contains__就可以给类添加这样一个功能，可以通过if … in、if … not in来调用类的实例，以一段代码来举例， 123456789101112131415161718192021222324class Contain(object): def __init__(self, data): self.data = data def __contains__(self, x): return x in self.data def __len__(self): return len(self.data) contain = Contain([2,3,4,5])if 2 in contain: print("222222") if 6 not in contain: print("666666") len(contain) # 输出2222226666664 从代码中可以看出，当我们调用if 2 in contain时会调用__contains__这个特殊方法，通过方法中的语句返回一个布尔型的值。 此外，可以看到代码中有这样一句调用len(contain)，它就是前面提到的特殊方法__len__的功能，它可以给类添加一个获取序列长度的功能，当使用len(instance)时会调用__len__方法中的程序。 算数运算用于实现算数运算的有以下类的特殊方法的有以下几个， 运算 代码 特殊方法 加法 a + b __add__ 减法 a - b __sub__ 乘法 a * b __mul__ 除法 a / b __truediv__ 向下取整除法 a // b __floordiv__ 取余 a % b __mod__ 以一段代码举例说名加法与乘法的使用， 123456789101112131415161718192021class Operation(object): def __init__(self, value): self.value = value def __add__(self, other): return Operation(self.value + other.value) def __mul__(self, other): return Operation(self.value * other.value) def __str__(self): return "the value if &#123;&#125;".format(self.value) a = Operation(3)b = Operation(5)print(a + b)print(a * b)# 输出the value if 8the value if 15 同理，其他几种算法运算的使用方法同加法、乘法相同。 比较运算类的特殊方法不仅提供了算术运算，还提供了比较运算的特殊方法，它们分别是，| 运算 | 代码 | 特殊方法 || —————— | ——— | ———————— || 等于 | a == b | __eq__ || 不等 | a != b | __ne__ || 大于 | a &gt; b | __gt__ || 小于 | a &lt; b | __lt__ || 大于等于 | a &gt;= b | __ge__ || 小于等于 | a &lt;= b | __le__ | 以一段代码解释比较运算符的使用， 123456789101112131415class Cmp(object): def __init__(self, value): self.value = value def __eq__(self, other): return self.value == other.value def __gt__(self, other): return self.value &gt; other.valuea = Cmp(3)b = Cmp(3)a == b# 输出True 可以看出，比较运算和算术运算的使用非常相似。 字典功能我们可以通过如下几个特殊方法为类添加如同字典一样的功能，| 运算 | 代码 | 特殊方法 || —————— | ——— | ———————— || 取值 | x[key] | __setitem__ || 设置值 | x[key]=value | __getitem__ || 删除值 | del x[key] | __delitem__ | 下面以一段代码举例说明， 123456789101112131415161718192021222324252627282930313233class Dictionaries(object): def __setitem__(self, key, value): self.__dict__[key] = value def __getitem__(self, key): return self.__dict__[key] def __delitem__(self, key): del self.__dict__[key]diction = Dictionaries()diction["one"] = 1diction["two"] = 2diction["three"] = 3diction['three']del diction['three']diction['three']# 输出3---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-57-dfe1a566046b&gt; in &lt;module&gt;()----&gt; 1 diction['three']&lt;ipython-input-55-21dcfd1e91cb&gt; in __getitem__(self, key) 4 5 def __getitem__(self, key):----&gt; 6 return self.__dict__[key] 7 8 def __delitem__(self, key):KeyError: 'three' 可以看出，当删除键值为three的值之后再次去获取会报错。 其他除了上述提到的特殊方法之后，Python还有很多特殊方法，这里不一一举例说明，下面列举出这些特殊方法以及它们的功能和使用方法，如果感兴趣的可以对应的去查找文档学习。 运算 代码 特殊方法 类析构函数 del instant __del__ 格式化字符串 format(x, format_spec) __format__ 遍历迭代器 iter(list) __iter__ 取迭代器下一个值 next(list) __next__ 列出类的所有属性和方法 dir(instance) __dir__ 自定义散列值 hash(instance) __hash__ 自定义拷贝 copy.copy(instance) __copy__ 自定义深层拷贝 copy.deepcopy(instance) __deepcopy__ 上下文环境布尔值 if instance: __bool__ 当然，除了这些，Python还有其他的特殊方法，例如逻辑运算、按位运算等，感兴趣的可以参考官方文档仔细学习一下，本文仅列举一些相对常用的一些特殊方法。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号【平凡而诗意】回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十三讲：批量归一化]]></title>
    <url>%2F2019%2F08%2F24%2Fcnn-bn%2F</url>
    <content type="text"><![CDATA[前言当我们用一些数据做一个预测系统时，我们首先需要对数据进行预处理，例如标准化、正则化、滑动窗口等，比如常用的Z-score、最大最小标准化，它能将数据转化为同一个量级，这样的话能够保证数据的稳定性、可比性。 这些标准化方法在浅层神经网络中已经足够使用，效果已经很不错。但是在深度学习中，网络越来越深，使用这些标准化方法就难以解决相应的问题。 为什么需要批量归一化？ 在训练过程中，每层输入的分布不断的变化，这使得下一层需要不断的去适应新的数据分布，在深度神经网络中，这让训练变得非常复杂而且缓慢。对于这样，往往需要设置更小的学习率、更严格的参数初始化。通过使用批量归一化(Batch Normalization, BN)，在模型的训练过程中利用小批量的均值和方差调整神经网络中间的输出，从而使得各层之间的输出都符合均值、方差相同高斯分布，这样的话会使得数据更加稳定，无论隐藏层的参数如何变化，可以确定的是前一层网络输出数据的均值、方差是已知的、固定的，这样就解决了数据分布不断改变带来的训练缓慢、小学习率等问题。 在哪里使用批量归一化？ 批量归一化是卷积神经网络中一个可选单元，如果使用BN能够保证训练速度更快，同时还可以具备一些正则化功能。 在卷积神经网络中卷积层和全连接层都可以使用批量归一化。 对于卷积层，它的位置是在卷积计算之后、激活函数之前。对于全连接层，它是在仿射变换之后，激活函数之前，如下所示： 1234conv_1 = tf.nn.conv2d()norm_1 = tf.nn.batch_normalization(conv_1)relu_1 = tf.nn.relu(norm_1)pool_1 = tf.nn.max_pool(relu_1) 以卷积层为例，网络架构的流程为： 卷积运算 批量归一化 激活函数 池化 批量归一化 在讲批量归一化之前，首先讲一下数据标准化处理算法Z-score。 Z-score标准化也成为标准差标准化，它是将数据处理成均值为0，方差为1的标准正态分布，它的转化公式为， x^{*}=\frac{x-\overline{x}}{\sigma}其中$x$是处理前的数据，$x^{*}$是处理后的数据，$\overline{x}$是原始数据的均值，$\sigma$是原始的标准差。这样的话就可以把数据进行标准化。 其实批量归一化在思想上和Z-score是有很多共通之处的。 在深度学习训练过程中会选取一个小批量，然后计算小批量数据的均值和方差， \boldsymbol{\mu}_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)}\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2}然后对数据进行归一化处理， \hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}}\boldsymbol{y}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}经过这样处理，就可以使得数据符合均值为$\boldsymbol{\mu}$、方差为$\sigma_{\mathcal{B}}^{2}$的高斯分布。 下面看一下原文中批量归一化的算法步骤： 获取每次训练过程中的样本 就算小批量样本的均值、方差 归一化 拉伸和偏移 这里要着重介绍一下最后一步尺度变换(scale and shift)，前面3步已经对数据进行了归一化，为什么还需要拉伸和偏移呢？ 因为经过前三步的计算使得数据被严格的限制为均值为0、方差为1的正态分布之下，这样虽然一定程度上解决了训练困难的问题，但是这样的严格限制网络的表达能力，通过加入$\gamma$和$\beta$这两个参数可以使得数据分布的自由度更高，网络表达能力更强。另外，这两个参数和其他参数相同，通过不断的学习得出。 tensorflow中BN的使用在tensorflow中可以直接调用批量归一化对数据进行处理，它的函数为， 1tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None) 来解释一下函数中参数的含义： x：输入的数据，可以是卷积层的输出，也可以是全连接层的输出 mean：输出数据的均值 variance：输出数据的方差 offset：偏移，就是前面提到的beta scale：缩放，前面提到的gamma variance_epsilon：一个极小的值，避免分母为0 更多内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用资源 | 推荐6个高赞有趣的Github项目]]></title>
    <url>%2F2019%2F08%2F23%2Fgithub-project%2F</url>
    <content type="text"><![CDATA[前言Github，应该很多同学都听过这个鼎鼎大名的开源项目托管平台。 刚开始接触github，我和大多数同学一样，仅仅把它当作一个开源项目搜索工具。比如当看到一篇文章，会去搜索一下看看作者有没有开源源代码，仅此而已，因此一直以来对github的依赖都不太强。 最近几个月以来，我每天会特意抽出一段时间去github看一下，看看近期有没有什么热门有趣的项目，慢慢的，在github上发现了很多不错的项目，久而久之就对其产生了依赖。现在每天都会去看一下，在这个过程总的确从这些优质的开源项目上学到了很多，收获了很多，下面我就推荐5个我个人觉得不错的开源项目。 free-programming-books-zh_CN项目地址：https://github.com/justjavac/free-programming-books-zh_CN star：54k+ 在从事IT、互联网相关的开发过程中，我们会用到各种各样的知识，linux、数据库、编程语言、虚拟化等。这样就需要我们不断的去学习，很多时间和金钱比较充足的可以选择一些课程或者买一些书籍，但是大多数人是不会把所用到知识相关的书籍都购买一遍。这样就面临一个问题：我们该去哪获取相应的学习资源？ free-programming-books-zh_CN收集了计算机领域很多知名的书籍，它包括但不限于以下种类： 各种编程语言 版本控制 数据库 大数据 操作系统 编译原理 web … 有了这个项目就再也不用担心学习资源的问题了，它手机了计算机各个分支相关的经典书籍和优质学习资源，避免了在茫茫的网络中去搜索的麻烦。 sherlock项目地址：https://github.com/sherlock-project/sherlock star：6.8k+ 在到处充斥着互联网、社交的时代，用户名是每个人都不陌生的词汇，当我们注册一个社交网站，例如知乎、微博等，需要起一个名称。很多人都喜欢独一无二、与众不同，但是在这么多用户名字，一不小心就和别人重复了，sherlock这个项目就可以解决这个问题，它能够在不同的社交网站上搜索是否存在指定的用户名，这样的话你就可以看到自己起的用户名有没有重复？有哪些重复。 weekly项目地址：http://link.zhihu.com/?target=https%3A//github.com/ruanyf/weeklystar：8.1k+ 这是一个科技爱好者周刊。 现在是一个信息爆炸的社会，各种新媒体、自媒体，每天各个APP有看不完、层出不穷的新闻，但是，我个人认为大多数自媒体的水平有待商榷，在他们看来，一个吸引人的标题比实际的内容还要重要。所以我很苦恼，每天喜欢看看新闻，但是花费几十分钟后发现都是一些乱七八糟没有价值的新闻。 直到几个月前我在github看到weekly这个项目，它每周五定期更新一次，慢慢的，周五在我心里有了一些期待，期待着这个项目更新周刊。 为什么它如此吸引我？ 它与众不同，而且都是经过筛选的一些新奇有趣的新闻，我觉得称其为新闻，但它不仅限于新闻，它包含如下内容： 新奇的资讯 优质中文、英文文章 高效工具 资源 精选图片 文摘 言论 UnblockNeteaseMusic项目地址：http://link.zhihu.com/?target=https%3A//github.com/nondanee/UnblockNeteaseMusic star：4.4k+ 从这个项目的名称即可看出它的功能，unblock netease music。 网易云音乐是很多人喜欢的一款音乐播放器，我也不例外，但是发现它上面的音乐越来越少，当你想听一首歌时发现，它竟然是灰色的，也就是不能听。 有了这个项目，它可以从QQ / 虾米 / 百度 / 酷狗 / 酷我 / 咕咪 / JOOX等音乐源寻找资源进行替换，也就是说，有了UnblockNeteaseMusic+网易云音乐，你可以听来自不同音乐源的歌曲。 ChineseBQB项目地址：https://github.com/zhaoolee/ChineseBQB star：5k+ 这个一个表情包博物馆，目前共收录了3319个表情包，包含各种各样热门、搞笑的表情包，有了这个github项目，再也不用为”斗图”担心了。 先来几个看一下， LeetCodeAnimation项目地址：https://github.com/MisterBooo/LeetCodeAnimation star：38k+ 娱乐之后不得不说一些沉重的话题，已经是8月底了，大批量的校招马上就要开始了 ~ 不知道关注我的同学里有多少位要参加今年的校园招聘，如果有，首先预祝各位找到理想的工作！ 其次，就是推荐一份不错的学习资源。 LeetCode，这个大名鼎鼎的平台应该很多参加过校招或者即将参加校招的同学应该都有所耳闻，是很多参加互联网、IT方向校招同学的必经之路，甚至周围有同事说”没有刷过LeetCode的校招，是不完整的！”。 github上有关leetcode的项目有很多，但是99%都是如出一辙，把leetcode上的题目做一下或者寻找一些答案，然后上传到github，只有静态的代码和简短的文字描述，这对于很多初学者是很难以理解的。 LeetCodeAnimation这个项目却与众不同，它通过动画的形式来阐述不同算法的解题思路，更加生动形象，下面就看一个例子—无重复字符的最长子串， 更多内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十二讲：激活函数]]></title>
    <url>%2F2019%2F08%2F21%2Factivation%2F</url>
    <content type="text"><![CDATA[完整代码链接：aiLearnNotes 前言激活函数不仅对于卷积神经网络非常重要，在传统机器学习中也具备着举足轻重的地位，是卷积神经网络模型中必不可少的一个单元，要理解激活函数，需要从2个方面进行讨论： 什么是激活函数？ 为什么需要激活函数？ 什么是激活函数？ 对于神经网络，一层的输入通过加权求和之后输入到一个函数，被这个函数作用之后它的非线性性增强，这个作用的函数即是激活函数。 为什么需要激活函数？ 试想一下，对于神经网络而言，如果没有激活函数，每一层对输入进行加权求和后输入到下一层，直到从第一层输入到最后一层一直采用的就是线性组合的方式，根据线性代数的知识可以得知，第一层的输入和最后一层的输出也是呈线性关系的，换句话说，这样的话无论中加了多少层都没有任何价值，这是第一点。 第二点是由于如果没有激活函数，输入和输出是呈线性关系的，但是现实中很多模型都是非线性的，通过引入激活函数可以增加模型的非线性，使得它更好的拟合非线性空间。 目前激活函数有很多，例如阶跃函数、逻辑函数、双曲正切函数、ReLU函数、Leaky ReLU函数、高斯函数、softmax函数等，虽然函数有很多，但是比较常用的主要就是逻辑函数和ReLU函数，在大多数卷积神经网络模型中都是采用这两种，当然也有部分会采用Leaky ReLU函数和双曲正切函数，本文就介绍一下这4个激活函数长什么样？有什么优缺点？在tensorflow中怎么使用？ SigmoidSigmoid函数的方程式为： f(x)=\sigma(x)=\frac{1}{1+e^{-x}} 绘图程序： 123456def sigmoid(): x = np.arange(-10, 10, 0.1) y = 1 / (1+np.exp(-x)) plt.plot(x, y) plt.grid() plt.show() Sigmoid函数就是前面所讲的逻辑函数，它的主要优点如下： 能够将函数压缩至区间[0, 1]之间，保证数据稳定，波动幅度小 容易求导 缺点： 函数在两端的饱和区梯度趋近于0，当反向传播时容易出现梯度消失或梯度爆炸 输出不是0均值(zero-centered)，这样会导致，如果输入为正，那么导数总为正，反向传播总往正方向更新，如果输入为负，那么导数总为负，反向传播总往负方向更新，收敛速度缓慢 对于幂运算和规模较大的网络运算量较大 双曲正切函数双曲正切函数方程式： f(x)=\tanh (x)=\frac{\left(e^{x}-e^{-x}\right)}{\left(e^{x}+e^{-x}\right)} 绘图程序： 123456def tanh(): x = np.arange(-10, 10, 0.1) y = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)) plt.plot(x, y) plt.grid() plt.show() 可以看出，从图形上看双曲正切和Sigmoid函数非常类似，但是从纵坐标可以看出，Sigmoid被压缩在[0, 1]之间，而双曲正切函数在[-1, 1]之间，两者的不同之处在于，Sigmoid是非0均值(zero-centered)，而双曲是0均值的，它的相对于Sigmoid的优点就很明显了： 提高了训练效率 虽然双曲正切函数解决了Sigmoid函数非0均值的问题，但是它依然没有解决Sigmoid的两位两个问题，这也是tanh的缺点： 梯度消失和梯度爆炸 对于幂运算和规模较大的网络运算量较大 ReLUReLU函数方程式： f(x)=\left\{\begin{array}{ll}{0} & {\text { for } x]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第三讲：类的特殊方法(上篇)]]></title>
    <url>%2F2019%2F08%2F18%2Fpython-3%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言Python是一种面向对象的语言，而特殊方法又是Python类中一个重点，因此学习Python类的特殊方法能够有助于设计出更加简洁、规范的代码架构。 Python类的特殊方法又称为魔术方法，它是以双下划线包裹一个词的形式出现，例如__init__。特殊方法不仅可以可以实现构造和初始化，而且可以实现比较、算数运算，此外，它还可以让类像一个字典、迭代器一样使用，可以设计出一些高级的代码，例如单例模式。 面向对象这个词大家应该都不陌生，在C++、Java等面向对象的语言中也经常出现，要想理解面向对象，首先要理解4个概念之间的关系：类、对象、实例、方法。 类：类是一种由不同属性、不同数据组成的一个集合。用直白的话来描述，它是由多种对象组成的一个组合，例如人是一个类，那么它包含男人、女人、儿童等对象。例如三角形是一个类，那么它包含等腰三角形、直角三角形、等边三角形等对象。 对象：前面介绍类中已经提到了对象这个词汇，一句话总结：对象具有具体状态和行为。例如直角三角形，它具有特定的状态和属性。 实例：对象就是类的一个实例。也许这有点绕，的确对象与实例之间的概念非常模糊。你可以理解为对象是一个概念性的存在，而实例是采取行为、动作的载体，以一段代码举例， 1234class Animal(object): passanimal = Animal() 其中Animal是一个类，而animal是一个实例，它可以访问类内的方法，实施“动作”和“行为”。 方法：定义在类外部的函数叫做函数，定义在类内部的函数称为方法。 这些概念在Python面向对象编程中非常概念，只有理解这些概念才能在后续学习中更加容易理解，对上述这些概念有一个简单的了解，在后续的讲解中会更加轻松。 __new__与__init__之所以把这个放在第一个，因为这个不仅非常常用，而且很容易被误解，甚至很多知名的书籍中都把这个特殊方法弄错。 很多博客和个别书籍中都把__init__当作类似于C++的构造方法，其实这个理解是错误的。 1234567891011121314class Animal(object): def __new__(cls, *args, **kargs): instance = object.__new__(cls, *args, **kargs) print("&#123;&#125; in new method.".format(instance))# return instance # 不返回实例 def __init__(self): print("&#123;&#125; in init method.".format(self)) animal = Animal()# 输出&lt;__main__.Animal object at 0x000002BB03001CF8&gt; in new method. 以上面为例，我们对基类中的__new__进行重构，不让它返回实例，可以从输出结果可以看出，程序没有进入__init__方法。这是因为__new__是用来构造实例的，而__init__只是用来对返回的实例进行一些属性的初始化，我们在写一个类的时候首先都会写一个__init__方法去初始化变量，却很少使用__new__，因此就容易忽略__new__，其实在我们继承基类object(例如，class Animal(object))时同时就从基类中继承了__new__方法，所以就不需要重新在子类中实现，如果把上述注释取消掉，再看一下， 123456789101112131415class Animal(object): def __new__(cls, *args, **kargs): instance = object.__new__(cls, *args, **kargs) print("&#123;&#125; in new method.".format(instance)) return instance def __init__(self): print("&#123;&#125; in init method.".format(self)) animal = Animal()# 输出&lt;__main__.Animal object at 0x000002BB03001B00&gt; in new method.&lt;__main__.Animal object at 0x000002BB03001B00&gt; in init method. 可以看出，程序先运行到new中，然后进入init方法。 对于__init__应该都很熟悉，为什么很少使用__new__呢？因为大多数情况下我们是用不到它的。但是存在的即是合理的，它自然有自己的价值。 __new__在哪些场景能够用到呢？ 当实现一些高级的软件设计模式可能会用到__new__方法，它主要有以下几点用处， 重构一些不可变方法，例如，int, str, tuple 实现单例模式(Singleton Pattern) 这里着重介绍一下单例模式。 单例模式是一种常用的软件设计模式，有时候我们需要严格的限制一个类只有一个实例存在，一个系统只有一个全局对象，这样有利于协调系统的整体行为。 先看一下我们常用的写法， 1234567891011class NewInt(object): passnew1 = NewInt()new2 = NewInt()print(new1)print(new2)# 输出&lt;__main__.NewInt object at 0x000002BB03001390&gt;&lt;__main__.NewInt object at 0x000002BB02FF4080&gt; 从输出可以看出，上述两个实例new1、new2地址不同，是两个实例。 然后通过__new__实现单例模式， 12345678910111213141516class NewInt(object): _singleton = None def __new__(cls, *args, **kwargs): if not cls._singleton: cls._singleton = object.__new__(cls, *args, **kwargs) return cls._singletonnew1 = NewInt()new2 = NewInt()print(new1)print(new2)# 输出&lt;__main__.NewInt object at 0x000002BB02FF6080&gt;&lt;__main__.NewInt object at 0x000002BB02FF6080&gt; 地址相同，指向同一个对象，所以每次实例化产生的实例都是完全相同的。 __enter__与__exit__在介绍这两个特殊方法之前我们首先讲一下with语句。 with语句主要用于对资源进行访问的场景，例如读取文件。以读取文件为例，我们可以使用open、close的方法，但是使用with语句有着无法比拟的优势。 首先就是简洁，你不需要再写file.close的语句去关闭文件。 其次，也是最重要的，它能够很好的做到异常处理，当处理过程中发生异常，它能够自动关闭、自动释放资源。 以读取文件来对比一下两个功能， 如果使用open、close方式需要打开、读取、关闭3个过程， 12345# file.txtfp = open("file.txt", "rb")fp.readline()fp.close() 而使用with语句只需要打开、读取两个过程，当执行完毕会自动关闭， 12with open("file.txt", "rb") as fp: fp.readline() 说了这么多with语句的好处，这和__enter__与__exit__有什么关系？ __enter__与__exit__就是实现with的类特殊方法。 以一段代码来解释这两个特殊方法的使用， 12345678910111213141516171819202122232425class FileReader(object): def __init__(self): print("in init method") def __enter__(self): print("int enter method") return self def __exit__(self, exc_type, exc_val, exc_tb): print("in exit method") del self def read(self): print("in read")# with语句调用with FileReader() as fr: fr.read() # 输出in init methodint enter methodin readin exit method 从上面输出可以看出，程序先进去init方法进行初始化，然后进入enter特殊方法，然后通过fr.read调用read()方法，最后退出时调用exit方法。 这就是enter与exit的调用过程， __enter__：初始化后返回实例 __exit__：退出时做处理，例如清理内存，关闭文件，删除冗余等 __str__与__repr__一句话描述这两个特殊方法的功能：把类的实例变为字符串。 我们都知道，我们可以用这种方法输出一个字符串， 1print("Hello world!") 那我们怎么能够像字符串一样把实例输出出来？ 可以通过__str__与__repr__来实现， 12345678910111213141516class Person(object): def __init__(self, name, age): self.name = name self.age = age def __str__(self): return "str: &#123;&#125; now year is &#123;&#125; years old.".format(self.name, self.age) def __repr__(self): return "repr: &#123;&#125; now year is &#123;&#125; years old.".format(self.name, self.age)person = Person("Li", 27)print(person)# 输出str: Li now year is 27 years old. 可以看出，当使用print语句打印实例person时，能够像输出字符串那样把实例信息输出出来。 但是可以看出，程序进入__str__方法，并没有进入__repr__，这就引出了这两个方法的不同之处， __str__：用于用户调用 __repr__：用于开发人员调用 这似乎不太好理解，因为对于写程序的我们无法理解，何为用户？何为开发人员？ 简单的来说，__str__是用些Python脚本(.py)时使用，用print语句输出字符串信息。__repr__是我们在交互式环境下测试使用，例如cmd下的Python、ipython，例如在交互式环境下调用， 123&gt;&gt;&gt; person = Person("li", 27)&gt;&gt;&gt; personrepr: li now year is 27 years old. 更为简单的理解就是：__str__需要用print语句打印，__repr__只需输入实例名称即可。 __setattr__、__getattr__、__getattribute__与__delattr__了解上述这4个方法之前，我们先来解释一下什么是属性？ 也许很多同学已经清楚，但是我觉得还是有必要介绍一下，因为这是要讲的这3个特殊方法的关键。 1234567891011121314class Person(object): def __init__(self, name, age, home, work): self.name = name self.age = age self.home = home self.work = workperson = Person("Li", 27, "China", "Python")print(person.name)print(person.age)# 输出Li27 例如上面我们定义一个Person类，name、age、home、work就是它的属性，当实例化之后我们可以通过点.来访问它的属性。 我可以可以通过传入参数，赋值给self来定义类的属性，但是这样未免太固定了，当实例化之后就不能更改它的属性了，如果我们想获取、添加、删除属性怎么办？这就用到这里要讲的4个特殊方法，__setattr__、__getattr__、__getattribute__与__delattr__，它们的功能分别是， __setattr__：设置属性 __getattr__：访问不存的属性时调用，可能会有同学有疑问，访问不存的属性要它干吗？可以用来做异常处理！ __getattribute__：访问存在的属性，如果访问属性不存在的时候随后会调用__getattr__ __delattr__：删除属性 以一个例子来说明一下， 1234567891011121314151617181920212223242526272829303132class Person(object): def __init__(self, name): self.name = name def __setattr__(self, key, value): object.__setattr__(self, key, value) def __getattribute__(self, item): print("in getattribute") return object.__getattribute__(self, item) def __getattr__(self, item): try: print("in getattr") return object.__getattribute__(self, item) except: return "Not find attribute: &#123;&#125;".format(item) def __delattr__(self, item): object.__delattr__(item)person = Person("Li")print(person.name)print(person.age)# 输出in getattributeLiin getattributein getattrNot find attribute: age 从上面输出来看一下就可以明白，当获取属性name时，由于已经有了，则进入__getattribute__中，获取对应的属性，当获取属性age时，由于没有这个属性，则先进入__getattribute__，然后进入__getattr__，没有找到属性返回异常信息。 然后再来看一下看一下设置属性和删除属性， 123456789101112person.age = 27print(person.age)delattr(person, "age")print(person.age)# 输出in getattribute27in delattrin getattributein getattrNot find attribute: age 从输出结果可以看出，通过instance.attribute的方式可以设置属性，通过delattr可以删除属性。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号【平凡而诗意】回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十一讲：卷积层、池化层与填充]]></title>
    <url>%2F2019%2F08%2F16%2Fcv-cnn-pool%2F</url>
    <content type="text"><![CDATA[前言从2012年AlexNet成名之后，CNN如同雨后春笋一样，出现了各种各样的Net，其中也有很多知名的，例如VGG、GoogleNet、Faster R-CNN等，每个算法都在前面研究工作的基础上做出了很大的改进，但是这些CNN模型中主要使用的组件却有很多重叠之处，这个组件主要有： 卷积层 池化层 激活函数 优化函数 全连接层 Dropout 批量正则化 填充padding …… 其实一个CNN网络的模型搭建过程非常容易，现在有很多优秀的机器学习框架，例如tensorflow、pytorch、mxnet、caffe、keras等，借助这些机器学习框架搭建一个CNN网络模型只需要几十行代码即可完成，而且使用到的函数屈指可数，难度并不大。而上述提到的这些组件却是CNN中非常核心的概念，了解它们是什么？有什么价值？在哪里起作用？掌握这些之后再回头看这些CNN模型就会发现轻而易举，因此，这几节会先把上述这些技术介绍一下，然后逐个讲解如何一步一步搭建那些成熟优秀的CNN模型。 由于上述每个技术都涉及很多知识点，本文为了效率就用简单的语言介绍它是什么？有什么价值？具体详细的内容可以阅读文章或者外网资料详细了解，本文主要介绍3点： 卷积层 池化层 填充padding 卷积层介绍 卷积神经网络(convolutional neural network)，从它的名称就可以看出，卷积是其中最为关键的部分。在前面讲解图像去噪和图像分割中提到了一些用于分割和去噪的算法，例如sobel算子、中值滤波，其实卷积的概念和这些有相同之处。 把输入图像看作是一个n维矩阵，然后拿一个mm维(m&lt;n)的卷积核(或者称为滤波器)，从图像的左上角开始沿着从左至右、*从上之下进行”扫描”，每当移动到一个窗口后和对应的窗口做卷积运算(严格的说是互相关运算)，用直白的话来说就是对应元素相乘之后加和。 移动过程中涉及一个重要的概念—步长(stride)，它的意思就是”扫描”过程中每次移动几个像素，如果步长stride=1，那么从左至右、从上之下逐个像素的移动。 以上图二维卷积运算为例，输入图像为一个5*5的矩阵，卷积核为3*3，以步长stride=1进行卷积运算，在左上角这个窗口每个对应元素先相乘再加和，即， 0*0+1*1+2*2+1*5+2*6+0*7+2*0+1*1+0*2=23以这种方式逐个窗口进行计算，就得到图中等号右边的输出结果。 tensorflow使用 在tensorflow中关于卷积层的函数为， 1tensorflow.nn. conv2d(input, filter, strides, padding) 其中参数分别为： input：输入数据或者上一层网络输出的结果 filter：卷积核，它的是一个1*4维的参数，例如filter=[5, 5, 3, 96]，这4个数字的概念分别是卷积核高度、卷积核宽度、输入数据通道数、输出数据通道数 strides：这是前面所讲的步伐，同卷积核一样，它也是一个1*4维的参数，例如strides=[1, 2, 2, 1]，这4个数字分别是batch方向移动的步长、水平方向移动的步长、垂直方向移动的步长、通道方向移动的步长，由于在运算过程中是不跳过batch和通道的，所以通常情况下第1个和第4个数字都是1 padding：是填充方式，主要有两种方式，SAME, VALID，后面会讲什么是填充 池化层介绍 池化层和卷积层一样，是CNN模型必不可少的一个部分，在很多卷积层后会紧跟一个池化层，而且在统计卷积神经网络时，池化层是不单独称为网络层的，它与卷积层、激活函数、正则化同时使用时共同称为1个卷积层。 池化层又成为下采样或者欠采样，它的主要功能是对于特征进行降维，压缩数据和参数量，避免过拟合，常用的池化方式有两种： 最大池化 平均池化 以最大池化为例介绍一下它是怎么实现的， 和卷积层类似，池化层也有窗口和步长的概念，其中步长在里面的作用也是完全相同的，就是窗口每次移动的像素个数，所以不再赘述。 池化层的窗口概念和卷积层中是截然不同的，在卷积层中每移动到一个窗口，对应的卷积核和输入图像做卷积运算。而在池化层中，窗口每移动到一个位置，就选择出这个窗口中的最大值输出，如果是平均池化就输出这个窗口内的平均值。 tensorflow使用 tensorflow中池化运算的函数为， 1tensorflow.nn.max_pool(value, ksize, strides, padding) 从函数的参数即可看出来，它和卷积层非常相似，它的参数概念分别是， value：输入数据或者上一层网络输出的结果 ksize：卷积核，它的是一个1*4维的参数，例如ksize=[1, 3, 3, 1]，这4个数字的概念分别是batch维度池化窗口、池化窗口高度、池化窗口宽度、通道维度窗口尺寸，由于在batch和通道维度不进行池化，所以通常情况下第1和第4个元素为1 strides：这和卷积层中相同 padding：这和卷积层中的也相同 填充在前面讲解卷积层和池化层时都提到了一个概念—填充，可见它是非常重要的。什么是填充？SAME, VALID这两种填充方式又有什么区别？下面来介绍一下。 从前面卷积层和池化层可以看出，卷积层和池化层的输出尺寸大小和选取的窗口大小有着密切关系，以卷积层为例，上述输入为5*5，但是输出为3*3，输出尺寸变小了，而且在输入图像的四周的元素只被卷积了一次，中间的元素却被利用多次，也就是说，如果是一副图像，图像四周的信息未被充分提取，这就体现了填充的价值， 保持边界信息 使得输入输出图像尺寸一致 那怎么样达到上述这2个目的？就是通过填充，一般情况下是在图像周围填充0，如下， 如上图所示，在输入图像周围填充0，然后通过卷积运算，输入和输出的尺寸都为5*5。当然，这是针对卷积核为3*3情况下，外层填充1层，具体填充几层，要根据卷积核大小而定。 然后回到前面所提到的，tensorflow中填充padding参数有两个选项：SAME, VALID，它们有什么区别呢 ？ VALID：不进行填充 SAME：填充0，使得输出和输入的尺寸相同，就如同上面这个例子。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 推荐一款提高Python编程效率的神器]]></title>
    <url>%2F2019%2F08%2F13%2Fkite%2F</url>
    <content type="text"><![CDATA[前言“AI自动补全工具”，这个其实很久之前就有所耳闻，但是我却始终没有去尝试，因为，在我看来这两年人工智能泡沫太严重，各行各业都在蹭AI的热度，我想，也许”AI自动补全工具”也只不过是一个噱头吧。 在工作中，对于Python开发我一直都是以pycharm为主力。它也是Python开发中非常知名的一款IDE，支持DEBUG、格式提示、快速补全等等，有着非常吸引人的优点。尽管它非常臃肿、启动速度非常缓慢，但是对于追求补全速度的我来说，我还是选择忍受它的种种不足。 直到前不久在开发过程中发生的几次问题让我忍无可忍，我决心换掉这款工具，主要有如下几个原因： 内存占用大：16G的内存，pycharm占据了1G以上，使得电脑卡顿 license服务器崩溃：购买的license总是莫名其妙的出问题 臃肿：pycharm很强大，但是它的强大是建立在开启了很多辅助工具的基础上，这使得它非常臃肿卡顿 于是，我开始尝试不同的工具，VIM、vscode、sublime等。其中VIM在补全速度方面还可以，但是在windows下无法使用，而我有时在服务器下开发、有时会在windows下开发。至于vscode和sublime，界面和启动速度等都没的说，但是补全功能太弱，虽然配置了几款所谓的强大插件，但是依然跟不上编码的速度，于是，我又回到了pycharm，直到我遇到这款神奇的工具—kite，让我有一种柳暗花明的感觉，实在太强大了。 甚至Python之父Guido van Rossum都说I really love the line-of-code completions in the new kite.com，可见这款工具多么强大。 有了这一款工具，再也不用繁琐的配置sublime、vscode中各种插件和设置项了。 kite安装 kite是一款安装包+插件的工具，首先需要到官网下载kite的安装包，安装作为引擎，安装之后打开相应的编辑器或IDE安装kite的插件，然后就可以使用了，不用像sublime、vscode那样需要安装一堆插件还要到设置中配置Python路径之类的。 安装包下载可以直接到官网进行下载： https://www.kite.com/download/ 我把安装包进行共享了，如果访问官网速度比较慢，无法下载的话，可以在公众号后台回复kite获取。 双击安装 为什么推荐这款工具？ 一款好的编程工具能够让编码效率事半功倍，它不仅避免我们逐个敲击代码，还避免我们去记忆一些函数的名称。目前有很多有名气的IDE\编辑器，pycharm、eclipse、spyder、Atom、sublime、vscode等，每个人都有自己的习惯和偏好，所以每个人心中都有自己最认可的工具。但是不可否认，pycharm在Python开发方面是使用最为广泛的一款，它最吸引我的一点就是补全速度。虽然sublime、vscode等也可以通过配置插件来实现Python自动补全，但是速度和效果等方面始终和pycharm有着巨大差距。 所以长久以来，尽管我也体会到它的种种缺点，我还是在坚持使用pycharm，直到最近我遇到这款kite之后。它是一款基于人工智能的代码补全和文档查询工具。我觉得完全可以脱离臃肿的pycharm，利用sublime、vscode这些轻量的编辑器与kite结合使用，即可以避免缓慢的开启速度，还可以实现不亚于pycharm的补全速度。 当然，kite的功能不仅限于补全，它主要包括： 代码自动补全 文档查询 代码自动补全 直接来看一下它的补全速度，非常快。 目前的代码自动补全工具大多数都是通过上下文匹配、扫描第三方库的方式实现补全，这样都是通过你输入一个单词，它去扫描，可想而知，速度自然会很慢。但是kite则不同，它是通过人工智能的方式进行补全，当你属于一个单词，它能够像谷歌搜索那样，预测你接下来会输入什么，并按相关性进行排序。 它不仅支持Python内置函数补全，还支持第三方工具包的补全。此外，它还支持一些模块的补全，例如if…main…，能够极大的节省编码的时间，提升编码效率，经过统计，Kite的人工智能可以帮助减少47%的击键次数。 文档查询 当我们使用一个第三方库时，例如numpy、tensorflow、scipy等，我们对其中很多函数怎么使用？需要传入哪些参数并不清楚。当然你可以上网搜索一下，但是我认为现在网上的学习资料鱼龙混杂，最好的方法还是看文档，这样比较权威、严谨。 但是问题是去哪看文档？而且，找文档也很耗时间啊。 kite不仅可以自动补全的问题，它还可以解决文档查询的问题。 打开kite，输入你想搜索的模块，即可找到你想要看的文档。而且它非常简洁， 怎么使用 传入参数 返回值 以最简单明了的几句话概括这个模块的使用方法。 支持平台 kite是一块完全免费的工具，它目前支持以下两个平台： windows linux 支持工具 kite支持以下几种IDE\编辑器： pycharm Atom vscode sublime vim 因此，你有多种可选项，可以根据自己的喜好进行配置。即便你对目前所使用的编辑工具补全速度已经很满意了，我认为也不妨使用一下kite，用它作为一款文档查询工具，能够使得阅读文档效率大大提升。 支持语言 官方把它定义为一款Python自动补全工具，但是我在使用vscode开发javascript时发现kite同样能够实现补全，而且效果也不错，至于C++、Java等其他语言，我没有尝试，暂不清楚，感兴趣的可以试一下。]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第二讲：装饰器]]></title>
    <url>%2F2019%2F08%2F10%2F1-decorators%2F</url>
    <content type="text"><![CDATA[完整代码请查看github项目: advance-python 前言前段时间我发了一篇讲解Python调试工具PySnooper的文章，在那篇文章开始一部分我简单的介绍了一下装饰器，文章发出之后有几位同学说”终于了解装饰器的用法了”，可见有不少同学对装饰器感兴趣。但是那篇文章主要的目的是在介绍PySnooper，所以没有太深入的展开讲解装饰器，于是在这里就详细的介绍一些装饰器的使用。 装饰器是Python中非常重要的一个概念，如果你会Python的基本语法，你可以写出能够跑通的代码，但是如果你想写出高效、简洁的代码，我认为离不开这些高级用法，当然也包括本文要讲解的装饰器，就如同前面提到的代码调试神器PySnooper一样，它就是主要通过装饰器调用的方式对Python代码进行调试。 什么是Python装饰器？ 顾名思义，从字面意思就可以理解，它是用来"装饰"Python的工具，使得代码更具有Python简洁的风格。换句话说，它是一种函数的函数，因为装饰器传入的参数就是一个函数，然后通过实现各种功能来对这个函数的功能进行增强。 为什么用装饰器？ 前面提到了，装饰器是通过某种方式来增强函数的功能。当然，我们可以通过很多方式来增强函数的功能，只是装饰器有一个无法替代的优势--简洁。 你只需要在每个函数上方加一个@就可以对这个函数进行增强。 在哪里用装饰器？ 装饰器最大的优势是用于解决重复性的操作，其主要使用的场景有如下几个： - 计算函数运行时间 - 给函数打日志 - 类型检查 当然，如果遇到其他重复操作的场景也可以类比使用装饰器。 简单示例 前面都是文字描述，不管说的怎么天花烂坠，可能都无法体会到它的价值，下面就以一个简单的例子来看一下它的作用。 如果你要对多个函数进行统计运行时间，不使用装饰器会是这样的， 12345678910111213141516171819202122from time import time, sleepdef fun_one(): start = time() sleep(1) end = time() cost_time = end - start print("func one run time &#123;&#125;".format(cost_time)) def fun_two(): start = time() sleep(1) end = time() cost_time = end - start print("func two run time &#123;&#125;".format(cost_time)) def fun_three(): start = time() sleep(1) end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) 在每个函数里都需要获取开始时间start、结束时间end、计算耗费时间cost_time、加上一个输出语句。 使用装饰器的方法是这样的， 1234567891011121314151617181920def run_time(func): def wrapper(): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): sleep(1) @run_timedef fun_two(): sleep(1) @run_timedef fun_three(): sleep(1) 通过编写一个统计时间的装饰器run_time，函数的作为装饰器的参数，然后返回一个统计时间的函数wrapper，这就是装饰器的写法，用专业属于来说这叫闭包，简单来说就是函数内嵌套函数。然后再每个函数上面加上@run_time来调用这个装饰器对不同的函数进行统计时间。 可见，统计时间这4行代码是重复的，一个函数需要4行，如果100个函数就需要400行，而使用装饰器，只需要几行代码实现一个装饰器，然后每个函数前面加一句命令即可，如果是100个函数，能少300行左右的代码量。 带参数的装饰器通过前面简单的例子应该已经明白装饰器的价值和它的简单用法：通过闭包来实现装饰器，函数作为外层函数的传入参数，然后在内层函数中运行、附加功能，随后把内层函数作为结果返回。 除了上述简单的用法还有一些更高级的用法，比如用装饰器进行类型检查、添加带参数的的装饰器等。它们的用法大同小异，关于高级用法，这里以带参数的装饰器为例进行介绍。 不要把问题想的太复杂，带参数的装饰器其实就是在上述基本的装饰器的基础上在外面套一层接收参数的函数，下面通过一个例子说明一下。 以上述例子为基础，前面的简单示例输出的信息是， 123func three run time 1.0003271102905273func three run time 1.0006263256072998func three run time 1.000312328338623 现在我认为这样的信息太单薄，需要它携带更多的信息，例如函数名称、日志等级等，这时候可以把函数名称和日志等级作为装饰器的参数，下面来时实现以下。 1234567891011121314151617181920212223242526def logger(msg=None): def run_time(func): def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("[&#123;&#125;] func three run time &#123;&#125;".format(msg, cost_time)) return wrapper return run_time@logger(msg="One")def fun_one(): sleep(1) @logger(msg="Two")def fun_two(): sleep(1) @logger(msg="Three")def fun_three(): sleep(1) fun_one()fun_two()fun_three() 可以看出，我在示例基本用法里编写的装饰器外层又嵌套了一层函数用来接收参数msg，这样的话在每个函数(func_one、func_two、func_three)前面调用时可以给装饰器传入参数，这样的输出结果是， 123[One] func three run time 1.0013229846954346[Two] func three run time 1.000720500946045[Three] func three run time 1.0001459121704102 自定义属性的装饰器上述介绍的几种用法中其实有一个问题，就是装饰器不够灵活，我们预先定义了装饰器run_time，它就会按照我们定义的流程去工作，只具备这固定的一种功能，当然，我们前面介绍的通过带参数的装饰器让它具备了一定的灵活性，但是依然不够灵活。其实，我们还可以对装饰器添加一些属性，就如同给一个类定义实现不同功能的方法那样。 以输出日志为例，初学Python的同学都习惯用print打印输出信息，其实这不是一个好习惯，当开发商业工程时，你很用意把一些信息暴露给用户。在开发过程中，我更加鼓励使用日志进行输出，通过定义WARNING、DEBUG、INFO等不同等级来控制信息的输出，比如INFO是可以给用户看到的，让用户直到当前程序跑到哪一个阶段了。DEBUG是用于开发人员调试和定位问题时使用。WARING是用于告警和提示。 那么问题来了，如果我们预先定义一个打印日志的装饰器， 123456def logger_info(func): logmsg = func.__name__ def wrapper(): func() log.log(logging.INFO, "&#123;&#125; if over.".format(logmsg)) return wrapper logging.INFO是打印日志的等级，如果我们仅仅写一个基本的日志装饰器logger_info，那么它的灵活度太差了，因为如果我们要输出DEBUG、WARING等级的日志，还需要重新写一个装饰器。 解决这个问题，有两个解决方法： 利用前面所讲的带参数装饰器，把日志等级传入装饰器 利用自定义属性来修改日志等级 由于第一种已经以统计函数运行时间的方式进行讲解，这里主要讲解第二种方法。 先看一下代码， 123456789101112131415161718192021222324252627282930313233343536import loggingfrom functools import partialdef wrapper_property(obj, func=None): if func is None: return partial(attach_wrapper, obj) setattr(obj, func.__name__, func) return funcdef logger_info(level, name=None, message=None): def decorate(func): logmsg = message if message else func.__name__ def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) @wrapper_property(wrapper) def set_level(newlevel): nonlocal level level = newlevel @wrapper_property(wrapper) def set_message(newmsg): nonlocal logmsg logmsg = newmsg return wrapper return decorate@logger_info(logging.WARNING)def main(x, y): return x + y 这里面最重要的是wrapper_property这个函数，它的功能是把一个函数func编程一个对象obj的属性，然后通过调用wrapper_property，给装饰器添加了两个属性set_message和set_level，分别用于改变输出日志的内容和改变输出日志的等级。 看一下输出结果， 12345main(3, 3)# 输出# WARNING:Test:main# 6 来改改变一下输出日志等级， 123456main.set_level(logging.ERROR)main(5, 5)# 输出# ERROR:Test:main# 10 输出日志等级改成了ERROR。 保留元信息的装饰器很多教程中都会介绍装饰器，但是大多数都是千篇一律的围绕基本用法在展开，少部分会讲一下带参数的装饰器，但是有一个细节很少有教程提及，那就是保留元信息的装饰器。 什么是函数的元信息？ 就是函数携带的一些基本信息，例如函数名、函数文档等，我们可以通过func.__name__获取函数名、可以通过func.__doc__获取函数的文档信息，用户也可以通过注解等方式为函数添加元信息。 例如下面代码， 1234567891011121314151617181920212223242526from time import timedef run_time(func): def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): ''' func one doc. ''' sleep(1) fun_one()print(fun_one.__name__)print(fun_one.__doc__)# 输出# wrapper# None 可以看出，通过使用装饰器，函数fun_one的元信息都丢失了，那怎么样才能保留装饰器的元信息呢？ 可以通过使用Python自带模块functools中的wraps来保留函数的元信息， 12345678910111213141516171819202122232425262728from time import timefrom functools import wrapsdef run_time(func): @wraps(func) # &lt;- 这里加 wraps(func) 即可 def wrapper(*args, **kwargs): start = time() func() # 函数在这里运行 end = time() cost_time = end - start print("func three run time &#123;&#125;".format(cost_time)) return wrapper@run_timedef fun_one(): ''' func one doc. ''' sleep(1) fun_one()print(fun_one.__name__)print(fun_one.__doc__)# 输出# fun_one # func one doc. 只需要在代码中加入箭头所指的一行即可保留函数的元信息。 文档获取本讲的Markdown格式文档我进行共享了，需要的可以关注公众号回复回复关键字”python“获取。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【进阶Python】第一讲：开篇]]></title>
    <url>%2F2019%2F08%2F04%2Fpython-one%2F</url>
    <content type="text"><![CDATA[前言 2015年，本科毕业的那个暑假我开始疯狂的投递简历，希望找一份数据分析与数据挖掘相关的实习工作。直到有一家公司的面试官问到我：“你会Python吗？”，我当时一脸疑惑，因为，大学期间只系统的学习过C语言，后期开发系统中用到过少量的C#。于是我问面试官：“你能给我拼写一下这么语言的名字吗”？多年之后回想起来还会觉得很尴尬，真的是孤陋寡闻。 从那以后，“Python”这么语言经常出现在我耳边。读研之后我开始之后我主要研究的方向是传统目标识别和图像处理，主要使用的语言就是C++和Matlab，所以Python在我读研第一年并不是主力工具。研二开始后我开始进入深度学习这个领域，开始用到很多第三方的工具包，例如caffe、tensorflow已经CNN，那以后开始以Python语言为主。 因为之前有一些编程基础，在加上当初面试实习时时间紧迫，所以我就抽了一天的时间把Python基础教程看了一遍，了解了基本用法之后就成功的面试上了一份实习工作。那时候我认为Python是简单的，因为它不像C++、Java那样有严格的语法规范、有变量类型的概念，你只需要记住缩进正确即可。而且在做自然语言和计算机视觉过程中很多部分的代码都是依托第三方工具包完成，真正自己开发的只是一些数据预处理、文本处理以及用一些条件循环语句对逻辑进行串联。 直到后来从事工作以后，做了更多有严格交付要求的项目之后才发现，Python并没有想象的那么简单，“会用Python容易，用好Python不易”，这是我使用几年Python之后的感触。 当你做一个项目要考虑到代码的复用性、易读性、运行效率、后期维护成本以及面对一些复杂的数据结构时，你会发现Python绝对不是简简单单利用那些基本知识能够实现的。 这也是我开始这个系列分享的原因，第一：把自己开发过程中的一些心得和经验总结下来。第二：如果能够帮助更多的Python学习者，那就更加荣幸了。 为什么要用Python？近几年唱衰Python的声音不拘于耳，有些人是的确发现并感受到了Python的缺点，但是更多的人是跟风式的唱衰Python。“Python效率低”，很多人都这样说，这显然有一些以偏概全的感觉，如果做游戏、软件，Python的确不占优势，但是如果作为算法工程师，进行算法验证，我想没有几个人会选择C/C++。口说无凭，先看几组数据对比。 PYPL 通过分析在谷歌上搜索语言教程的频率，创建了编程语言索引的PYPL流行度。 首先看一下PYPL最新编程语言流行程度， Python居于第一，力压Java、JS、PHP这些名气非常大的编程语言，而且前10名中2~9名都出现了负增长，而Python却4.5%的正向增长率。 如果觉得一个平台不够具有说服力，可以再看看另外一个知名的编程社区的排名。 TIOBE编程社区 TIOBE编程社区指数是编程语言受欢迎程度的一个指标。该指数每月更新一次。这些排名是基于全球熟练工程师、课程和第三方供应商的数量。流行的搜索引擎，如谷歌，必应，雅虎!美国、维基百科(Wikipedia)、亚马逊(Amazon)、YouTube和百度被用来计算收视率。值得注意的是，TIOBE索引不是关于最好的编程语言，也不是编写大多数代码行的语言。 来看一下TIOBE社区7月的编程语言排名， Python仅次于Java和C，排在第三名，而且对比去年同期，前10名中Python增长速度最快，达到2.9%。 从这里可以看出，Python一直被唱衰、一直很坚挺，尽管几年量关于Go、julia、Rust的呼声很高，但是依然无法撼动Python的地位，而且这些编程语言到底好不好用？有没有炒作的成分在里面？现在还是一个问号。 话说回来为什么Python如此受欢迎？ 我认为存在的即是合理的，如果它真的一无是处、漏洞百出，是经不住众人的考验的。它之所以如此受欢迎，自然有很多吸引人的方面： 简单易用、节省时间 丰富的第三方工具包 强大的社区 应用场景丰富 其他三个方面暂且不说，就说一些第一点，简单易用、节省时间，我觉得有这一个理由就足以吸引很多人。尤其是对于算法、测试等岗位，真正的耗费心思的并不在编程、开发这一块，编程语言是用来验证算法的可靠性的，但是没有这个编程语言，自然无法验证，这就体现出有一个简单易用的语言有多么重要了。 吴恩达在《机器学习》这么课程里提到“硅谷的工程师大多数都会选择一个简单的编程语言对自己的算法进行验证，当确认有效之后会用c/c++等语言重新实现一遍”，这足以体现Python语言简单易用的优点。 Python距离第一个版本发布以及有28年，唱衰的言论从未间断，但是依旧坚挺。 尤其是机器学习的大规模应用、国家把人工智能智能技术上升到战略层次，使得Python称为独树一帜的编程语言，虽然这两年Go、Julia号称性能更好、更加易用，但是一直无法撼动Python在机器学习领域的地位，很难望其项背，为什么？我认为最主要的原因就是拥有强大的用户基础。现在在大多数企业，从事算法相关岗位的清一色的使用Python，更别说计算机视觉、自然语言这些强依赖Python第三方库的方向。 Python该怎么学习？我认为大多数编程语言的学习都可以简化为3个过程： 入门 进阶 强化 入门阶段网上的教程已经很多了，关于入门我个人是不推荐参加培训班的，因为就如同前面所说的那样，Python基础语法非常简单，尤其是有一些C、Matlab、C++等编程基础的同学来说，Python中的很多概念虽然和恰语言不是完全相同，但是相似度还是非常高的，可以达到触类旁通。我个人更倾向于使用在线教程，这里推荐两个不错的入门教程， 菜鸟教程 https://www.runoob.com/python/python-tutorial.html 廖雪峰Python https://www.liaoxuefeng.com/wiki/1016959663602400 强化阶段我认为需要在实际的项目和工作中去得到提升，就如同计算机视觉、自然语言处理一样，你从文章和练手项目中所能获取的只有那么多，如果像进一步得到升华就需要在项目中去面对困难、解决困难，这时候就会想尽方法去解决各种难题，不知不觉中会得到很大的提升。 本系列的主要目的是介绍进阶阶段，讲解一些Python的高级用法，对于入门和强化阶段自己可以私下完成。 书籍推荐如果时间比较冲突，我觉得可以系统的看一些Python书籍，因为书籍的严谨性和条理性更加有保障，在这我推荐3本我个人认为不错的书籍， 1.《Python编程 从入门到实践》 如果时间有限，我认为入门阶段可以通过菜鸟教程、廖雪峰Python进行学习。如果时间充足，我认为可以看一下入门书籍，因为更加严谨一些。 《Python编程 从入门到实践》是一本比较适合入门的书籍，环境配置、变量、列表、if语句、函数等基础的概念都会详细的展开介绍，这对于没有编成基础的同学非常有帮助。 2.《流畅的Python》 这是一本经得起考验的Python书籍。 它和大多数书籍和在线教程蜻蜓点水式的讲解不同，它更加深入，深入而不冗余，在你看这本书的时候你会发现，它的每一段话都是有意义的，没有什么废话。 它分别从数据结构、字典集合、文本和字节序列、函数、设计、装饰器、闭包等讲起，然后对每一块知识进行展开，详细介绍里面最根本的原理，然后告诉你，该怎么用好它，高效的使用它。 举一个最简单的例子，在绝大多数教程都会讲到循环和条件语句，千篇一律的告诉你”if..else..”, “for…while”，这个有一点编程语言的同学都知道，但是在Python里面循环和条件语句有什么特殊的地方吗？该怎么用好它？ 《流畅的Python》这本书就教你怎么去使用它，告诉你列表推导该怎么用还有它的意义所在。 这就是这本书的优点：不仅告诉你怎么用Python，而是告诉你怎么用好Python。 3. 《Python CookBook》 学而不精的同学都会认为Python是一门很简单的编程语言，不错，Python相对于Java、C++要简单很多，没有严格的语法结构、没有变量类型，而且如果有一些编程基础去学Python的话可以一个周甚至一天即可学完。 但是我认为，Python入门简单，但是用好并不简单，当你接触到标准的商业项目时你就会意识到Python高级用法的重要性以及它的价值所在。 《Python CookBook》这本书就是这样的一本进阶教材，它不同于大多数教程，反复的介绍基本语法，它直接跳过基本语法开始讲解数据结构、算法、迭代器、生成器、类、对象、元编程等，我认为这些才是工作中真正有价值、拉开差距的地方，而那些基本语法是默认应该会的。 《Python CookBook》会在每个知识点开始提出一个应用场景，然后告诉你怎么去解决这种应用，同时会编程实现，这样对于提升Python是最为实际的，而且让你更加容易理解它这样用的价值所在。 这本书不仅有出版的书籍，也有免费的在线教程，需要可以看一下。 https://python3-cookbook.readthedocs.io/zh_CN/latest/preface.html 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第十讲：传统目标检测之卷积神经网络概述]]></title>
    <url>%2F2019%2F08%2F03%2Fcnn%2F</url>
    <content type="text"><![CDATA[前言 提起卷积神经网络(CNN)，应该很多人都有所耳闻。自从2012年AlexNet在ImageNet挑战赛一举夺魁，它再一次的回到的人们的视野。 为什么称之为”再一次”，因为CNN并不是近几年的产物，早在20世纪90年代Yann LeCun就提出了最基础的卷积神经网络模型(LeNet)，但是由于算力和数据的限制，它一直处于一种被冷遇的地位，传统目标识别方法，例如之前所讲到的SIFT、HOG、DPM占据着不可撼动的统治地位。 但是随着算力的提升和数据集的积累，这一切都变了，在AlexNet成功之后，CNN如同雨后春笋一样，每年各种各样的Net数不胜数，近其中知名的就有AlexNet、VGG、GoogleNet、UNet、R-CNN、FCN、SSD、YOLO等。 入门计算机视觉领域的绝大多数同学应该都学过或听说过斯坦福大学的公开课(CS231n: Convolutional Neural Networks for Visual Recognition)，主要就围绕CNN进行展开，甚至很多近几年入门计算机视觉的同学就斩钉截铁的认为，计算机视觉就是卷积神经网络，我认为这有一些”一叶障目，不见泰山的”感觉。 CNN只是计算机视觉的一个子集，而且是一个很小的子集，更确切的说，计算机视觉是一种应用性技术，CNN是一种工具。 但是，不可否认，CNN是目前阶段我们能力所达到的、在大多数CV方向应用最为成功的一项技术，尤其是R-CNN系列和YOLO系列，在商业中，例如交通监测、车站安检、人脸识别应用非常多，效果对比于传统目标识别算法也要好很多，所以，它是学习计算机视觉中非常重要的一环，本文就概述一下近年来比较成功的CNN模型。本文只是用简略的语言进行概述，后续会挑选一些比较经典的模型进行详解和编程实现。 卷积神经网络概述 按功能对卷积神经网络进行分类主要可以分为两类， 检测(detection) 分割(segmentation) 检测的目的是要判断一副图像中是否有特定的目标，以及它所在的位置，通过一些手段识别出它所在的包围合区域。 分割的目的要更加严格一些，它不仅要识别出目标的所在区域，还要分割出目标的边缘，尤其在CNN图像分割领域，和传统的图像分割不同，它不能简单的依靠梯度变化幅度把目标分割出来，还需要进行语义上的分割，识别到像素级的类别。 目前比较知名的用于识别的CNN模型有， AlexNet VGG R-CNN系列 Resnet MobileNet YOLO系列 在分割方面比较知名的CNN模型有， Mask R-CNN FCN U-Net SegNet CNN中主要用到的技术 系统学习以上上述所提到的知名CNN模型会发现，其中所使用到的技术手段大同小异，而那些知名度较小的CNN模型更是如此，创新点更是微乎其微，其中所使用到的技术主要有， 卷积 池化 基础块 Dropout 跳跃连接 锚点 优化算法 激活函数 批量正则化 回归 卷积和池化是非常基础的，在特征提取过程中至关重要。 基础块的思想最初出自于VGG，它在AlexNet的基础上进行了很大的改进，基础块思想的引入增加了网络的重用性，后续很多模型都死在这一举出上进行改进的，因此，在很多后续的网络模型都是以VGG为基础模型。 Dropout这个几乎成了CNN模型中必不可少的一个组件，它在应对过拟合问题中具有非常重要的价值。 跳跃连接最初出现在ResNet，在网络的不断改进中发现，其中的思想都是使网络越来越深，网络适当的加深的确能够带来识别精度的提到，但是真的越深越好吗？当然不是。随着网络的加深，很容易出现梯度消失和梯度爆炸现象，ResNet中提出的跳跃连接在后来的网络模型中扮演者非常重要的角色。 锚点这一概念最初是在2008年的DPM模型中看到，后来Faster R-CNN中主要的使用了这项技术，使得它名声大噪，后来的经典模型几乎都用到了锚点这个思想。 优化算法对于上述CNN模型的价值自然不言而喻，梯度下降、Adam、牛顿法等，可以说这是深度计算机视觉的核心所在，也是理论体系最完善、最能够用数学模型解释的一部分。 激活函数和Dropout一样，也是CNN模型中必不可少的一个组件，它的主要价值在于解决模型的线性不可分问题，把非线性的特性引入到网络模型中。 批量正则化也是CNN中常用的一个功能，它的主要作用是加速模型的收敛，避免深层神经网络的梯度消失和梯度爆炸。 回归中用到的较多的自然是softmax，它将经过各种网络层处理得到的特性向量进行回归，得到每一个类别对应的概率，在多分类问题中是一个必不可少的功能。 CNN模型架构 纵观上述所提及的经典CNN模型，它们的模型架构非常相似，主要包含如下几个部分： 输入层 特征提取层 全连接层 回归 输出层 输入层主要是用于读取图像，用于后面的网络层使用。 特征提取层主要通过卷积来获取图像局部的特征，得到图像的特征图。 全连接层用于对特征层进行后处理，然后用于回归层处理。 回归主要通过一些回归函数，例如softmax函数来对前面得到的特征向量进行处理，得到每个类别对应的概率。 输出层用于输出检测和分类的结果。 当然，在这个过程中某些环节会用到上述提到的激活函数、批量正则化、优化算法以及非极大值抑制。 搭建CNN目标识别系统有了上述强大的模型，在实际项目中该怎么搭建一个有价值的CNN目标识别系统呢？我认为主要分为如下几个步骤， 数据获取 数据预处理 模型搭建 数据后处理 在CNN，乃至整个深度学习领域都可以说数据获取是至关重要的一部分，甚至可以说占据了超过50%的地位。深度学习的发展主要就是得益于这么多年来数据的积累，很多项目和工程也是由于数据的限制和却是只能中途作废。因此，数据获取部分是搭建目标识别系统中最重要的一个环节，它直接决定着是否能够继续走下去。 目前有一些公开的数据集可以获取，例如MNIST、Pascal VOC、ImageNet、Kaggle等。如果自己所做的方向恰好巧合，这些公开数据集里有相应的数据，那么的确是幸运的，可以从这些数据中直接获取。 数据预处理对于CNN同样非常重要，各种视频、摄像头在数据采集的过程中很难保证数据是有价值的，或者干净的，这里就需要对数据进行去噪、去模糊、增强分辨率，如果数据集不充足，还需要对数据进行扩充。 模型搭建我认为是这几个环节中相对较为容易的一部分，首先目前这些经典的框架都有开源的项目，有的甚至不止一个版本，我们可以借鉴甚至直接拿来用这些模型。即便不愿意选择开源的项目，也可以使用tensorflow、pytorch进行搭建，其中需要的代码量是非常有限的。 输出检测的结果需要进行非极大值抑制、绘出包围合等后续工作，以及和一些系统进行对接，这样它才是一个可用的完整系统。 更多精彩内容，请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐5款值得安装的Windows工具]]></title>
    <url>%2F2019%2F08%2F01%2Ffive-windows-tools%2F</url>
    <content type="text"><![CDATA[前言电脑，是我们日常学习和办公中主要依赖的工具之一。而windows作为最流行、使用最为广泛的桌面操作系统，与我们的生活有着密不可分的关系。尤其是近两年随着windows10的逐渐完善和丰富，使得windows操作系统更加受欢迎。我个人使用win10已经超过两年，不得不说，这的确是一个不错的操作系统，在此之前会想方设法安装Ubuntu、centos、redhat等操作系统来替代win7和win8，但是随着windows发布和晚上，加入了linux内核，丰富实用的小工具，让我认识到微软在操作系统和软件生态方面的强大。 在使用windows的过程中，仅仅依靠系统自带的功能是无法满足各种各样的工作、学习需求的，因此需要借助一些第三方工具，虽然有很多知名的软件，例如office、Matlab、Photoshop、CAD等，但是这些工具太过于臃肿，不仅占用很大硬盘空间，而且需要付出高额的费用。windows上其实有很多使用、免费，但不失强大的工具，因为是免费开源，所以没有那么过广告和宣传，所以知名度相对较低，本文就介绍5款值得安装的windows工具。 1. Click&amp;Clean 浏览器是我们使用最多的一款工具之一，甚至没有其中的之一。 每天我们花费大量的时间在浏览器上面，访问各种网址，也留下了很多访问的足迹，这就涉及一个问题，除了缓存垃圾之外就是隐私和信息安全。不知不觉中我们把自己的信息展露无疑。 我认为有着Click&amp;Clean这款超强的隐私保护工具就再也不用担心这个问题了。 当浏览器关闭时，这款应用程序删除你的浏览历史,防止他人跟踪你的网上活动，它支持以下诸多功能， 清空缓存 删除 Cookie 清除已保存的密码 浏览器关闭时运行外部应用程序 关闭所有窗口/标签前清理 Delete Web Local Storages Delete Extension Local Storages Delete Web SQL Databases Delete Extension SQL Databases Google Gears 认证数据删除 …… 2. 石墨文档 颠覆传统办公 就如同它的定位那样“颠覆传统办公”，我觉得它做到了。和以往臃肿的office、昂贵的xmind不同，它首先免费，其次它支持多平台同步，windows、mac、手机均支持。此外，它将常用的办公工具融合为一体。 虽然拿它和office做对比，但是它不仅仅是传统意义上的office工具，它还包含如下功能， 文档 表格 幻灯片 思维导图 协作空间 可以说，上述每一项功能都是目前一个完整的商业产品，需要付出高额的服用，而且非常臃肿，但是石墨文档把这些问题都给解决了，不仅使用简单，而且轻量化、见面简洁大方。 3. Squoosh 这是谷歌出品的一款强大的图片压缩工具，比之前较为知名的TinyPng还要强大一些。 我们都知道在我们传输图片，或者在一些平台上传图片时都会有图像大小的限制，例如微信公众号对上传图片就有限制，很多报名系统对上传图片也有限制。 但是我们又不想损失图片质量怎么办？可以尝试一下Squoosh，它采用谷歌强大的算法，在保障图像质量的前提下最大化压缩图片。 严格意义上说，它是一个网页工具，如果觉得用网页方便，可以直接保存网站https://squoosh.app/，到书签即可，如果不喜欢网页工具，没问题，它也支持安装， 打开网站后点击右上角会发现，菜单栏出现”安装Squoosh”的资源，点击安装即可，占用内存非常小。 前面铺垫了很多，效果到底真的那么强大吗？下面来看一下对比图， 把一副1.51MB的原图压缩到104KB，压缩率高达93%，但是视觉上并不是很明显，看上去依然很清晰。 此外，它还支持一些简单的在线编辑。 4. uTools 这可以称得上上“软件中的百宝箱”，内含丰富的插件，通过安装插件能够实现几十种功能，可以说，有了这款软件可以把很多软件卸载给你的系统节省一些空间了，此外，它还支持设置全局快捷键。 它的插件主要包括如下几类： 通用：例如二维码、翻译、待办事项、剪切板、颜色助手、本地搜索等实用的小工具。 图片：包括压缩图片、图床、图片转文字等功能。 开发：包括JSON、正则表达式、http抓包等功能。 这款工具可以适用于不同的人群，满足不同的需求，有了这款工具就不需要人群。 安装插件 插件安装非常简单，只需要点击对应软件-下载即可， 使用 安装之后打开对应的工具即可使用， 5. WGestures我之前其实并不看好鼠标手势，因为尝试过很多手势工具，大多数都是功能花哨，但是使用体验很差，我一度甚至认为鼠标手势就是一个鸡肋，直到我遇到WGestures这款工具，可以说是让人心里感觉豁然开朗，原来鼠标手势可以这么好用，下面简单举几个例子。 基本手势包括复制、剪切、粘贴、前进、后退、添加书签、刷新、Home、End、”摩擦”边缘、触发角等。 复制、粘贴 Web搜索 摩擦边缘 摩擦右边缘弹出任务管理器、摩擦左边缘弹出控制面板 自定义手势 如果WGestures自带的手势无法满足自己的需求，还可以根据自己的偏好添加手势，添加手势类型包括打开文件、窗口控制、命令行、音量控制、执行快捷键等，例如，我自定义一个打开文件的手势， 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>文件查找</tag>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习资源 | 推荐2份Github热门校招面试汇总资料]]></title>
    <url>%2F2019%2F07%2F28%2Finterview%2F</url>
    <content type="text"><![CDATA[前言 秋季招聘一般集中在每年的9月-10月份，有早一些的城市或者公司会在8月就开始进入校园，开启招聘，也有比较晚的少数公司会在11月-12月进行招聘。 一年一度的秋季招聘马上就要开始了，秋季招聘是校园招聘中最为集中、岗位最多的一次集体招聘会，虽然也有春招，但是对于很多公司而言主要是“查漏补缺”，岗位数量相对于秋招有很大的差距。因此，很多即将毕业的学生会把秋季招聘看的非常重要，毕竟第一份工作对一个人来说是至关重要的，每个同学都希望找到称心如意的工作。 但是，“天下没有免费的午餐”，在秋季招聘中这个道理同样适用，好的工作岗位和面试的难度是成正比的，无论是银行、金融，还是互联网、IT。因此，要想找到一份称心如意的工作，必然需要做好充分的准备，毕竟，机会都是留给有准备的人。 面试是否有规可循？答案是肯定的，以阿里、百度、华为这些知名的互联网、IT公司为例，每年招聘的人数有限，但是应聘人数却是招聘人数的几十倍，甚至上千倍。招聘对于毕业生来说是一次煎熬的过程，对于企业同样是一个非常耗时耗力的事情，因此他们会通过一些“落入俗套”的方式进行人才的筛选，虽然这样会错失一部分真正的人才，但是能够筛选掉更多不符合要求的平庸人员，这样的损失是企业愿意接受的。有哪些“落入俗套”的方式呢？无非就是学历、学科、笔试、面试。其实经过一轮简历上学历、学科的筛选已经筛选下去一大批，而通过笔试又会筛选去一大批没有做好充分准备的同学，真正进入最后面试的已经是经过层层筛选留下来的。 学历、学科这些是人为无法改变的，但是笔试、面试却可以，因为成熟、经典的知识体系已经经过多年的洗礼逐渐完善了起来，笔试、面试的内容无非是变着花样的考书本上、教材上的知识，例如数据结构、算法设计等。因此，我认为通过准备，学习掌握目标公司历年来的出题类型和面试方式，会让应聘过程变的顺畅很多。 近期在github上发现两个不错的面试笔记，总结了各大知名互联网、IT公司，例如阿里、百度、腾讯、华为、美团等公司面试中常见的笔试、面试题型，并且给出了详细的解答。我认为每个人都有薄弱的地方，所以，如果心中有目标的公司，可以根据自身的不足之处学习一下对应公司近两年面试、笔试中常见的题型，好好准备，这样能够有效的帮助你在校招中找到称心如意的公司，废话不多说，下面介绍一下这两个开源学习项目。 0voice / interview_internal_reference https://github.com/0voice/interview_internal_reference 这是一个按公司和知识体系分类的的学习资源，目前已经13w+star。 如果心中有明确的目标公司，可以针对性的看一下对应公司的面试总结。它包含阿里、华为、百度、腾讯、美团、头条、滴滴、京东等。此外，还针对企业中比较常用的工程技术进行知识类型的总结，例如MySQL、Redis、MongoDB、Zookeeper、Nginx、算法、内存、磁盘、网络通信、安全、并发等，可以说是涵盖的非常全面。 和往常见到主要针对算法实现的笔试题目不同，这个项目更加偏向面试。我觉得对于有一些编程和数据结构知识的同学通过笔试都不是特别困难的事情，而真正能够在面试官心中留下深刻印象的往往在笔试中，而这些面试官往往是工作多年，深耕业务和产品的工作人员，因此，他们更多的关注的是现实中遇到的问题，所以，如果在这些问题上回答的不错更容易抓住面试官的心。 interview_internal_reference这个项目主要的针对这些问题，从系统稳定性到缓存机制，从并行计算内存优化，非常全面，知识体系也非常分散，我认为这远远要比反复的刷leetcode要有价值的多，尤其是对于腾讯、阿里这些偏工程的公司，在面试过程中会问很多非常分散的工程技术问题。 imhuay/Algorithm_Interview_Notes-Chinese https://github.com/imhuay/Algorithm_Interview_Notes-Chinese 如果说interview_internal_reference偏向于工程技术，那么Algorithm_Interview_Notes-Chinese更多的是围绕着算法进行展开，目前已经25w+star。 在面试过程中，偏开发和偏算法的面试差别非常大，偏开发，例如前端、后端、云服务等工程技术的会询问很多分散技术的问题，但是偏算法的往往会集中在算法和项目方面，例如，做过哪些相关的项目？使用了什么算法？这个算法具体细节是什么？等等。大多数是围绕着应聘岗位进行面试，如果发散一些，会问一些经典的数据结构和数学方面的知识。 这几年随着人工智能火热，计算机视觉、深度学习、自然语言处理方面的工作岗位也多了起来，应聘者也多了起来。这个学习资源主要围绕人工智能领域的技术进行总结，同时涵盖数学、编程、数据结构等方面的基础知识。 如图所示，在深度学习方面，它包含了深度学习的基础知识，例如过拟合、欠拟合、激活函数、反向传播、正则化、加速训练，同时还包含深度学习领域核心的优化算法，例如，梯度下降法、动量法、牛顿法等。 此外，在计算机视觉、自然语言处理方面也包含了诸如VGG、词向量等基础的知识。我认为，它不仅适用于即将应聘的同学，同时对于已经参加工作的同学也非常有用。 除了应用层面的算法之外，它还有数学基础，例如微积分、概率论方面的知识，从求导到极限、从微分到积分，泰勒级数等，样样都有。 123456789101112131415161718192021222324252627class Solution &#123;public: int widthOfBinaryTree(TreeNode* root) &#123; if (root == nullptr) return 0; queue&lt;TreeNode*&gt; Q; Q.push(root); int ans = 1; while(!Q.empty()) &#123; int cur_w = Q.size(); // 当前层的宽度 ans = max(ans, cur_w); for (int i=0; i&lt;cur_w; i++) &#123; auto p = Q.front(); Q.pop(); if (p-&gt;left) Q.push(p-&gt;left); if (p-&gt;right) Q.push(p-&gt;right); &#125; &#125; return ans; &#125;&#125;; 除了这些偏理论的知识，它还有面试中常出现的数据结构和算法方面的总结，同时给出了编程实现，例如，动态规划、双指针、排列组合、二叉树、链表、堆、栈等，此外，Algorithm_Interview_Notes-Chinese吸引人的地方是在这些算法方面不是单纯的给出编程实现，还是列出实现的步骤和思路，非常有助于理解。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第九讲：传统目标检测之DPM模型]]></title>
    <url>%2F2019%2F07%2F28%2Fcv-dpm%2F</url>
    <content type="text"><![CDATA[前言 DPM(Deformable Part Model)模型，又称为可变型部件模型，是Felzenszwalb于2008年提出的一个模型。这可以说是传统目标识别算法中最为经典的算法之一，我认为对计算机视觉有一些深入了解的同学应该对DPM模型都有所耳闻。 首先说一下DPM模型这篇文章有多牛。DPM模型的坐着Felzenszwalb凭借这个模型一举获得2010年voc挑战赛的终身成就奖，感觉还是不够牛？不知道Felzenszwalb是何许人也？Felzenszwalb正是Ross B. Girshick(也就是DPM模型的第二作者)硕士和博士期间的导师。我想，如果连Ross B. Girshick都不知道的话就真的称不上是一个计算机视觉领域的学习者了。它正是R-CNN系列、YOLO系列等现如今被封为经典的计算机视觉模型的提出者或共同提出者，可以说是这几年计算机视觉领域比较有作为的一位研究者。 说完DPM的作者很牛，那和DPM有什么关系？前面提到，它的作者是近几年计算机视觉领域非常知名的研究者，因此，自然而然，这几年比较成功的计算机视觉模型都会受到这个标杆性算法的影响。多尺度、锚点、可变型部件，都对后面深度学习计算机视觉带了巨大的影响。 介绍完DPM模型的背景，再回到这个算法本身。DPM模型和前文讲到的HOG整体流程非常类似，HOG采用HOG特征加linear SVM，而DPM采用多尺度特征加latent SVM，此外，DPM在特征提取方面也是在HOG特征的基础上进行稍加改进。虽然从文中看上去两者差别并不大，但是其实DPM无论是在特征提取层面还是在机器学习层面都做了巨大的改进。 首先是特征提取思想，HOG模型仅仅考虑根模型的特征，不考虑部件模型的特征，而DPM模型采用根模型加部件模型的思路，同时考虑外观和细节部分的特征。 其次是SVM方面，Latent SVM加入了潜在信息的训练。 下面就分别从特征提取到模型训练介绍一下这个模型。 特征提取 文章中讲的有点让新学者难以理解，这里我就对照着HOG特征讲解一下，更有助于理解。 两者相同的是第一步都要先计算梯度方向，然后对梯度方向进行统计。 不同之处是，HOG特征含有块(block)的概念，它首先把一副图像划分成若干个块，然后再把块划分成若干个单元，然后对单元内部的像素进行梯度统计，然后对同一个块内的特征向量进行归一化，HOG采用的是0~180度之间的梯度方向，20度一个区间，这样每个细胞单元就统计得到一个9维特征向量，一个块内就得到n * 9维特征向量。 由于HOG采用的梯度方向为0~180度方向不敏感特征，这样会丢失很多特征信息，DPM模型对HOG做了很大的改进。首先DPM模型没有快的概念，它是去一个细胞单元四角对应的领进单元的特征进行归一化，此外，更重要的是DPM不仅提取结合0~180度方向不敏感特征和0~360度方向敏感特征两种特征，它首先提取0~180度之间的特征，得到上图所示4*9维的特征，拼接起来得到13维特征向量，然后再提取0~360度之间的特征，得到18维特征向量，二者相加得到31维特征向量。 模型训练前面介绍了一下DPM模型特征提取的方法，虽然思想与HOG有很大不同之处，但是在最基本的梯度方向统计方面是相同的。 知道了如何从一副图像中提取我们想要的特征，要进一步深入理解一个算法，我认为从模型训练、模型预测方面是最简单明了的方法，无论是传统目标识别还是深度计算机视觉。知道它是如何训练、如何预测的就知道这个模型的运作情况，输入是什么？中间经历了什么过程？输出是什么？下面就来看一下DPM模型的训练过程。 本算法采用的训练说句来自于Pascal VOC，用过这个数据集的都知道，它只标记了图片中目标的包围合，并没有标记图像的部件，例如它只标记了一个人，并没有标记人的胳膊、腿、头部等，而DPM被称为可变型部件模型，那么部件体现在哪里？怎么知道它的部件在哪？下面来了解一下它的训练过程，能够帮助理解这个算法。 DPM的在训练之前先进性了初始化，主要包括3个阶段： 初始化根滤波器 为了训练一个有m个组件的混合模型，首先将正样本按照长宽比划分成m组，然后针对每一组训练一个根滤波器F1、F2、…、Fm，在训练根模型过程中使用的是标准的SVM， 不含有潜在信息，例如上图(a)、(b)就是初始化的两个根模型。 合并组件 把初始化的根滤波器合并到一个没有部件的混合模型中并且重新训练参数，在这个过程中，组件的标签和根的位置是潜在变量(组件和部件不是同一个概念)。 初始化部件滤波器 前面提到，数据集中并没有标记部件的位置，因此文中在初始化部件滤波器是用了一个简单的假设，将每个组件的部件数量固定在6个，并使用一个矩形部件形状的小池，文中贪婪地放置部件，以覆盖根过滤器得分较高的区域。 另外需要清楚的是，部件滤波器是在根据滤波器2倍分辨率的图像上进行初始化，因为分辨率越高，细节越清晰，越能提取部件的特征。 经过初始化之后就可以训练模型参数。 下面是详细的训练过程， 模型检测前面介绍了DPM模型的特征提取和训练过程，下面就来看一下模型检测过程。 上述就是就是DPM模型检测的详细过程： 对输入图像进行特征提取，得到特征图和2倍分辨率的特征图 分别在特征图和2倍分辨率上计算根滤波器和部件滤波器的得分 合并根位置的得分，得到总得分 用数学语言表示，图像的总得分为， \begin{array}{l}{\operatorname{score}\left(x_{0}, y_{0}, l_{0}\right)=} {\quad R_{0, l_{0}}\left(x_{0}, y_{0}\right)+\sum_{i=1}^{n} D_{i, l_{0}-\lambda}\left(2\left(x_{0}, y_{0}\right)+v_{i}\right)+b}\end{array}模型检测过程就是获取局部最大响应(得分)的过程，前面已经训练得到了模型参数，然后利用模型参数在图像特征图上滑动求点积，计算得分。DPM的得分包括两个方面：$R_{0, l_{0}}\left(x_{0}, y_{0}\right)$是根滤波器的得分， $\sum_{i=1}^{n} D_{i, l_{0}-\lambda}\left(2\left(x_{0}, y_{0}\right)+v_{i}\right)$是部件滤波器的得分，$b$是偏移量。 Latent SVM在经典的SVM中，认为训练样本的标记是严格符合类别标签的，标记的正样本就是正样本、标记负样本就是负样本，但是由于标记过程中有很多人为因素，因此，虽然能保证负样本一定是负的，但是却不能保证正样本一定属于正的。因此在训练过程中有很多潜在的未知信息，作者发现，将根位置作为一个潜在变量，可以有效地补偿正样本中存在噪声的边界框标签。 Latent SVM训练的目标函数为， L_{D}(\beta)=\frac{1}{2}\|\beta\|^{2}+C \sum_{i=1}^{n} \max \left(0,1-y_{i} f_{\beta}\left(x_{i}\right)\right)其中， f_{\beta}(x)=\max _{z \in Z(x)} \beta \cdot \Phi(x, z), $z$是潜在信息。 源码解析由于DPM模型工程量较大，而且作者已经开源代码并且经过多个版本的迭代，目前非常成熟，因此不在这里逐步实现，在这里主要讲解一下怎么使用源码去检测目标和训练模型。 目前源码版本为 voc-release5，可以直接访问官网下载， http://www.rossgirshick.info/latent/ 也可以关注公众号回复voc获取。 DPM的源码是由Matlab和C++进行混编而成，Matlab主要用于做一些简单的图像处理，由于在模型训练和特征提取过程中非常缓慢，因此，为了提高效率，作者用C++实现了特征提取和模型训练部分，另外，由于C++部分使用了一些多线程的库，所以在windows下无法直接运行，需要做一些修改，在linux和mac下可以直接运行。 目标检测 用训练好的模型检测目标，主要有如下几个步骤， 解压缩代码。 运行Matlab。 运行’compile’函数来编译helper函数。 加载模型和图像。 检测目标。 示例， 1234&gt;&gt; load VOC2007/car_final.mat; &gt;&gt; im = imread('000034.jpg'); &gt;&gt; bbox = process(im, model, -0.5); &gt;&gt; showboxes(im, bbox); 训练模型 可以自己按照voc的格式准备数据，训练自己的模型，去检测相应的目标，详细过程如下， 下载数据集和VOC devkit工具包。 根据自己的数据配置voc_config.m。 运行’compile’函数来编译helper函数。 利用pascal.m脚本训练模型 示例， 1&gt;&gt; pascal('bicycle', 3); 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 推荐10款浏览器插件]]></title>
    <url>%2F2019%2F07%2F24%2Fchrome-plugin%2F</url>
    <content type="text"><![CDATA[前言前一篇文章花大篇幅详细的介绍了一款强大的浏览器插件—tampermonkey，评论里很多同学对这款插件给予很高的评价。可以看出，浏览器插件在日常工作和生活中的确占据着很重要的地位，因此，本文整理推荐10款浏览器插件，每一款都是经过长时间使用并且最终保留下来的，插件主要是一些效率工具，希望能够对各位提供有效的帮助。 Infinity 这款插件自称是一款新标签页工具，但是我觉得，如果仅仅把它视为新标签页工具，那就把它想的太简单了。 它提供极简的标签页设计，而且含有丰富的标签页壁纸，轻轻点击即可切换新标签壁纸，但是它的功能远不止于此，它还包含如下丰富功能： 待办事项 实用笔记 精美天气 轻松切换搜索引擎 …… Forest 保持专注，用心生活，这是这款工具的宗旨。 这是一款习惯养成插件，就如同前面所说，浏览器是我们工作和学习中常用的一款工具，但是如果把过多的时间花费在浏览器上，那么就是一种巨大的时间浪费。Forest利用一种轻松有趣的方式让你远离网络成瘾。 在每次开始工作时，可以种下一颗树苗，它会慢慢长大。如果你频繁的打开浏览器，它会用各种方式提醒你应该专注工作。此外，你还可以把一些网站加入黑名单，如果在专注时间内访问这些网址则会被禁止，如果强制访问，那么辛辛苦苦种下的小树苗则会枯萎，利用这种游戏的方式让你更加专注于学习与工作。 LastPass 这是一款强大的密码管理工具。 互联网的时代，让我们困扰的就是频繁的注册、数不清的账号和密码，我们不断的在重复着设置密码、重置密码。怎么样才能解决这种困扰？可以尝试一下LastPass，它采用256位AES密匙的强大加密算法，首先保证了在本机上不获取得到您的信息，其次，每当访问对应的网站时，它能够快速提示你对应的密码并填充，这样就避免了重复记忆密码的困扰。 当然，它不仅仅包含密码管理，还包含安全笔记这个实用的功能。 OneTab 在浏览网站时我们会发现，不知不觉中打开了很多网页，这时候标签栏变的非常密集而混乱。 这时候该怎么办？逐个关闭不仅麻烦，而且如果后续用到的话又找不到了。如果不关闭吧，又影响了浏览器的使用体验。 有了OneTab这款工具，只需单击一下，就可以把所有标签页转化成一个列表，如果再次需要某个网页的时候，可以单个或者全部恢复标签页。此外，它还节省高达95%的内存占用。 Pocket 一款轻松捕获视频、文章等内容的快捷插件。 当我们看到一个视频或者文章时，由于种种原因无法当时去看，希望以后某个时段有了时间再去看，有了Pocket就使得这件事情变得简单起来，只需要点击一下Pocket图标或者鼠标右键保存即可轻松把内容保存到Pocket里面，而且Pocket还支持多平台、多终端，在浏览器上保存后再手机上也可以查看。 Grammar and Spell Checker 从名字就可以知道这款工具的功能—语法和拼写检查工具。 它能够在网站上任何位置对你输入的段落进行拼写和语法检查，它的强大之处主要有如下2点： 支持超过25种语言 适用于几乎所有的网站 FireShot 这是一款截图工具。 截图，是我们常用的一个功能，windows自带的截图功能自然不用多说，真的挺差的，借助QQ、微信等截图又比较麻烦，当然，可以借助Snipaste等强大的截图工具。支持截图的工具有很多，但是大多数只能截取可见部分，支持截取完整页面的却很少，FireShot就可以做到这一点，此外，它还支持： 截图保存到磁盘为PDF，PNG和JPEG 截图复制到剪贴板 打印截图 crxMouse 鼠标手势依赖者的福音。 它可以跨windows、linux、mac多平台支持自定义鼠标手势，同时支持超级拖拽、平滑滚动、摇杆手势。此外，还支持导入和导出配置文件。 它默认携带手势包括前进、后退、向上滚动、关闭标签页、到底部、刷新等，如果不满足于这些手势，还可以自定义添加。 Dark Reader 这是一款主题插件，现在有些手机和软件都实现了夜间模式，因为黑色的主题对于保护眼睛更有好处，能够减少明亮色彩带来的眼睛疲劳，Dark Reader就可以实现浏览器的夜间模式，同时它具有更强的定制性，能够调整亮度，对比度，应用棕褐色滤镜，黑暗模式，设置字体和忽略的网站列表。 SwitchyOmega 这是一款可以称得上“神器”的插件，轻松快捷的管理代理，同时支持自动切换多个代理模式。 在很多场景下，尤其是在有些公司内部对于信息安全的限制，需要设置代理才能访问不同类型的网站。如果通过IE代理设置，这样需要每次打开IE、连接设置…一系列流程，非常繁琐。而且针对不同的网络下只能使用同一种代理模式。SwitchyOmega能够让你轻松的切换不同的代理模式，而且支持自动代理切换，当访问不同网络时能够选取最快的代理方式，这样就不会出现有的网址可以访问，有的网址访问速度缓慢甚至打不开的现象。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>文件查找</tag>
        <tag>工具</tag>
        <tag>实用</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用工具 | 也许，这是最强大的一款浏览器插件]]></title>
    <url>%2F2019%2F07%2F14%2Ftampermonkey%2F</url>
    <content type="text"><![CDATA[前言 浏览器是我们日常工作中接触最多的工具之一，甚至在很多人的排行榜里毫无争议的夺得第一的位置。目前市面上浏览器可谓是五花八门，谷歌浏览器、IE浏览器、火狐浏览器、QQ浏览器、搜狗浏览器、360浏览器等等，但是归根结底，使用的内核主要分为两类：Chromium内核和Trident内核(又称IE内核)。由于浏览器在工作中扮演者至关重要的作用，使用比重也非常之大，因此，简单的官网默认浏览器很难满足我们各种各样的需求，所以，浏览器插件也就应用而生。甚至，对于很多浏览器来说，它的最大特色和吸引人的地方就是丰富而实用的插件。 如果让选出几款不错的插件推荐给大家，不同的使用者应该会推荐不同的插件，毕竟每个人的使用偏好和工作内容不同。但是我相信，对于大多数推荐者都不会忽略一个插件，也就是本文的主角：Tampermonkey。可以毫不谦虚的说，Tampermonkey是目前最为流行的用户脚本管理器，它适用于 Chrome, Microsoft Edge, Safari, Opera Next, 和 Firefox。用户脚本是一个什么东西？简而言之，不同脚本可以实现不同的功能，Tampermonkey可以对这些功能进行管理，让你的浏览器如虎添翼。 由于我个人日常使用谷歌浏览器较多，因此在这里就以谷歌浏览器为例为大家推荐几款不错的插件，每一款都让人赞不绝口。 概述Tampermonkey有很多可选的脚本，但是如果让推荐的话，我认为以下5款是必不可少的： AC-baidu Yet Another Weibo Filter 百度网盘直链下载助手 豆瓣资源下载大师 破解VIP会员视频集合 下面就逐个详细介绍一下上述5款插件，耐心往后面看，一个比一个强大。 AC-baidu 提及百度搜索，应该很多人想到的就是广告、混乱，的确，经常使用谷歌搜索，每当回到百度搜索时都会克制不住的质疑：“为什么会存在百度搜索这样的东西？” 的确，广告、相关推荐、垃圾信息，样样都有，就是没有我们想要的东西。 有了AC-baidu这个脚本，上述困扰就迎刃而解了。 它能够让你的搜索重定向到原始网页，拦截百家号等无用推广，让搜索网页回到最原始、最本质的样子。同时，每个搜索条后面都会有一个block字样，如果觉得对某些网站或者搜索条不满意，可以点击一些block就可以在以后的搜索中屏蔽这些搜索条。 Yet Another Weibo Filter 微博，是我们日常接触到较多的社交工具，甚至很多人每天都会反复多次刷微博。如果你喜欢用电脑浏览器刷微博应该清楚，它的页面信息十分混乱，多而杂，热门视频、特别关注、微博电影榜等等。我们唯一想看的就是微博，但是在浏览过程中却不得不被这些混乱的信息所干扰。有了Yet Another Weibo Filter就不用为此烦恼了，让你看真正想看的微博。 安装Yet Another Weibo Filter脚本之后打开微博会发现，右上角会出现一个漏斗状的一个图标，点击图标会打开上述界面，我们可以对微博进行内容、账号、话题、来源等进行过滤和设置，而且可以对版面进行清理，功能进行改造，而且还可以通过外观样式来修改字体、字号等内容，看看下面这幅图，经过版面清理之后是不是很整洁？ 百度网盘直链下载助手百度网盘是资源共享使用较多的一个工具，因此很多同学会通过各种网盘搜索工具寻找百度网盘的资源。但是资源找到了，会发现一个令人头疼的问题，文件太大无法直接下载，必须保存到个人网盘、打开PC客户端才可以下载。而打开客户端下载又被百度限速，非常痛苦，百度网盘直链下载助手就能够轻松解决这个问题。 安装百度网盘直链下载助手这个脚本之后会发现，浏览器打开百度网盘时上端会出现一个下载助手的选项卡，点击后会弹出两个选项：API下载、外链下载。这样的话可以直接点击调用IDM等下载工具进行下载，也可以复制下载链接，粘贴到一些下载工具后下载。 豆瓣资源下载大师 喜欢影视、音乐、图书的同学对豆瓣应该都不陌生，有大量的影视评论、书评。很多人会想，豆瓣上书籍、影视、音乐倒是不少，但是只能看看评论、评分，又有什么意义呢？豆瓣资源下载大师就让这个网站变的有了意义，把一个单纯的论坛和资源紧密的联系了一起。 安装豆瓣资源下载大师脚本之后，打开要找的电影、电视剧、图书等，会在右端状态栏很多匹配的资源列表，当然也包括下载的链接，下面就通过一个动画来演示下载《流畅的Python》这本书籍。 点击匹配的对应资源即可找到下载链接。 破解VIP会员视频集合 看电影、追剧，是很多同学闲暇之余最大的乐趣之一。但是发现我们要看的影视分布在优酷、腾讯、爱奇艺等平台，如果要买吧，太耗钱，不买吧，又要忍受冗长的广告。狠下心买了之后发现，很多电影需要观影券，这时候都会愤恨的说一句”与其这样，我还买你们会员干什么？“ 既然买了会员还不行，那么只有通过暴力方法来解决这个问题。关于视频破解工具，网上可谓是层出不穷，但是经过我的试用发现，真的不敢恭维，绝大多数都是不稳定或者压根不能用，而剩余个别能用的在打开时又非常缓慢，卡顿，直到遇到破解VIP会员视频集合这个脚本，发现真的令人惊讶，怎么可以有这么强大的神器？ 安装脚本之后会发现，打开视频后会在左边缘出现一个黄色箭头，点击这个箭头之后会弹出多个资源选项，点击其中一个会对我们看的视频进行解析，能够跳过广告、破解会员、观影券限制，更重要的是，它还很快，下面就来演示一下。 重点！！！亲测优酷、爱奇艺、腾讯视频均可用！ 脚本安装方法安装方式有两种： Chrome网上应用店 离线安装 Chrome网上应用店 如果能够访问Chrome网上应用店，我建议通过应用商店安装，便捷、安全。只需打开应用商店，搜索Tampermonkey，添加至Chrome即可。 离线安装 如果无法访问应用商店，则只能通过离线下载crx格式插件，然后点击右上角—更多工具—扩展程序，把crx格式插件拖动到空白处即可， tampermonkey安装之后点击图标，选择获取新脚本， 然后点击GreasyFork， 然后搜索、点击对应的的脚本安装即可， 插件下载如果无法访问Chrome网上应用店，则只能通过离线下载安装的方式，网上有很多Tampermonkey的资源，但是大多数很混乱，为了避免寻找的麻烦，我把插件进行共享了，需要的可以关注公众号，回复关键字”tmk“获取。]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习资源 | 如何学习优化算法？]]></title>
    <url>%2F2019%2F07%2F13%2Flearn-optimization%2F</url>
    <content type="text"><![CDATA[前言 在学习机器学习的一段时间之后，如果善于总结都会发现，无论是传统机器学习中比较经典的支持向量机，还是深度学习，无论是目前应用较广的计算机视觉，还是让AlphaGo大发神威的强化学习，最终都会涉及一个优化问题，或者是微积分系列的凸优化算法，或者是粒子群、蚁群等群体智能优化算法，或者是近几年比较热门的黑盒优化算法。尤其是近两年在工业控制方面契合度较高的强化学习，仔细分析它的模型，其实就是一个数学优化模型。最优化问题在当今的军事、工程、管理等领域有着极其广泛的应用。因此，优化算法的重要性可见一斑。理解优化算法，能够有助于理解深度学习的运作过程，对于模型的优化和改进也大有益处。本文就概括一下常用的优化算法并介绍一些学习资源。 优化算法概括 我个人对优化算法划分主要为3类，分别是： 凸优化 智能优化 黑盒优化 其中目前用的比较广泛的，尤其是在机器学习领域就是凸优化，例如梯度优化算法系列的梯度下降法、随即梯度下降、小批量梯度下降法、动量法momentum、Adagrad、RMSProp、Adadelta、Adam等，它们都是以梯度下降法为基础，在梯度下降法的基础上进行改进和优化。除了常用的这些还有牛顿法系列，以及无约束优化算法中的模式搜索法、Rosenbrock方法、单纯形搜索法、Powell方法。 凸优化虽然很成熟，但是很多工程问题并非是严格的符合凸优化的要求，换句话说，它是一个非凸优化问题，这样直接利用前面提到的这些算法很容易陷入局部最小值。因此，为了满足工程需求，研究者会根据问题的需求提出一些新颖的优化算法，其中就包括目前在工程应用领域比较热门的群体智能优化算法系列，例如，粒子群优化、模拟退火法、遗传算法，它们以独特而适应性强的有点在工程应用领域倍受欢迎，尤其是在复杂数学模型求解问题中能够更快速的求解同时避免陷入局部最优。 黑盒优化算法我最初是在谷歌开放的内部调参系统Google Vizier介绍论文Google Vizier: A Service for Black-Box Optimization提到的。在前面的优化算法中，优化问题都是建立在一个完整的数学模型基础之上，但是现实世界中很多场景是很难用数学模型来描述，或者没有数学模型，例如我们经常接触到的交通系统。在这种问题求解过程中，上述严格依赖数学模型的优化算法就显得有些捉襟见肘。谷歌在2017年在开放内部调参系统的介绍论文中详细介绍了它们用于调参的几种优化算法，其中包括如下几种算法： 贝叶斯优化 进化策略 SMAC 随机搜索 并在文中详细的对比了几种黑盒优化算法的效果。 下面分别针对这3类优化算法介绍一些学习资料。 凸优化算法 凸优化算法在目前机器学习中用的较多，其中分别有： 梯度下降法 动量法 Adam RMSProp Adagrad … 感兴趣的可以看我的另一篇文章，里面对机器学习中常用的优化算法推导过程及不同算法之间的关系进行了详细的阐述：一文了解人工智能中常用的优化算法 由于凸优化发展时间较长，而且理论体系比较完善，因此在微积分、数值计算等课程中都会涉及一部分，但是分布比较零散，不同于目前机器学习系列的课程，针对性较强，而且内容专一。虽然课程方面没有针对纯粹优化算法的，但是书籍方面却有很多，在这里我推荐两本不错的凸优化算法的书籍， 《最优化理论与方法》—袁亚湘，孙文瑜 作者袁亚湘为中国科学院院士、数学家，在计算数学、运筹学、应用数学领域有较深入的研究。曾有幸听过袁亚湘院士到学校开的优化算法专题讲座，真可谓是”听君一席话，胜读十年书”，于是就购买了袁亚湘院士的这本书籍。语言生动而易懂，系统地介绍了无约束量优化，约束优化和非光滑量优化的理论和计算方法，内容全面而丰富。 《最优化理论与算法（第2版）》—陈宝林 本书是陈宝林教授在多年的授课基础之上编著而成，与袁亚湘院士的书籍目录划分结构不同，但是我认为这种内容分层更有助于初学者的学习，他分别把优化算法划分成单纯形方法、对偶理论、灵敏度分析、运输问题、内点算法、非线性规划KT条件、无约束优化方法、约束优化方法、整数规划和动态规划等内容。 智能优化算法 智能优化算法的发展历史相对而言要短一些，但是由于都是在工程应用领域遇到瓶颈是应运而生，因此它的实用价值和效果更加让它们受欢迎，目前比较经典的智能优化算法有， 遗传算法 禁忌搜索 模拟退火法 蚁群算法 粒子群优化算法 由于智能优化算法更多是应工程应用需求而生，因此在数学模型方面并没有太多改进，因此在通识教育的数学课程中也很少涉及，同时，相关的书籍较少，在这里我就推荐一本智能优化算法的书籍。 《智能优化方法》—汪定伟 之所以推荐这本书，更多的是因为它的全面，它几乎囊括了目前所有主流的智能优化算法，其中当然就有遗传算法、蚁群算法、粒子群算法等。书中讨论这些算法的产生和发展、算法的基本思想和理论、基本构成、计算步骤和主要的变形以及数值例子和实际应用，对于学习者非常友好。 黑盒优化算法 就如同前文所讲，黑盒优化算法我最初实在2017年谷歌开放内部调参系统的介绍论文中看到的，它详细的介绍了内部调参系统Google Vizier使用的几种主流黑盒优化算法。之所以称之为黑盒，就是因为在这类优化问题中我们没有数学模型，我们不清楚优化的目标函数到底什么样。这种场景和我们日常所接触的现实场景更加贴近，因此它的实用价值自然不言而喻。 在谷歌的这篇文章中，不仅介绍了系统内部使用的黑盒优化算法，还在不同维度求解问题下对比了以下几种优化算法的效果： 随机搜索 贝叶斯 SMAC 进化策略 概率搜索 虽然谷歌的文章发表于2017年，但是里面提及的算法并不算新颖，其中的算法都是经过几年甚至几十年的不断改进而形成现如今的样子，所以要想详细学习需要看一下Google Vizier: A Service for Black-Box Optimization这篇提及的参考文献，比较零散。虽然这些成熟算法的理论体系比较零散，但是它们共同用到的理论知识却是成体系的，它们都用到了概率论\随机过程相关的知识，尤其是其中表现较好的贝叶斯优化和进化策略，都是建立在高斯过程的基础之上，因此，本文就推荐1本随机过程方面的书籍，对这些概率论\随机过程的基础知识有所了解更加有助于对这些成形算法的理解。 《随机过程（原书第2版）》—Sheldon M.Ross 本书由世界著名的应用概率专家和统计学家Sheldon M. Ross编著，本书介绍了从概率论基础概念，到各种常见的分布模型。详细的介绍随机过程中经典的知识，包括Poisson过程、Markov链、鞅、Brown运动、随机序关系、Poisson逼近，并详细的介绍了这些理论的应用，更加有助于理解和学习。]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | 推荐两款提高windows下工作效率的工具]]></title>
    <url>%2F2019%2F07%2F13%2Ftwo-windows-tools%2F</url>
    <content type="text"><![CDATA[前言好的工具能让做事效率事半功倍，学习和工作都是这样。不同专业方向都会有一些很知名、耳熟能详的工具，例如开发方面的visual studio、pycharm，办公方面的office、xmind。这些软件的确很强大，但是再强大的工具都很难做到面面俱到，把效率考虑的十分周全。而有一些高手就及时发现里面的不足之处并开发出一些强大高效的工具，能够让日常工作效率大大提升，本文要介绍的两款工具就是这样的：冷门而强大，它们分别是：DropletIt和Quicker，下面就来看一下这两款工具究竟强大在哪里。 DropIt 工作中，日积月累会积攒很多各种各样的文件，有word、Excel、powerpoint、pdf等文档，有png、jpg等图片，有zip、tgz、7z等压缩包，尤其是很多同学都有个习惯，为了方便会放在桌面上，当想要找自己需要的东西时如同大海捞针一样，不知道从何下手。我想这是困扰很多人的问题，删除—积累，不断的重复，但是始终没有找到一个高效的解决文件分类方法。我想说，有了DropIt，再也不用担心文件分类与管理了，真正的实现了文件一建整理，下面来介绍一下DropIt的使用。 下载安装 下载安装之后桌面会有这样一个图标， 添加协议 为了满足我们整理文件的偏好和需求，需要对DropIt设置一下协议，让它按照我们预先设定的协议整理，添加协议主要包括4个部分： 名称：添加协议的名称，按照自己的爱好随便命名即可。 规则：匹配文件的规则，按照我们需要整理的文件设置匹配规则，例如\.png*匹配以png结尾的文件，如果包含多个规则可以用;隔开。 操作：对我们规则匹配到的文件采用的操作，其中包括移动、删除、压缩等。 目标文件夹：对文件处理的目标文件夹，例如，移动规则匹配到的文件到目标文件夹。 例如，上述我个人设置的两个协议，分别对图片(bmp、gif、jpg)和压缩包(zip、7z)进行处理，将图片和压缩包分别移动到指定的文件夹内。 设置好协议之后只需要选中文件，拖动到DropIt图标上方即可，下面来看看效果， 这样，选中的文件会按照我们预先设定的协议分别移动到对应的文件夹内，就不用我们逐个选中文件然后剪切、粘贴到指定文件夹。 Quicker 之前介绍过两款高效的办公工具：Listary和Wox。如果说Listary和Wox是键盘增强工具，那么Quicker就是一款强大的鼠标增强工具，能够让对鼠标比较依赖的同学发现，原来鼠标可以做这么多事情。 我们都知道，大多数鼠标包含3个按键，分别是：左键、右键、中键。其中左键和右键日常工作中使用较为频繁，但是中键除了上下翻页之外很少使用。Quicker就合理的利用了这一点，为鼠标中键添加上一个强大的快捷面板。软件默认的快捷面板包含我们常用的记事本、计算器、截图、我的电脑、Excel等工具，能够避免再去开始菜单寻找这些小工具，能够大大提高效率。 如果觉得软件自带的动作不足以满足自己的使用需求，那么还可以添加其他的动作，添加方式包括两种： 添加他人分享的动作 自己设计动作 添加他人分享的动作 首先打开官网，https://getquicker.net/Share 会发现官网动作库包含很多别人分享的动作，其中不乏查询搜索、翻译、文本处理、编程相关，可以根据自己的需求搜索对应的动作，然后复制动作，中键打开快捷面板，点击鼠标右键，选择粘贴分享的动作，然后安装即可，下面来演示一下安装过程， 如果在分享的动作库找不到自己需要的，或者感觉别人分享的无法满足自己的需求，没问题，还可以选择自己设计动作，Quicker提供两种动作创建方式： 基础动作 组合动作 基础动作 基础动作主要包含打开网址、发送文本、模拟按键等，创建方法很简单：点击鼠标中键打开快捷面板，点击+号，创建基础动作，选择动作类型，录制即可，当然也可以选择快捷的方式录入打开网址等动作。 组合动作 如果觉得基础动作太单一，还不够便捷，没问题，我认为Quicker最强大的地方就是支持组合动作。创建组合动作相对而言也要复杂一些，我认为它像是一种高阶的编程语言，可以用条件语句、添加变量名来实现一连串的动作，这个相对复杂而且不够大众化，因此在这里不多阐述，如果喜欢折腾的可以查看官网教程，创建一些高级的动作来提高自己的效率。 https://www.yuque.com/quicker/help/xaction-editor]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【动手学计算机视觉】第八讲：传统目标检测之HOG特征]]></title>
    <url>%2F2019%2F07%2F10%2Fcv-hog%2F</url>
    <content type="text"><![CDATA[前言 如果自称为计算机视觉工程师，没有听说过前文提到的尺度不变特征变换(SIFT)，可以理解，但是如果没有听说过方向梯度直方图(Histogram of oriented gradient，HOG)，就有一些令人诧异了。这项技术是有发过国家计算机技术和控制研究所(INRIA)的两位研究院Navneet Dalal和Bill Triggs在2005年CVPR上首先发表提出(那时的CVPR含金量还是很高的)。原文Histograms of oriented gradients for human detection截止2019年7月10日引用率已经达到26856。 HOG通过计算局部图像提取的方向信息统计值来统计图像的梯度特征，它跟EOH、SIFT及shape contexts有诸多相似之处，但是它有明显的不同之处：HOG特征描述子是在一个网格秘籍、大小统一的细胞单元上进行计算，而且为了提高性能，它还采用了局部对比度归一化思想。它的出现，使得目标检测技术在静态图像的人物检测、车辆检测等方向得到大量应用。 在传统目标检测中，HOG可以称得上是经典中的经典，它的HOG+SVM+归一化思想对后面的研究产生深远的影响，包括后面要讲到的神作DPM，可以说，HOG的出现，奠定了2005之后的传统目标检测的基调和方向，下面就来了解一下这个经典之作。 方向梯度直方图 HOG特征的算法可以用一下几个部分概括， 梯度计算 单元划分 区块选择 区间归一化 SVM分类器 下面分别来详细阐述一下。 梯度计算由于后面要进行归一化处理，因此在HOG中不需要像其他算法那样需要进行预处理，因此，第一步就成了梯度计算。为什么选择梯度特征？因为在目标边缘处灰度变化较大，因此，在边缘处灰度的梯度就较为明显，所以，梯度能够更好的表征目标的特征。 我们都知道在数学中计算梯度需要进行微分求导，但是数字图像是离散的，因此无法直接求导，可以利用一阶差分代替微分求离散图像的梯度大小和梯度方向，计算得到水平方向和垂直方向的梯度分别是， G_{h}(x, y)=f(x+1, y)-f(x-1, y),\forall x, yG_{v}(x, y)=f(x, y+1)-f(x, y-1) ,\forall x, y其中$f(x,y)$表示图像在$(x,y)$的像素值1。 可以得到梯度值(梯度强度)和梯度方向分别为, M(x, y)=\sqrt{G_{h}(x, y)^{2}+G_{v}(x, y)^{2}}\theta(x, y)=\arctan \left(G_{h}(x, y) / G_{v}(x, y)\right.单元划分 计算得到梯度的幅值和梯度方向之后，紧接着就是要建立分块直方图，得到图像的梯度大小和梯度方向后根据梯度方向对图像进行投影统计，首先将图像划分成若干个块(Block)，每个块又由若干个细胞单元(cell)组成，细胞单元由更小的单位像素(Pixel)组成，然后在每个细胞单元中对内部的所有像素的梯度方向进行统计。Dalal和Triggs通过测试验证得出，把方向分为9个通道效果最好，因此将180度划分成9个区间，每个区间为20度，如果像素落在某个区间，就将该像素的直方图累加在该区间对应的直方图上面，例如，如果像素的梯度方向在0~20度之间，则在0~20对应的直方图上累加该像素对应的梯度幅值。这样最终每个细胞单元就会得到一个9维的特征向量，特征向量每一维对应的值是累加的梯度幅值。 区块选择为了应对光照和形变，梯度需要在局部进行归一化。这个局部的区块该怎么选择？常用的有两种，分别是矩形区块(R-HOG)和圆形区块(C-HOG)，前面提供的例子就是矩形区块，一个矩形区块由三个参数表示：每个区块由多少放歌、每个方格有多少像素、每个像素有多少通道。前面已经提到，经过作者验证，每个像素选择9个通道效果最佳。同样，作者对每个方格采用的像素数也进行验证，经过验证每个方格采用3*3或者6*6个像素效果较好。 区间归一化每个方格内对像素梯度方向进行统计可以得出一个特征向量，一个区块内有多个方格，也就有多个特征向量，例如前面的示例区块Block内就有4个9维向量。这一步要做的就是对这4个向量进行归一化，Dalal和Triggs采用了四种不同的方式对区块进行归一化，分别是L2-norm、L2-hys、L1-norm、L1-sqrt，用$v$表示未被归一化的向量，以L2-norm为例，归一化后的特征向量为， v=\frac{v}{\sqrt{\|v\|_{2}^{2}+\varepsilon^{2}}}作者通过对比发现，L2-norm、L2-hys、L1-sqrt三种方式所取得的效果是一样的，L1-norm表现相对差一些。 SVM分类器最后一步，也是比较关键的一步，就是训练分类器，用SVM对前面提取的图像特征向量进行训练，寻找一个最优超平面作为决策函数，得到目标的训练模型。 编程实践完整代码请查看： https://github.com/Jackpopc/aiLearnNotes/blob/master/computer_vision/HOG.py HOG是一个优秀的特征提取算法，因此本文就仅介绍并实现特征提取算法部分，后面的训练分类器和目标检测偏重于机器学习内容，在这里就不多赘述。 HOG算法非常经典，因此，很多成熟的第三方库都已经集成了这个算法，例如比较知名的计算机视觉库OpenCV，对于HOG特征提取比较简单的方式就是直接调用OpenCV库，具体代码如下， 1234import cv2hog = cv2.HOGDescriptor()img = cv2.imread("../data/2007_000129.jpg", cv2.IMREAD_GRAYSCALE)des = hog.compute(img) 为了更好的理解HOG算法，本文就跟随文章的思路来重新实现一遍算法。 第一步：计算梯度方向和梯度幅值 这里用Sobel算子来计算水平和垂直方向的差分，然后用对梯度大小加权求和的方式来计算统计时使用的梯度幅值， 123456def compute_image_gradient(img): x_values = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5) y_values = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5) magnitude = cv2.addWeighted(x_values, 0.5, y_values, 0.5, 0) angle = cv2.phase(x_values, y_values, angleInDegrees=True) return magnitude, angle 第二步：统计细胞单元的梯度方向 指定细胞单元尺寸和角度单元，然后对用直方图统计一个细胞单元内的梯度方向，如果梯度角度落在一个区间内，则把该像素的幅值加权到和角度较近的一个角度区间内， 123456789101112def compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit): centers = [0] * bin_size # 遍历细胞单元，统计梯度方向 for i in range(cell_magnitude.shape[0]): for j in range(cell_magnitude.shape[1]): strength = cell_magnitude[i][j] gradient_angle = cell_angle[i][j] min_angle, max_angle, mod = choose_bins(gradient_angle, unit, bin_size) # 根据角度的相近程度分别对邻近的两个区间进行加权 centers[min_angle] += (strength * (1 - (mod / unit))) centers[max_angle] += (strength * (mod / unit)) return centers 第三步：块内归一化 根据HOG原文的思想可以知道，图像内分块，块内分细胞单元，然后对细胞单元进行统计。一个块由多个细胞单元组成，统计了每个细胞单元的梯度特征之后需要对这几个向量进行归一化， 1234567891011121314151617def normalized(cell_gradient_vector): hog_vector = [] for i in range(cell_gradient_vector.shape[0] - 1): for j in range(cell_gradient_vector.shape[1] - 1): block_vector = [] block_vector.extend(cell_gradient_vector[i][j]) block_vector.extend(cell_gradient_vector[i][j + 1]) block_vector.extend(cell_gradient_vector[i + 1][j]) block_vector.extend(cell_gradient_vector[i + 1][j + 1]) mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector)) magnitude = mag(block_vector) if magnitude != 0: # 归一化 normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector] block_vector = normalize(block_vector, magnitude) hog_vector.append(block_vector) return hog_vector 第四步：可视化 为了直观的看出特征提取的效果，对下图进行特征提取并且可视化， 可视化的方法是在每个像素上用线段画出梯度的方向和大小，用线段的长度来表示梯度大小， 12345678910111213141516171819def visual(cell_gradient, height, width, cell_size, unit): feature_image = np.zeros([height, width]) cell_width = cell_size / 2 max_mag = np.array(cell_gradient).max() for x in range(cell_gradient.shape[0]): for y in range(cell_gradient.shape[1]): cell_grad = cell_gradient[x][y] cell_grad /= max_mag angle = 0 angle_gap = unit for magnitude in cell_grad: angle_radian = math.radians(angle) x1 = int(x * cell_size + magnitude * cell_width * math.cos(angle_radian)) y1 = int(y * cell_size + magnitude * cell_width * math.sin(angle_radian)) x2 = int(x * cell_size - magnitude * cell_width * math.cos(angle_radian)) y2 = int(y * cell_size - magnitude * cell_width * math.sin(angle_radian)) cv2.line(feature_image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude))) angle += angle_gap return feature_image 提取的特征图为，图中白色的线段即为提取的特征， 完整代码如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import cv2import numpy as npimport mathimport matplotlib.pyplot as pltimg = cv2.imread("../data/2007_000129.jpg", cv2.IMREAD_GRAYSCALE)def compute_image_gradient(img): x_values = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5) y_values = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5) magnitude = abs(cv2.addWeighted(x_values, 0.5, y_values, 0.5, 0)) angle = cv2.phase(x_values, y_values, angleInDegrees=True) return magnitude, angledef choose_bins(gradient_angle, unit, bin_size): idx = int(gradient_angle / unit) mod = gradient_angle % unit return idx, (idx + 1) % bin_size, moddef compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit): centers = [0] * bin_size for i in range(cell_magnitude.shape[0]): for j in range(cell_magnitude.shape[1]): strength = cell_magnitude[i][j] gradient_angle = cell_angle[i][j] min_angle, max_angle, mod = choose_bins(gradient_angle, unit, bin_size) print(gradient_angle, unit, min_angle, max_angle) centers[min_angle] += (strength * (1 - (mod / unit))) centers[max_angle] += (strength * (mod / unit)) return centersdef normalized(cell_gradient_vector): hog_vector = [] for i in range(cell_gradient_vector.shape[0] - 1): for j in range(cell_gradient_vector.shape[1] - 1): block_vector = [] block_vector.extend(cell_gradient_vector[i][j]) block_vector.extend(cell_gradient_vector[i][j + 1]) block_vector.extend(cell_gradient_vector[i + 1][j]) block_vector.extend(cell_gradient_vector[i + 1][j + 1]) mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector)) magnitude = mag(block_vector) if magnitude != 0: normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector] block_vector = normalize(block_vector, magnitude) hog_vector.append(block_vector) return hog_vectordef visual(cell_gradient, height, width, cell_size, unit): feature_image = np.zeros([height, width]) cell_width = cell_size / 2 max_mag = np.array(cell_gradient).max() for x in range(cell_gradient.shape[0]): for y in range(cell_gradient.shape[1]): cell_grad = cell_gradient[x][y] cell_grad /= max_mag angle = 0 angle_gap = unit for magnitude in cell_grad: angle_radian = math.radians(angle) x1 = int(x * cell_size + magnitude * cell_width * math.cos(angle_radian)) y1 = int(y * cell_size + magnitude * cell_width * math.sin(angle_radian)) x2 = int(x * cell_size - magnitude * cell_width * math.cos(angle_radian)) y2 = int(y * cell_size - magnitude * cell_width * math.sin(angle_radian)) cv2.line(feature_image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude))) angle += angle_gap return feature_imagedef main(img): cell_size = 16 bin_size = 9 unit = 360 // bin_size height, width = img.shape magnitude, angle = compute_image_gradient(img) cell_gradient_vector = np.zeros((height // cell_size, width // cell_size, bin_size)) for i in range(cell_gradient_vector.shape[0]): for j in range(cell_gradient_vector.shape[1]): cell_magnitude = magnitude[i * cell_size:(i + 1) * cell_size, j * cell_size:(j + 1) * cell_size] cell_angle = angle[i * cell_size:(i + 1) * cell_size, j * cell_size:(j + 1) * cell_size] cell_gradient_vector[i][j] = compute_cell_gradient(cell_magnitude, cell_angle, bin_size, unit) hog_vector = normalized(cell_gradient_vector) hog_image = visual(cell_gradient_vector, height, width, cell_size, unit) plt.imshow(hog_image, cmap=plt.cm.gray) plt.show()if __name__ == '__main__': img = cv2.imread('../data/2007_002293.jpg', cv2.IMREAD_GRAYSCALE) cv2.imshow("origin", img) cv2.waitKey() main(img)]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[效率工具 | Windows下一款强大的启动搜索工具]]></title>
    <url>%2F2019%2F07%2F02%2Ftools-wox%2F</url>
    <content type="text"><![CDATA[前言对于大多数人来说，日常生活和工作中接触较多的软件和工具就是浏览器、专业软件、翻译软件、笔记、办公等。其实有很多软件在名气上虽然不如这些商业软件，但是功能却丝毫不输这些知名软件。在工作中能够大大提高办公效率，而且内存占用小、免费开源。大家都知道windows自带的文件浏览器查找文件是一件非常令人痛苦的事情，不仅速度缓慢，而且准确度出奇的低，让人感觉很鸡肋。但是当我们要找一个文档时却忘记放在哪里，挨个硬盘去翻更加令人感到折磨。所以不得不去借助一些高效的搜索工具，其中用的较多、名气较大的就是everything。我个人也一直在用这款工具，的确非常强大，快速、支持正则表达式匹配。它作为一个文件搜索工具的确很称职，但是当我们想要更多扩展功能，例如用于程序启动工具时everything就显得有些不足了。之前我介绍过一款工具叫做Listary，能够完美的与everything结合，既能涵盖everything强大的搜索功能，还能融合Listary实用的启动功能。本文再给大家介绍一款与Listary类似的工具—Wox，有相同之处，也有很大的差异之处，各位可以根据自己的喜好进行选择。 Wox Wox是一款启动器。这就是它与Listary的最大的不同之处—定位不同。Listary本身兼顾搜索与启动，但是在搜索方面不如everything，如果想使用更加丰富的搜索功能需要在设置里配置一下everything，如果满足于Listary提供的快速搜索功能则无需配置。而Wox的定位就是一个简单、纯净的启动器。它可以快速的启动本机安装的各种程序、文件、网页等。当然，它也可以用于文件搜索，它指定的后端搜索工具是everything，所以在打开Wox之前需要先启动everything，这样才能够使用强大的搜索功能。它不仅可以用于搜索程序和文件，还可以配置各种丰富的插件满足更多场景的需求，例如计算器、天气、翻译、网页搜索等。 Wox与Listary前面提到了，Wox与Listary有很多类似之处： 搜索 启动器 配合everything使用 但是Wox也有很多特别之处是Listary无法比拟的，Wox的特别之处主要有如下几点： 支持丰富的插件 支持自己定义插件 支持多种主题切换 支持自定义快捷键 支持丰富插件Wox的插件主要分类两种： 系统插件 第三方插件 系统插件不需要关键字唤醒，直接用Alt + Space调出Wox的工具栏输入相应的命令即可，系统插件主要包含如下几类： 程序插件 颜色插件 控制面板插件 计算器插件 网址插件 Web搜索插件 命令行插件 文件夹插件 拿其中几个举个例子， 程序插件 Alt+空格键激活Wox，然后输入要启动的程序即可， 计算器插件 计算器对于很多人来说虽然不是主要的工作工具，但是偶尔会用到，当我们需要用计算器的时候就需要点击windows图标，搜索“计算器”，这样比较麻烦，Wox集成了计算器插件，激活Wox后输入要计算的公式即可， 网址插件 当我们要浏览某个网站时往往需要打开浏览器-&gt;在地址栏输入网址，Wox的浏览器插件大大简化这个过程，只需要激活Wox，输入相应网址即可， 其他还有很多实用的系统插件，可以查看网站进行了解， http://doc.wox.one/zh/basic/ 除了Wox自带的系统插件，Wox还提供了多大230款第三方插件，其中就包含有道翻译、天气查询、Steam、Putty、二维码、维基百科、书签搜索、待办事项、进制转换、哔哩哔哩、Skype、FileZilla、Stack Overflow、沪江日语等等。只需要下载安装一下即可，而且Wox提供了多选、简单的安装方式， 安装第三方插件 命令安装 这是最简单的一种安装方式，使用wpm进行插件的安装、卸载管理， 123456789# 安装插件wpm install &lt;插件名称&gt;# 卸载插件wpm uninstall &lt;插件名称&gt;# 列出已安装插件wpm list 手动安装 如果由于网络、代理等原因无法命令安装，可以打开插件主页[http://www.wox.one/plugin]下载到本地(以.wox结尾),拖动到Wox搜索框进行安装， 支持自己定义插件除了官网提供的系统插件和第三方插件之外，Wox还支持自定义插件，它支持以下3种方式来定义插件， plugin.json C# Python Wox与插件之间的通信原理： 支持多种主题切换 Wox安装后会发现自带BlurBlack、BlurWhite、Dark、Gray、Light、Metro Server、Pink七种主题，除了上述提到的7种主题之外，还可以在官网自定义主题，配置之后下载主题(.xaml文件)，放置到C:\Users\YourUserName\AppData\Local\Wox\app-1.3.524\Themes路径下，重启Wox即可。 支持自定义快捷键这一点也是Wox吸引人的一点，它支持自定义快捷键。如果觉得Alt+空格启动程序、文件夹还不够快捷，可以把常用的命令保存到快捷键，这样当使用快捷键时能够快速达到目的。 例如，我想百度搜索“哈尔滨工业大学”，使用Wox的方式是这样的， Alt + 空格激活Wox 输入”bd 哈尔滨工业大学” 这样比起”打开浏览器-&gt;打开百度-&gt;搜索”已经便捷了很多，但是还有更便捷的，就是Wox支持的快捷键。 可以把常用的命令添加到快捷键，例如把”bd 哈尔滨工业大学”添加为快捷键”Ctrl+Alt+H”,能够同时激活Wox并输入相应的命令，然后按Enter键即可搜索。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>实用工具</category>
      </categories>
      <tags>
        <tag>文件查找</tag>
        <tag>工具</tag>
        <tag>实用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一份热门机器学习资源]]></title>
    <url>%2F2019%2F07%2F01%2Fhomemade-machine-learning%2F</url>
    <content type="text"><![CDATA[前言最近几年人工智能异常火热，随之而来的就是各种针对入门者的学习资源，其中不乏很多经典的教程例如吴恩达的《机器学习》、《深度学习工程师》，但是也有很多千篇一律、照本宣科的学习资源。在学习进阶过程中很多人会到GitHub寻找一些可以动手实践的机器学习项目，会发现GitHub上会有和机器学习相关的各种awesome，恨不得把所有和机器学习、深度学习的资源都囊括进去。这样虽然全面，但是我认为它的价值并不高。我们之所以希望有经验者推荐学习资源，就是因为时间、精力有限，希望能够在鱼龙混杂的学习资源里筛选出真正有价值，或者与众不同的，能够让我们利用有限的精力和时间内真正学会一些东西。近期GitHub有一个关于机器学习的热门开源项目，homemade-machine-learning，目前已经11k+个star，近一周达到1.1k+，经过一段时间的学习发现这的确一个不错的学习项目，下面就详细介绍一下这个项目。 Homemade Machine Learning 开门见山，这个开源项目主要有以下几个优点： 少而精 不依赖python第三方库 详细解释它们背后的数学原理 交互式Jupyter notebook演示程序 丰富易懂的示例 这个项目用Python实现了目前热门、使用的一些机器学习算法，而不是像很多开源项目那样，从头至尾把每个机器学习算法都实现一遍。换句话说，这个开源项目追求“少而精”，它分别从监督学习、非监督学习、神经网络、异常检测、回归、分类这些类别中选择一种算法进行详细阐述算法背后的数学原理，然后使用jupyter notebook交互式的演示，随后会用多个示例进行实现，动手操作，不依赖集成的python第三方库，更容易理解机器学习算法的原理。 项目概括该项目主要包括如下几个方面的机器学习算法： 监督学习 无监督学习 异常检测 神经网络 其中监督学习又分为回归和分类，回归算法选取的是比较常用的线性回归，分类算法选取的是比较实用的逻辑回归。无监督学习中主要针对聚类进行讲解，项目中选取的是热门的k-means。异常检测是指通过大多数数据来检测出有显著差异的事件、观测结果，在数据处理、图像处理都有应用。神经网络中选择的是多层感知机。 安装首先要保证电脑上正确的安装了Python，然后安装一些项目依赖， 1pip install -r requirements.txt requirements: 1234567jupyter==1.0.0matplotlib==3.0.1numpy==1.15.3pandas==0.23.4plotly==3.4.1pylint==2.1.1scipy==1.1.0 如果要使用jupyter notebook，需要在命令行输入下面命令， 1jupyter notebook 然后会在浏览器中打开如下窗口， 详细介绍数学原理 我认为这是这个项目吸引人的地方，也是它与众不同的地方，它和很多项目不同，浮于表面，把很多环节都认为是既定的去阐述，有一些初学者会看的云里雾里，不明白“为什么是这样？”这个项目则不同，它详细、深入的阐述每个算法背后的数学原理，循序渐进，配合可视化很容易让人理解。 详细编码过程 该项目不过多依赖tensorflow、pytorch、keras这些高度集成的机器学习平台，它从梯度下降到损失函数、从训练到预测都是一步一步实现，尽量减少对高度集成第三方库的依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104@staticmethoddef gradient_descent(data, labels, initial_theta, lambda_param, max_iteration): """Gradient descent function. Iteratively optimizes theta model parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param initial_theta: initial model parameters. :param lambda_param: regularization parameter. :param max_iteration: maximum number of gradient descent steps. """ # Initialize cost history list. cost_history = [] # Calculate the number of features. num_features = data.shape[1] # Launch gradient descent. minification_result = minimize( # Function that we're going to minimize. lambda current_theta: LogisticRegression.cost_function( data, labels, current_theta.reshape((num_features, 1)), lambda_param ), # Initial values of model parameter. initial_theta, # We will use conjugate gradient algorithm. method='CG', # Function that will help to calculate gradient direction on each step. jac=lambda current_theta: LogisticRegression.gradient_step( data, labels, current_theta.reshape((num_features, 1)), lambda_param ), # Record gradient descent progress for debugging. callback=lambda current_theta: cost_history.append(LogisticRegression.cost_function( data, labels, current_theta.reshape((num_features, 1)), lambda_param )), options=&#123;'maxiter': max_iteration&#125; ) # Throw an error in case if gradient descent ended up with error. if not minification_result.success: raise ArithmeticError('Can not minimize cost function: ' + minification_result.message) # Reshape the final version of model parameters. optimized_theta = minification_result.x.reshape((num_features, 1)) return optimized_theta, cost_history@staticmethoddef gradient_step(data, labels, theta, lambda_param): """GRADIENT STEP function. It performs one step of gradient descent for theta parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param theta: model parameters. :param lambda_param: regularization parameter. """ # Initialize number of training examples. num_examples = labels.shape[0] # Calculate hypothesis predictions and difference with labels. predictions = LogisticRegression.hypothesis(data, theta) label_diff = predictions - labels # Calculate regularization parameter. regularization_param = (lambda_param / num_examples) * theta # Calculate gradient steps. gradients = (1 / num_examples) * (data.T @ label_diff) regularized_gradients = gradients + regularization_param # We should NOT regularize the parameter theta_zero. regularized_gradients[0] = (1 / num_examples) * (data[:, [0]].T @ label_diff) return regularized_gradients.T.flatten()@staticmethoddef cost_function(data, labels, theta, lambda_param): """Cost function. It shows how accurate our model is based on current model parameters. :param data: the set of training or test data. :param labels: training set outputs (0 or 1 that defines the class of an example). :param theta: model parameters. :param lambda_param: regularization parameter. """ # Calculate the number of training examples and features. num_examples = data.shape[0] # Calculate hypothesis. predictions = LogisticRegression.hypothesis(data, theta) # Calculate regularization parameter # Remember that we should not regularize the parameter theta_zero. theta_cut = theta[1:, [0]] reg_param = (lambda_param / (2 * num_examples)) * (theta_cut.T @ theta_cut) # Calculate current predictions cost. y_is_set_cost = labels[labels == 1].T @ np.log(predictions[labels == 1]) y_is_not_set_cost = (1 - labels[labels == 0]).T @ np.log(1 - predictions[labels == 0]) cost = (-1 / num_examples) * (y_is_set_cost + y_is_not_set_cost) + reg_param # Let's extract cost value from the one and only cost numpy matrix cell. return cost[0][0] 丰富示例 理解了算法背后的数学原理，跟着作者一步一步实现了算法，要想更加深入的理解就需要把算法应用到不同方面，本项目提供了丰富的示例，其中不乏MNIST这类经典的演示样例。 其中每个项目后面都包含至少一个示例，可以获取对应的数据进行实现，这样对算法的理解和应用会有更加清晰而深入的认识。 更多精彩内容请关注公众号【平凡而诗意】~]]></content>
      <categories>
        <category>学习资源</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Github</tag>
        <tag>资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文熟练掌握Docker使用]]></title>
    <url>%2F2019%2F06%2F30%2Flearning-docker%2F</url>
    <content type="text"><![CDATA[Docker是由dotCloud公司发起并与2013年开源的一个项目，一径开源就备受欢迎，其主要项目至今在github已经54k个star。它是使用Go语言开发实现，基于Linux内核cgroup、namespace以及AUFS类等技术对进程进行封装隔离，属于一种操作系统层面的虚拟化技术。此后，进一步开发开始使用runC和containerd，进一步封装，从文件系统到网路互联，再到进行都进行隔离，极大的简化了容器的创建和维护，使得Docker比虚拟机更为轻便、快捷。 为什么要用docker？Docker与传统虚拟机一样，同属于虚拟化技术，但是它拥有众多虚拟机无法比拟的优势： 持续交付和部署 更快的迁移 更高效的利用系统资源 更快的启动时间 一致的运行环境 更轻松的维护和扩展 容器与虚拟机对比详情： 对于大多数开发人员感受最为就是前两点：持续交付和部署、更快的迁移。 我想这对于很多开发人员都是一个很头疼的问题，在开发过程中会遇到这种抱怨：“在我电脑上可以运行啊？为什么换一台电脑就不行了？” 虽然诸如maven、nodejs的package.json、Python的requirement.txt的出现使得迁移变得简单，但是它们更多的是使得在第三方工具包的迁移方面变得简单方面，但是在系统和开发环境方面却没有什么作用。docker确保了直行环境的一致性，可以在多平台上运行，使得应用迁移更加容易。此外，docker使用分层存储以及镜像技术，使得应用重复部分的复用更加容易，可以基于基础镜像做更多的扩展，使得系统的维护变得更加简单。 基本概念使用docker接触最多的就是以下3个概念， 镜像：image 容器：container 仓库：repository 了解这三个概念，对容器的整个生命周期便有了认识。在这里，我用简单的语言对上述3个概念进行描述 镜像：进行就相当于一个精简化的文件系统，例如官方提供的Ubuntu镜像，就只包含了最小化的root文件系统。 容器：容器是一个拥有自己root文件系统、自己网络配置、自己命名空间的进程。镜像和容器就像是编程中的类和实例，镜像时静态的定义，而镜像运行时的实体是容器。什么是类和实例？举一个编程的例子阐述一下， 1234567891011121314# 类class HelloWorld: def __init__(self, x, y): self.x = x self.y = y def add(self): return self.x + self.y# 实例hello_world = HelloWorld(2, 3)print(hello_world.add())&gt;&gt;&gt; 5 其中HelloWorld是类，hello_world是实例，类比一下，就能够理解容器和镜像之间的关系。 仓库：docker镜像仓库就如同github代码仓库一样，当一个人构建一个项目，想在其他其他电脑上运行这个项目，那么就去从代码仓库把这个项目克隆下来。docker镜像仓库也是这样，当构建一个镜像之后，想在其他服务器上使用这个镜像，就需要一个集中的存储、分发服务，仓库就是这样的服务。官方的镜像仓库是DockerHub，它存储了丰富的镜像，但是国内拉取镜像速度缓慢，因此可以使用国内镜像仓库进行替代，例如阿里云镜像仓库、网易云镜像仓库、DaoCloud镜像市场等。 安装docker目前支持Linux、Windows 10、macOS，下面就一个Linux安装为例， APT方式安装 首先安装HTTPS软件包和CV证书， 123456$ sudo apt-get update$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 添加软件源GPG密钥， 1$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add 添加docker软件源， 1234$ sudo add-apt-repository \ "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable" 安装docker ce, 12$ sudo apt-get update$ sudo apt-get install docker-ce 添加用户组 docker命令会使用Unix socket与docker引擎通讯，因此每次使用时会需要root权限，也就是需要在命令前加sudo比较麻烦，为了避免这个麻烦可以把建立docker组并把当前用户加入docker用户组， 12$ sudo groupadd docker$ sudo usermod -aG docker $USER 启动、退出、重启docker 123$ systemctl start docker$ systemctl stop docker$ systemctl restart docker 也可以使用， 123$ service docker start$ service docker stop$ service docker restart Dockerfile理解docker中一些基本概念，并完成docker安装下一步就是学习docker的使用。对于大多数开发人员来说，docker使用过程中最为核心的部分就是Dockerfile。 Dockerfile是一个文本文件，它包含了一些指令，docker镜像的构建就是通过Dockerfile中的这一条一条的指令完成的。也就是说，要构建一个镜像，就需要一个Dockerfile，然后根据自己的需求配置一些指令集合，下面就看一下Dockerfile中使用的一些指令。 FROM：指定基础镜像 定制我们的镜像，是需要以一个镜像为基础的，就是基础镜像，例如Ubuntu、 nginx、postgres、mysql等，例如，FROM Ubuntu:16.04，如果本地有Ubuntu基础镜像则使用本地基础镜像，如果没有则会到官方镜像仓库拉取，16.04是镜像版本号，如果不指定则会拉取lastest。 RUN：执行命令 RUN指定我们在构建镜像时需要执行的命令，比如apt-get install安装某个软件，pip install安装Python依赖包，配置软件源，配置时区等， 例如，RUN apt-get install python3。 ADD和COPY：文件操作 ADD和COPY是两个功能类似的指令，一般优先使用COPY，它比ADD更透明，它的功能是将本地文件拷贝到容器中，例如，COPY ./ /home/jackpop/test。 WORKDIR：指定工作路径 指定镜像的运行时的工作路径，例如，WORKDIR /home/jackpop/test 。 ENTRYPOINT：设置镜像主命令 指定镜像运行是运行的命令，例如, ENTRYPOINT [“python”, “-m”, “main”]。 LABEL：添加标签 可以为镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。 CMD：执行目标镜像中包含的软件 如果创建镜像的目的是为了部署某个服务，可能会执行某种形式的命令，可以包含参数。 EXPOSE：指定监听端口 给外部访问指定访问端口。 ENV：环境变量 为了方面程序运行，有时需要更新环境变量。 VOLUME：暴露数据库存储文件 USER：指定当前用户 其中常用的命令就是FROM、COPY、WORKDIR、RUN、ENTRYPOINT。 常用命令了解了Dockerfile的常用指令，我们该怎么对镜像和容器进行操作呢？下面就来学习一下docker常用的一些命令， 备注：由于我已经把当前用户加入到docker用户组，所以下面命令没有加sudo，如果没有加用户组需要使用sudo docker。 查看本地镜像 123$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 16.04 ****** 10 days ago 119MB 查看容器 123$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES*** *** *** *** *** *** *** *** 启动、停止、重启容器 123$ docker start $container_id$ docker stop $container_id$ docker restart $container_id 退出和进入镜像 12$ exit$ docker exec $container_id /bin/bash 启动镜像 1$ docker run $image_id 可以用—p和—dns指定端口和dns来配置网络。 container_id是容器ID，image_id是镜像ID。 拉取镜像 1$ docker image pull ubuntu 从Dockerfile创建镜像 1$ docker build 从一个修改的容器创建镜像 1$ docker commit 容器与本地之间复制文件 1$ docker cp 推送镜像 1$ docker push 为镜像打标签 1$ docker tag 重命名容器 1$ docker rename 删除容器 1$ docker rm 删除镜像 1$ docker rmi 搜索镜像 1$ docker search docker常用命令概括： 实践创建项目 123Test/├── Dockerfile└── main.py 写一个简单的测试程序 1234567891011121314151617181920# main.pyimport loggingfrom time import sleepimport numpy as nplogging.basicConfig(level=logging.DEBUG, format="'%(asctime)s - " "%(filename)s[line:%(lineno)d] - " "%(levelname)s: %(message)s")def main(): for i in range(10): logging.debug(np.random.randint(0, 5)) sleep(0.1)if __name__ == '__main__': main() Dockerfile 这是构建镜像中的重点部分， 1234567891011FROM ubuntu:16.04COPY ./ /home/Test_dockerWORKDIR /home/Test_dockerRUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip \&amp;&amp; ln -s pip3 /usr/bin/pip \&amp;&amp; ln -sf /usr/bin/python3 /usr/bin/python \&amp;&amp; rm -rf ls /var/cache/apt/* \ENTRYPOINT ["python3", "-m", "main"] 进入项目根目录 1$ cd Test 开始创建 1$ docker build test:v1.0 . test是指定构建镜像的名称，v1.0指定镜像标签，如果不指定，镜像名称和标签会显示为。 运行镜像 1234567891011$ docker run $image_id'2019-06-29 12:26:38,298 - main.py[line:13] - DEBUG: 0'2019-06-29 12:26:38,399 - main.py[line:13] - DEBUG: 2'2019-06-29 12:26:38,499 - main.py[line:13] - DEBUG: 1'2019-06-29 12:26:38,599 - main.py[line:13] - DEBUG: 3'2019-06-29 12:26:38,699 - main.py[line:13] - DEBUG: 0'2019-06-29 12:26:38,799 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:38,900 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,000 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,100 - main.py[line:13] - DEBUG: 4'2019-06-29 12:26:39,200 - main.py[line:13] - DEBUG: 2 当然也可以在基础镜像的基础上进行修改来创建我们的镜像，例如，我们拉取一个Ubuntu基础镜像，可以启动镜像后安装我们需要的软件和环境，然后利用docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]来创建一个新镜像。 延伸阅读除了基础的docker之外，还有一些高级的docker开源工具，比较知名的有如下3项， docker compose docker machine docker swarm 其中docker compose是官方编排项目之一，用于快速在集群中部署分布式应用。docker machine同样是官方编排项目之一，负责在多种平台上快速安装docker环境。docker swarm提供docker容器集群服务，是docker官方对容器云生态进行支持的核心方案。 除此之外，还有一些比较知名的集群管理系统，例如， Mesos Kubernetes 其中Mesos是来自UC Berkeley的集群资源管理开源项目，它可以让用户很容易实现分布式应用的自动化调度。Kubernetes是由Google团队发起并维护的给予docker的开源容器集群管理系统，应用比较广泛，它不仅支持场景的云平台，而且支持内部数据中心。 学习资源上述所讲的常用命令、指令含义等对于日常开发使用已经够用了，如果对Docker更深入的内容，例如，数据管理、安全、底层实现、容器与云计算等感兴趣可以选取其他的学习资料。在这里我推荐一份我认为不错的学习资料。就是yeasy大神在github开源的一份详细的docker教程—docker_practice，目前docker_practice项目在github已经13.7k个star，想深入学习的可以查看github项目， 也可以查看gitbooks， 或者关注公众号【平凡而诗意】回复关键字”dk”获取pdf和epub版教程， 更多内容请关注公众号【平凡而诗意】]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019最佳论文解读]]></title>
    <url>%2F2019%2F06%2F28%2FCVPR2019%E6%9C%80%E4%BD%B3%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[前言 CVPR，全称IEEE Conference on Computer Vision and Pattern Recognition，与ECCV(Europeon Conference on Computer Vision)，ICCV( IEEE International Conference on Computer Vision)并称为计算机视觉领域三大会议，均为计算机视觉领域的顶级会议。由于近几年计算机视觉的异常火热，CVPR也就成为很多计算机视觉领域研究者趋之若鹜的盛宴，它的受关注程度更是今非昔比。CVPR2019于2019年6月16日在美国召开，此次会议共收到来自全球14104位研究者提交的5160篇文章，同比2018年增长56%，一举打破记录，受欢迎程度可见一斑。 CVPR2019最终共接收1294篇文章，尽管CVPR被计算机视觉领域视为顶尖，我个人认为，其中不乏质量平平的水文，真正令人印象深刻，几年之后依然被人所熟知且实用，并在算法思想方面取得跨越的却寥寥无几。闲话说完，回到本文的重点CVPR2019最佳论文，该荣誉最终由卡耐基梅隆大学、多伦多大学、伦敦大学学院的多位研究者斩获，论文名称为A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction，接下来，我详细解读一下这篇文章。 数学符号含义$s$ 光源上的点$v$ 可见场景内的点$x$ 不可见场景内的点$d$ 检测器上的点$\tau_{\mathcal{F}}$ 费马路径长度$I(\tau ; \boldsymbol{v})$ 瞬态 概念解释瞬态(transients):一种测量值，用于重建隐藏形状信息的大多数方法采用快速调制光源已经传感器来记录光子强度和旅行时间的测量值。费马路径(Fermat paths):首先返回的光子路径的超集合。费马路径长度(Fermat pathlengths):顾名思义，就是通过光速等计算出来的离散路径长度。 算法详解 目前大多数计算机视觉领域的研究都是围绕着视觉可见范围内的研究，但是理解视野范围之外的场景在很多领域却有着非常重要的应用，因此，这使得这项研究更加具有价值。被动方式通过分析隐藏场景所投射的阴影来粗略估计物体的运动和结构，或者利用光的相干性来定位隐藏对象。这些方法没有足够的信息来精确计算位置隐藏场景的三维形状。主动方式提取隐藏场景的附加信息时可能的。大多数重建隐藏形状信息的方法都是用调制光源和时间分辨传感器、超快光电二极管等，这些传感器不仅记录入射光子的强度，还在时间分辨率范围内记录它们到达的时间，这种测量称为瞬态。 大多数主动技术都是通过测量一个已知可见场景的不同位置的瞬态，然后根据已经获取的辐射成像逆向来进行三维重建，例如椭圆反投影、正则化线性系统、光锥变换等。这些方法主要有两个缺点： 它们依赖于辐射测量信息 为了简化反演问题，所有现有的重建技术都依赖于非视线场景的Lambertian 反射假设 在这篇文章中，作者提出一种使用视线以外场景的瞬态测量得到的几何信息的方法，克服了上述的限制。简言之，它主要使用视线内和视线外场景之间的一种称之为费马路径的几何路径星系，通过观察发现这些路径遵循镜面或者物体边界特点的反射定理。作者证明，费马路径对应于瞬态测量中的不连续性，不连续点的时间位置仅是视线外场景对象的形状而不是其反射率。利用上述理论，推导出一种精确重建视线外物体形状的算法，称之为费马流(Fermat Flow)。作者证明，费马路径长度的空间导数提供了一个简单的约束，它唯一地决定了隐藏场景点的深度和法线。这个导数是通过将光滑的路径函数拟合到一组稀疏的测量值上而得到，然后结合深度和法线信息来计算平滑网网格。概括一下，本文在隐藏物体重建方面主要包含以下3个步骤： 瞬态测量 求解费马流方程 表面拟合 测量瞬态 假设已经校准了从光源到可见点，和从可见点到检测器的距离 \mathcal{\tau\mathcal{V}}(\boldsymbol{v}) \triangleq\|s-v\|+\|d-v\|，那么可以通过光速等计算在非可见场景的路径长度， I(\tau ; \boldsymbol{v})=\int_{\mathcal{X}} f(\boldsymbol{x} ; \boldsymbol{v}) \delta(\tau-\tau(\boldsymbol{x} ; \boldsymbol{v})) \mathrm{d} A(p, q)其中 $\tau(\boldsymbol{x} ; \boldsymbol{v}) \triangleq 2 \cdot|\boldsymbol{x}-\boldsymbol{v}|,(p, q) \in[0,1]^{2} $是非可见物体表面的参数化表示。 费马流方程 给定测量的瞬态$I(\tau ; \boldsymbol{v})$ ，可以把它的离散性定义为对费马路径长度的贡献，每个路径长度约束了球面上的点的法线和曲率。这是本文的核心所在，给定一组费马路径长度，就可以得到隐藏物体表面点集的位置和法线。首先定义费马路径函数， \tau_{\mathcal{F}}(\boldsymbol{v})=\{\tau : I(\tau ; \boldsymbol{v}) \text { is discontinuous }\}在每个瞬态可以有多个不连续的路径长度，因此费马路径函数是一个多值函数 \boldsymbol{x}_{\mathcal{F}}=\boldsymbol{v}-\left(\tau_{\mathcal{F}}(\boldsymbol{v}) / 4\right) \nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})其中 $\boldsymbol{x}_{\mathcal{F}} $是隐藏物体球面上的点，因此，物体可以唯一的被可见点\boldsymbol{v}、路径长度、梯度 $\nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})$ 重建，得到隐藏物体表面的点，然后通过一些简单的集合操作即可。但是就算路径长度的导数是一件非常难的事情，它和选取的可视面的形状、位置有密切的关系，为了简化，文中采用选取平面作为可视区域，得到的导数为， \begin{array}{l}{\nabla_{\boldsymbol{v}^{\tau} \mathcal{F}}(\boldsymbol{v})=} \\ {\left.\left(\frac{\partial \tau_{\mathcal{F}}}{\partial x}, \frac{\partial \tau_{\mathcal{F}}}{\partial y}, \sqrt{4-\left(\frac{\partial \tau_{\mathcal{F}}}{\partial x}\right)^{2}-\left(\frac{\partial \tau_{\mathcal{F}}}{\partial y}\right)^{2}}\right)\right|_{\boldsymbol{v}}}\end{array}表面拟合上述的步骤生成了一系列的有向点云，它的密度相当于在可视区域 \mathcal{V} 上的测量密度，然后，可以使用算法，利用正常信息，以更高的精度将曲面表示(如三角形网格)匹配到点云，给定这样一个初始的表面重建，在补充中，我们描述了一个基于高光路径扰动理论的优化过程，该过程对拟合表面进行了细化，以考虑由于梯度 $\nabla_{\boldsymbol{v}} \tau_{\mathcal{F}}(\boldsymbol{v})$ 估计不准确而可能产生的误差。 实验结果 如图中所示，分别从两个视图中重建了一个有方向的点云，点按它们的法线着色。最后，我们将一个表面与点云相匹配，显示在右边的两个视图下。 扫描的对象跨越各种形状(凸，凹)和反射(半透明，光泽，镜面)。对于每一个物体，我们展示了环境光下的照片，以及它表面重建的两个视图。 更多我的作品Jackpop：【动手学计算机视觉】第一讲：图像预处理之图像去噪 Jackpop：【动手学计算机视觉】第二讲：图像预处理之图像增强 Jackpop：【动手学计算机视觉】第三讲：图像预处理之图像分割 Jackpop：【动手学计算机视觉】第四讲：图像预处理之图像增广 Jackpop：【动手学计算机视觉】第五讲：传统目标检测之特征工程 Jackpop：【动手学计算机视觉】第六讲：传统目标检测之Harris角点检测 Jackpop：【动手学计算机视觉】第七讲：传统目标检测之SIFT特征]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>AI</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
</search>
